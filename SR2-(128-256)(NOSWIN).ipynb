{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ff8e93",
   "metadata": {},
   "source": [
    "# Super-Resolution Model Using Diffusion Model (Without SWIN Transformer)\n",
    "\n",
    "This notebook presents the implementation of super-resolution model SR1 that upscales images from 128x128 to 256x256 using a Diffusion Probabilistic Model (DDPM). The model's forward process involves concatenating the low-resolution image with its bilinearly upscaled version, which is then passed through the network to upscale and produce a high-resolution output.\n",
    "\n",
    "The notebook also includes the training routine for the model, which uses a combination of AdamW optimizer and a learning rate scheduler to minimize the mean squared error (MSE) between the predicted and target high-resolution images. The code saves the model's checkpoints periodically and generates comparison images that showcase the input, interpolated, upscaled, and target images side by side.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaf20a5",
   "metadata": {},
   "source": [
    "##### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79299322-b79b-48f0-8f80-bab75283a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b75093",
   "metadata": {},
   "source": [
    "##### 2. Applying Transforms and Initializing Dataset Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90843e2d-e196-4cc2-8f41-05b32eab7387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define transformations for 128x128 and 256x256 images\n",
    "transform_128 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((128, 128)),  # Resize to 128x128\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "transform_256 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "class MRNetUpscaleDataset(Dataset):\n",
    "    def __init__(self, slice_dir, label_files, transform_128=None, transform_256=None):\n",
    "        super().__init__()\n",
    "        self.slice_dir = slice_dir\n",
    "        self.transform_128 = transform_128\n",
    "        self.transform_256 = transform_256\n",
    "\n",
    "        self.labels_dict = {}\n",
    "        for label_file in label_files:\n",
    "            records = pd.read_csv(label_file, header=None, names=['id', 'label'])\n",
    "            records['id'] = records['id'].map(lambda i: '0' * (4 - len(str(i))) + str(i))\n",
    "            self.labels_dict.update(dict(zip(records['id'], records['label'])))\n",
    "\n",
    "        # List all slice files\n",
    "        self.slice_files = [os.path.join(slice_dir, fname) for fname in os.listdir(slice_dir) if fname.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        slice_path = self.slice_files[index]\n",
    "        image = Image.open(slice_path)\n",
    "        \n",
    "        if self.transform_128:\n",
    "            image_128 = self.transform_128(image)\n",
    "        if self.transform_256:\n",
    "            image_256 = self.transform_256(image)\n",
    "        \n",
    "        # Extract ID from filename to match with label\n",
    "        # Filename format: abnormal_0000_slice_0 -> extract \"0000\" as ID\n",
    "        slice_id = os.path.basename(slice_path).split('_')[1]\n",
    "\n",
    "        if slice_id in self.labels_dict:\n",
    "            label = self.labels_dict[slice_id]\n",
    "            label = torch.FloatTensor([label])\n",
    "        else:\n",
    "            print(f\"Label for ID {slice_id} not found in the CSV file.\")\n",
    "            label = torch.FloatTensor([0])  # or raise an error if preferred\n",
    "\n",
    "        return {'data_128': image_128, 'data_256': image_256, 'label': label, 'id': slice_id}\n",
    "\n",
    "\n",
    "# Initialize datasets and data loaders\n",
    "root_dir = \"Raw_Images\"\n",
    "train_slice_dir = os.path.join(root_dir, \"train_slices_raw\")\n",
    "valid_slice_dir = os.path.join(root_dir, \"valid_slices_raw\")\n",
    "\n",
    "train_label_files = [\n",
    "    os.path.join(root_dir, \"train-acl.csv\"),\n",
    "    os.path.join(root_dir, \"train-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"train-meniscus.csv\")\n",
    "]\n",
    "\n",
    "valid_label_files = [\n",
    "    os.path.join(root_dir, \"valid-acl.csv\"),\n",
    "    os.path.join(root_dir, \"valid-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"valid-meniscus.csv\")\n",
    "]\n",
    "\n",
    "train_dataset = MRNetUpscaleDataset(slice_dir=train_slice_dir, label_files=train_label_files, transform_128=transform_128, transform_256=transform_256)\n",
    "valid_dataset = MRNetUpscaleDataset(slice_dir=valid_slice_dir, label_files=valid_label_files, transform_128=transform_128, transform_256=transform_256)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1c5c6c",
   "metadata": {},
   "source": [
    "##### 3. Implementing the UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c816a39a-8339-4bfb-9888-ac1aced5df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sinusoidal positional embedding\n",
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=10000):\n",
    "        super(SinusoidalPositionalEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, timesteps):\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        emb = math.log(self.max_len) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n",
    "        emb = timesteps[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "        if self.embedding_dim % 2 == 1:\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=1)\n",
    "        return emb\n",
    "\n",
    "# Define the UNet model with attention\n",
    "class SuperResUNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=128):\n",
    "        super(SuperResUNet, self).__init__()\n",
    "\n",
    "        self.encoder1 = self.conv_block(in_channels + emb_dim, 64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self.conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.conv_block(256, 128)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self.conv_block(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "        self.timestep_embedding_layer = SinusoidalPositionalEmbedding(emb_dim)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.timestep_embedding_layer(t)\n",
    "        t_embed = t_embed.view(t.size(0), -1, 1, 1)\n",
    "        t_embed = t_embed.repeat(1, 1, x.size(2), x.size(3))\n",
    "\n",
    "        x = torch.cat((x, t_embed), dim=1)\n",
    "\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc1_pooled = self.pool1(enc1)\n",
    "\n",
    "        enc2 = self.encoder2(enc1_pooled)\n",
    "        enc2_pooled = self.pool2(enc2)\n",
    "\n",
    "        enc3 = self.encoder3(enc2_pooled)\n",
    "        enc3_pooled = self.pool3(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc3_pooled)\n",
    "\n",
    "        # Decoder\n",
    "        upconv3 = self.upconv3(bottleneck)\n",
    "        dec3 = torch.cat((upconv3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        upconv2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((upconv2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        upconv1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((upconv1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        final_output = self.final_conv(dec1)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc291f0c",
   "metadata": {},
   "source": [
    "##### 4. Implementing the DDPM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb80b048-5c00-4a59-bb9a-cb6e3ece9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DDPM implementation adapted from:\n",
    "https://github.com/hojonathanho/diffusion/tree/master\n",
    "\"\"\"\n",
    "\n",
    "class SuperResDDPM(nn.Module):\n",
    "    def __init__(self, model, num_timesteps, beta_start=0.00085, beta_end=0.0120):\n",
    "        super(SuperResDDPM, self).__init__()\n",
    "        self.model = model\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', 1 - betas)\n",
    "        self.register_buffer('alphas_cumprod', torch.cumprod(1 - betas, dim=0))\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(self.alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - self.alphas_cumprod))\n",
    "\n",
    "    def forward(self, z_t, t, low_res_image):\n",
    "        # Concatenate low_res_image with z_t to condition the model\n",
    "        low_res_upsampled = F.interpolate(low_res_image, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        return self.model(torch.cat([z_t, low_res_upsampled], dim=1), t)\n",
    "\n",
    "    def sample_timesteps(self, batch_size):\n",
    "        return torch.randint(0, self.num_timesteps, (batch_size,)).to(device)\n",
    "\n",
    "    def forward_diffusion(self, target_high_res_img, t, noise):\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        return sqrt_alphas_cumprod_t * target_high_res_img + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_losses(self, input_low_res_img, target_high_res_img, t):\n",
    "        sqrt_alpha_t = self.sqrt_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        sqrt_one_minus_alpha_t = self.sqrt_one_minus_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "        input_high_res_img = F.interpolate(input_low_res_img, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        noise = torch.randn_like(target_high_res_img)\n",
    "        z_t = self.forward_diffusion(target_high_res_img, t, noise)\n",
    "\n",
    "        predicted_noise = self.forward(z_t, t, input_low_res_img)\n",
    "\n",
    "        return nn.MSELoss()(predicted_noise, noise)\n",
    "\n",
    "    def sample(self, low_res_image):\n",
    "        z_t = torch.randn_like(F.interpolate(low_res_image, scale_factor=2, mode='bilinear', align_corners=False))\n",
    "\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "            t_tensor = torch.tensor([t], device=z_t.device).long()\n",
    "            alpha_t = self.alphas[t]\n",
    "            sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "            sqrt_one_minus_alpha_t = torch.sqrt(1 - self.alphas_cumprod[t])\n",
    "            beta_t = self.betas[t]\n",
    "\n",
    "            predicted_noise = self.forward(z_t, t_tensor, low_res_image)\n",
    "\n",
    "            z_t = (z_t - (1 - self.alphas[t]) * predicted_noise / sqrt_one_minus_alpha_t) / sqrt_alpha_t\n",
    "\n",
    "            if t > 0:\n",
    "                z_t += torch.randn_like(z_t) * torch.sqrt(beta_t)\n",
    "\n",
    "        return z_t\n",
    "\n",
    "    def p_sample(self, z, t, low_res_image):\n",
    "        alpha_t = self.alphas[t]\n",
    "        sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "        sqrt_one_minus_alpha_t = torch.sqrt(1 - self.alphas_cumprod[t])\n",
    "        beta_t = self.betas[t]\n",
    "        predicted_noise = self.forward(z, t, low_res_image)\n",
    "\n",
    "        z = (z - beta_t / sqrt_one_minus_alpha_t * predicted_noise) / sqrt_alpha_t\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc61eb",
   "metadata": {},
   "source": [
    "##### 5. Functions for Training and Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a076c6b-7911-421b-af1f-f83e9ed35f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "def save_model(ddpm, epoch, checkpoint_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': ddpm.state_dict()\n",
    "    }, checkpoint_path)\n",
    "    print(f'Model checkpoint saved at {checkpoint_path}')\n",
    "\n",
    "# Load the model checkpoint\n",
    "def load_model(ddpm, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    ddpm.load_state_dict(checkpoint['model_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    return ddpm, start_epoch\n",
    "\n",
    "# Function to generate and save comparison images\n",
    "def compare_and_save_images(ddpm, valid_loader, epoch, save_dir='generated_images/training'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    ddpm.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(valid_loader):\n",
    "            inputs = batch['data_128'].to(device)\n",
    "            targets = batch['data_256'].to(device)\n",
    "            \n",
    "            # Prepare lists to store images for comparison\n",
    "            inputs_list = []\n",
    "            interpolated_list = []\n",
    "            upscaled_list = []\n",
    "            targets_list = []\n",
    "            \n",
    "            # Process each image individually\n",
    "            for j in range(inputs.size(0)):\n",
    "                input_image = inputs[j].unsqueeze(0)\n",
    "                target_image = targets[j].unsqueeze(0)\n",
    "                \n",
    "                # Interpolate the low-resolution image\n",
    "                interpolated_image = F.interpolate(input_image, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "                \n",
    "                # Upscale the image using the DDPM model\n",
    "                upscaled_image = ddpm.sample(input_image)\n",
    "                \n",
    "                inputs_list.append(input_image.cpu().numpy().squeeze())\n",
    "                interpolated_list.append(interpolated_image.cpu().numpy().squeeze())\n",
    "                upscaled_list.append(upscaled_image.cpu().numpy().squeeze())\n",
    "                targets_list.append(target_image.cpu().numpy().squeeze())\n",
    "                \n",
    "            \n",
    "            # Plot comparison for the first 5 images\n",
    "            num_images = min(5, inputs.size(0))\n",
    "            fig, axes = plt.subplots(num_images, 4, figsize=(20, 5 * num_images))\n",
    "            \n",
    "            for j in range(num_images):\n",
    "                ax = axes[j, 0]\n",
    "                ax.imshow(inputs_list[j], cmap='gray')\n",
    "                ax.set_title(f'Input 128x128 Image {j+1}')\n",
    "                ax.axis('off')\n",
    "\n",
    "                ax = axes[j, 1]\n",
    "                ax.imshow(interpolated_list[j], cmap='gray')\n",
    "                ax.set_title(f'Interpolated 256x256 Image {j+1}')\n",
    "                ax.axis('off')\n",
    "                \n",
    "                ax = axes[j, 2]\n",
    "                ax.imshow(upscaled_list[j], cmap='gray')\n",
    "                ax.set_title(f'Upscaled 256x256 Image {j+1}')\n",
    "                ax.axis('off')\n",
    "\n",
    "                ax = axes[j, 3]\n",
    "                ax.imshow(targets_list[j], cmap='gray')\n",
    "                ax.set_title(f'Target 256x256 Image {j+1}')\n",
    "                ax.axis('off')\n",
    "            \n",
    "            save_path = os.path.join(save_dir, f'comparison_epoch_{epoch+1}.png')\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "            print(f'Comparison images saved at {save_path}')\n",
    "            break  # Only process the first batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c230f",
   "metadata": {},
   "source": [
    "##### 6. Training Routine for the Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190be532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diffusion_model(ddpm, train_loader, valid_loader, epochs=10, save_interval=10, checkpoint_path='cascadedddpm_checkpoint.pth'):\n",
    "    optimizer = optim.AdamW(ddpm.parameters(), lr=1e-5, weight_decay=1e-2)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "    start_epoch = 0\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        ddpm, start_epoch = load_model(ddpm, checkpoint_path)\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "        print(f\"Resuming training from epoch {start_epoch+1}\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}, starting from scratch.\")\n",
    "\n",
    "    ddpm.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        ddpm.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs = batch['data_128'].to(device)\n",
    "            targets = batch['data_256'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            t = ddpm.sample_timesteps(inputs.size(0))\n",
    "            loss = ddpm.p_losses(inputs, targets, t)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient checking\n",
    "            total_norm = 0\n",
    "            for p in ddpm.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "\n",
    "            if total_norm > 1e3:  # Threshold for exploding gradients\n",
    "                print(f\"Warning: Exploding gradients detected at epoch {epoch+1}, total norm: {total_norm:.2f}\")\n",
    "                continue\n",
    "            if total_norm < 1e-3:  # Threshold for vanishing gradients\n",
    "                print(f\"Warning: Vanishing gradients detected at epoch {epoch+1}, total norm: {total_norm:.2f}\")\n",
    "                continue\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Gradient Norm: {total_norm:.2f}')\n",
    "\n",
    "        ddpm.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                inputs = batch['data_128'].to(device)\n",
    "                targets = batch['data_256'].to(device)\n",
    "                t = ddpm.sample_timesteps(inputs.size(0))\n",
    "                loss = ddpm.p_losses(inputs, targets, t)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        avg_valid_loss = valid_loss / len(valid_loader)\n",
    "        valid_losses.append(avg_valid_loss)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_valid_loss:.4f}')\n",
    "\n",
    "        # Step the scheduler with the validation loss\n",
    "        scheduler.step(avg_valid_loss)\n",
    "\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            save_model(ddpm, epoch, checkpoint_path)\n",
    "            compare_and_save_images(ddpm, valid_loader, epoch)\n",
    "            print(f'Model saved at epoch {epoch+1}')\n",
    "        \n",
    "        # Special checkpoint save\n",
    "        # if (epoch + 1) % 100 == 0:\n",
    "        #     special_checkpoint_path = checkpoint_path.replace(\".pth\", f\"_{epoch+1}.pth\")\n",
    "        #     save_model(ddpm, epoch, special_checkpoint_path)\n",
    "        #     print(f'Special checkpoint saved at epoch {epoch+1}')\n",
    "\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79f9e3b",
   "metadata": {},
   "source": [
    "##### 7. Running the Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "791c44b3-e198-4fbc-8484-351c5eff9400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found at cascadedddpm128NOSWIN_checkpoint.pth, starting from scratch.\n",
      "Epoch [1/100], Train Loss: 0.8480, Gradient Norm: 1.64\n",
      "Epoch [1/100], Validation Loss: 0.4752\n",
      "Epoch [2/100], Train Loss: 0.1959, Gradient Norm: 0.79\n",
      "Epoch [2/100], Validation Loss: 0.1098\n",
      "Epoch [3/100], Train Loss: 0.0997, Gradient Norm: 4.72\n",
      "Epoch [3/100], Validation Loss: 0.0948\n",
      "Epoch [4/100], Train Loss: 0.0711, Gradient Norm: 1.80\n",
      "Epoch [4/100], Validation Loss: 0.0651\n",
      "Epoch [5/100], Train Loss: 0.0563, Gradient Norm: 2.71\n",
      "Epoch [5/100], Validation Loss: 0.0607\n",
      "Epoch [6/100], Train Loss: 0.0468, Gradient Norm: 2.89\n",
      "Epoch [6/100], Validation Loss: 0.0435\n",
      "Epoch [7/100], Train Loss: 0.0379, Gradient Norm: 1.30\n",
      "Epoch [7/100], Validation Loss: 0.0310\n",
      "Epoch [8/100], Train Loss: 0.0311, Gradient Norm: 2.44\n",
      "Epoch [8/100], Validation Loss: 0.0343\n",
      "Epoch [9/100], Train Loss: 0.0284, Gradient Norm: 0.92\n",
      "Epoch [9/100], Validation Loss: 0.0241\n",
      "Epoch [10/100], Train Loss: 0.0268, Gradient Norm: 0.19\n",
      "Epoch [10/100], Validation Loss: 0.0216\n",
      "Model checkpoint saved at cascadedddpm128NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/128to256NOSWIN/comparison_epoch_10.png\n",
      "Model saved at epoch 10\n",
      "Epoch [11/100], Train Loss: 0.0211, Gradient Norm: 0.42\n",
      "Epoch [11/100], Validation Loss: 0.0228\n",
      "Epoch [12/100], Train Loss: 0.0196, Gradient Norm: 0.38\n",
      "Epoch [12/100], Validation Loss: 0.0186\n",
      "Epoch [13/100], Train Loss: 0.0160, Gradient Norm: 3.96\n",
      "Epoch [13/100], Validation Loss: 0.0159\n",
      "Epoch [14/100], Train Loss: 0.0167, Gradient Norm: 3.39\n",
      "Epoch [14/100], Validation Loss: 0.0251\n",
      "Epoch [15/100], Train Loss: 0.0158, Gradient Norm: 0.40\n",
      "Epoch [15/100], Validation Loss: 0.0177\n",
      "Epoch [16/100], Train Loss: 0.0149, Gradient Norm: 2.87\n",
      "Epoch [16/100], Validation Loss: 0.0141\n",
      "Epoch [17/100], Train Loss: 0.0148, Gradient Norm: 0.12\n",
      "Epoch [17/100], Validation Loss: 0.0163\n",
      "Epoch [18/100], Train Loss: 0.0123, Gradient Norm: 0.80\n",
      "Epoch [18/100], Validation Loss: 0.0100\n",
      "Epoch [19/100], Train Loss: 0.0124, Gradient Norm: 0.38\n",
      "Epoch [19/100], Validation Loss: 0.0103\n",
      "Epoch [20/100], Train Loss: 0.0123, Gradient Norm: 0.78\n",
      "Epoch [20/100], Validation Loss: 0.0132\n",
      "Model checkpoint saved at cascadedddpm128NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/128to256NOSWIN/comparison_epoch_20.png\n",
      "Model saved at epoch 20\n",
      "Epoch [21/100], Train Loss: 0.0113, Gradient Norm: 1.06\n",
      "Epoch [21/100], Validation Loss: 0.0097\n",
      "Epoch [22/100], Train Loss: 0.0098, Gradient Norm: 1.69\n",
      "Epoch [22/100], Validation Loss: 0.0123\n",
      "Epoch [23/100], Train Loss: 0.0110, Gradient Norm: 0.13\n",
      "Epoch [23/100], Validation Loss: 0.0121\n",
      "Epoch [24/100], Train Loss: 0.0110, Gradient Norm: 2.90\n",
      "Epoch [24/100], Validation Loss: 0.0091\n",
      "Epoch [25/100], Train Loss: 0.0100, Gradient Norm: 1.11\n",
      "Epoch [25/100], Validation Loss: 0.0091\n",
      "Epoch [26/100], Train Loss: 0.0095, Gradient Norm: 5.66\n",
      "Epoch [26/100], Validation Loss: 0.0195\n",
      "Epoch [27/100], Train Loss: 0.0094, Gradient Norm: 0.27\n",
      "Epoch [27/100], Validation Loss: 0.0150\n",
      "Epoch [28/100], Train Loss: 0.0090, Gradient Norm: 1.69\n",
      "Epoch [28/100], Validation Loss: 0.0111\n",
      "Epoch [29/100], Train Loss: 0.0097, Gradient Norm: 0.34\n",
      "Epoch [29/100], Validation Loss: 0.0107\n",
      "Epoch [30/100], Train Loss: 0.0086, Gradient Norm: 2.08\n",
      "Epoch [30/100], Validation Loss: 0.0132\n",
      "Model checkpoint saved at cascadedddpm128NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/128to256NOSWIN/comparison_epoch_30.png\n",
      "Model saved at epoch 30\n",
      "Epoch [31/100], Train Loss: 0.0088, Gradient Norm: 0.85\n",
      "Epoch [31/100], Validation Loss: 0.0087\n",
      "Epoch [32/100], Train Loss: 0.0080, Gradient Norm: 2.27\n",
      "Epoch [32/100], Validation Loss: 0.0079\n",
      "Epoch [33/100], Train Loss: 0.0088, Gradient Norm: 1.12\n",
      "Epoch [33/100], Validation Loss: 0.0098\n",
      "Epoch [34/100], Train Loss: 0.0088, Gradient Norm: 0.24\n",
      "Epoch [34/100], Validation Loss: 0.0046\n",
      "Epoch [35/100], Train Loss: 0.0082, Gradient Norm: 0.12\n",
      "Epoch [35/100], Validation Loss: 0.0083\n",
      "Epoch [36/100], Train Loss: 0.0074, Gradient Norm: 0.49\n",
      "Epoch [36/100], Validation Loss: 0.0092\n",
      "Epoch [37/100], Train Loss: 0.0083, Gradient Norm: 0.47\n",
      "Epoch [37/100], Validation Loss: 0.0096\n",
      "Epoch [38/100], Train Loss: 0.0080, Gradient Norm: 1.10\n",
      "Epoch [38/100], Validation Loss: 0.0081\n",
      "Epoch [39/100], Train Loss: 0.0077, Gradient Norm: 1.82\n",
      "Epoch [39/100], Validation Loss: 0.0078\n",
      "Epoch [40/100], Train Loss: 0.0073, Gradient Norm: 0.74\n",
      "Epoch [40/100], Validation Loss: 0.0107\n",
      "Model checkpoint saved at cascadedddpm128NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/128to256NOSWIN/comparison_epoch_40.png\n",
      "Model saved at epoch 40\n",
      "Epoch [41/100], Train Loss: 0.0078, Gradient Norm: 0.32\n",
      "Epoch [41/100], Validation Loss: 0.0076\n",
      "Epoch [42/100], Train Loss: 0.0062, Gradient Norm: 0.50\n",
      "Epoch [42/100], Validation Loss: 0.0080\n",
      "Epoch [43/100], Train Loss: 0.0080, Gradient Norm: 0.20\n",
      "Epoch [43/100], Validation Loss: 0.0088\n",
      "Epoch [44/100], Train Loss: 0.0071, Gradient Norm: 0.56\n",
      "Epoch [44/100], Validation Loss: 0.0052\n",
      "Epoch [45/100], Train Loss: 0.0074, Gradient Norm: 0.26\n",
      "Epoch [45/100], Validation Loss: 0.0056\n",
      "Epoch 00045: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch [46/100], Train Loss: 0.0071, Gradient Norm: 0.10\n",
      "Epoch [46/100], Validation Loss: 0.0070\n",
      "Epoch [47/100], Train Loss: 0.0071, Gradient Norm: 0.26\n",
      "Epoch [47/100], Validation Loss: 0.0062\n",
      "Epoch [48/100], Train Loss: 0.0068, Gradient Norm: 1.14\n",
      "Epoch [48/100], Validation Loss: 0.0062\n",
      "Epoch [49/100], Train Loss: 0.0065, Gradient Norm: 0.89\n",
      "Epoch [49/100], Validation Loss: 0.0058\n",
      "Epoch [50/100], Train Loss: 0.0069, Gradient Norm: 0.09\n",
      "Epoch [50/100], Validation Loss: 0.0060\n",
      "Model checkpoint saved at cascadedddpm128NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/128to256NOSWIN/comparison_epoch_50.png\n",
      "Model saved at epoch 50\n",
      "Epoch [51/100], Train Loss: 0.0061, Gradient Norm: 0.23\n",
      "Epoch [51/100], Validation Loss: 0.0066\n",
      "Epoch [52/100], Train Loss: 0.0070, Gradient Norm: 0.68\n",
      "Epoch [52/100], Validation Loss: 0.0081\n",
      "Epoch [53/100], Train Loss: 0.0070, Gradient Norm: 1.81\n",
      "Epoch [53/100], Validation Loss: 0.0057\n",
      "Epoch [54/100], Train Loss: 0.0068, Gradient Norm: 0.19\n",
      "Epoch [54/100], Validation Loss: 0.0069\n",
      "Epoch [55/100], Train Loss: 0.0061, Gradient Norm: 0.25\n",
      "Epoch [55/100], Validation Loss: 0.0051\n",
      "Epoch [56/100], Train Loss: 0.0060, Gradient Norm: 0.10\n",
      "Epoch [56/100], Validation Loss: 0.0060\n",
      "Epoch 00056: reducing learning rate of group 0 to 2.5000e-06.\n",
      "Epoch [57/100], Train Loss: 0.0071, Gradient Norm: 0.20\n",
      "Epoch [57/100], Validation Loss: 0.0061\n",
      "Epoch [58/100], Train Loss: 0.0063, Gradient Norm: 0.32\n",
      "Epoch [58/100], Validation Loss: 0.0061\n",
      "Epoch [59/100], Train Loss: 0.0063, Gradient Norm: 0.83\n",
      "Epoch [59/100], Validation Loss: 0.0054\n",
      "Epoch [60/100], Train Loss: 0.0068, Gradient Norm: 0.50\n",
      "Epoch [60/100], Validation Loss: 0.0089\n",
      "Model checkpoint saved at cascadedddpm128NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/128to256NOSWIN/comparison_epoch_60.png\n",
      "Model saved at epoch 60\n",
      "Epoch [61/100], Train Loss: 0.0068, Gradient Norm: 0.13\n",
      "Epoch [61/100], Validation Loss: 0.0047\n",
      "Epoch [62/100], Train Loss: 0.0063, Gradient Norm: 1.01\n",
      "Epoch [62/100], Validation Loss: 0.0059\n",
      "Epoch [63/100], Train Loss: 0.0061, Gradient Norm: 0.31\n",
      "Epoch [63/100], Validation Loss: 0.0052\n",
      "Epoch [64/100], Train Loss: 0.0065, Gradient Norm: 0.63\n",
      "Epoch [64/100], Validation Loss: 0.0073\n",
      "Epoch [65/100], Train Loss: 0.0063, Gradient Norm: 0.86\n",
      "Epoch [65/100], Validation Loss: 0.0061\n",
      "Epoch [66/100], Train Loss: 0.0065, Gradient Norm: 0.27\n",
      "Epoch [66/100], Validation Loss: 0.0035\n",
      "Epoch [67/100], Train Loss: 0.0058, Gradient Norm: 1.09\n",
      "Epoch [67/100], Validation Loss: 0.0050\n",
      "Epoch [68/100], Train Loss: 0.0063, Gradient Norm: 0.89\n",
      "Epoch [68/100], Validation Loss: 0.0043\n",
      "Epoch [69/100], Train Loss: 0.0062, Gradient Norm: 0.18\n",
      "Epoch [69/100], Validation Loss: 0.0065\n",
      "Epoch [70/100], Train Loss: 0.0060, Gradient Norm: 0.14\n",
      "Epoch [70/100], Validation Loss: 0.0066\n",
      "Model checkpoint saved at cascadedddpm128NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/128to256NOSWIN/comparison_epoch_70.png\n",
      "Model saved at epoch 70\n",
      "Epoch [71/100], Train Loss: 0.0060, Gradient Norm: 0.12\n",
      "Epoch [71/100], Validation Loss: 0.0056\n",
      "Epoch [72/100], Train Loss: 0.0060, Gradient Norm: 0.81\n",
      "Epoch [72/100], Validation Loss: 0.0051\n",
      "Epoch [73/100], Train Loss: 0.0065, Gradient Norm: 0.88\n",
      "Epoch [73/100], Validation Loss: 0.0067\n",
      "Epoch [74/100], Train Loss: 0.0058, Gradient Norm: 0.47\n",
      "Epoch [74/100], Validation Loss: 0.0059\n",
      "Epoch [75/100], Train Loss: 0.0066, Gradient Norm: 0.11\n",
      "Epoch [75/100], Validation Loss: 0.0074\n",
      "Epoch [76/100], Train Loss: 0.0062, Gradient Norm: 0.33\n",
      "Epoch [76/100], Validation Loss: 0.0056\n",
      "Epoch [77/100], Train Loss: 0.0056, Gradient Norm: 0.17\n",
      "Epoch [77/100], Validation Loss: 0.0051\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.2500e-06.\n",
      "Epoch [78/100], Train Loss: 0.0056, Gradient Norm: 0.10\n",
      "Epoch [78/100], Validation Loss: 0.0053\n",
      "Epoch [79/100], Train Loss: 0.0062, Gradient Norm: 0.11\n",
      "Epoch [79/100], Validation Loss: 0.0054\n",
      "Epoch [80/100], Train Loss: 0.0061, Gradient Norm: 0.27\n",
      "Epoch [80/100], Validation Loss: 0.0069\n",
      "Model checkpoint saved at cascadedddpm128NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/128to256NOSWIN/comparison_epoch_80.png\n",
      "Model saved at epoch 80\n",
      "Epoch [81/100], Train Loss: 0.0061, Gradient Norm: 1.15\n",
      "Epoch [81/100], Validation Loss: 0.0062\n",
      "Epoch [82/100], Train Loss: 0.0063, Gradient Norm: 0.15\n",
      "Epoch [82/100], Validation Loss: 0.0065\n",
      "Epoch [83/100], Train Loss: 0.0058, Gradient Norm: 0.46\n",
      "Epoch [83/100], Validation Loss: 0.0066\n",
      "Epoch [84/100], Train Loss: 0.0061, Gradient Norm: 0.39\n",
      "Epoch [84/100], Validation Loss: 0.0052\n",
      "Epoch [85/100], Train Loss: 0.0054, Gradient Norm: 0.31\n",
      "Epoch [85/100], Validation Loss: 0.0054\n",
      "Epoch [86/100], Train Loss: 0.0058, Gradient Norm: 0.50\n",
      "Epoch [86/100], Validation Loss: 0.0043\n",
      "Epoch [87/100], Train Loss: 0.0062, Gradient Norm: 0.06\n",
      "Epoch [87/100], Validation Loss: 0.0053\n",
      "Epoch [88/100], Train Loss: 0.0061, Gradient Norm: 0.32\n",
      "Epoch [88/100], Validation Loss: 0.0043\n",
      "Epoch 00088: reducing learning rate of group 0 to 6.2500e-07.\n",
      "Epoch [89/100], Train Loss: 0.0061, Gradient Norm: 0.46\n",
      "Epoch [89/100], Validation Loss: 0.0059\n",
      "Epoch [90/100], Train Loss: 0.0058, Gradient Norm: 0.14\n",
      "Epoch [90/100], Validation Loss: 0.0057\n",
      "Model checkpoint saved at cascadedddpm128NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/128to256NOSWIN/comparison_epoch_90.png\n",
      "Model saved at epoch 90\n",
      "Epoch [91/100], Train Loss: 0.0058, Gradient Norm: 0.07\n",
      "Epoch [91/100], Validation Loss: 0.0066\n",
      "Epoch [92/100], Train Loss: 0.0054, Gradient Norm: 0.18\n",
      "Epoch [92/100], Validation Loss: 0.0061\n",
      "Epoch [93/100], Train Loss: 0.0054, Gradient Norm: 0.24\n",
      "Epoch [93/100], Validation Loss: 0.0068\n",
      "Epoch [94/100], Train Loss: 0.0061, Gradient Norm: 0.15\n",
      "Epoch [94/100], Validation Loss: 0.0052\n",
      "Epoch [95/100], Train Loss: 0.0057, Gradient Norm: 0.28\n",
      "Epoch [95/100], Validation Loss: 0.0049\n",
      "Epoch [96/100], Train Loss: 0.0060, Gradient Norm: 0.06\n",
      "Epoch [96/100], Validation Loss: 0.0046\n",
      "Epoch [97/100], Train Loss: 0.0055, Gradient Norm: 0.05\n",
      "Epoch [97/100], Validation Loss: 0.0070\n",
      "Epoch [98/100], Train Loss: 0.0062, Gradient Norm: 0.09\n",
      "Epoch [98/100], Validation Loss: 0.0046\n",
      "Epoch [99/100], Train Loss: 0.0053, Gradient Norm: 0.04\n",
      "Epoch [99/100], Validation Loss: 0.0070\n",
      "Epoch 00099: reducing learning rate of group 0 to 3.1250e-07.\n",
      "Epoch [100/100], Train Loss: 0.0060, Gradient Norm: 0.12\n",
      "Epoch [100/100], Validation Loss: 0.0067\n",
      "Model checkpoint saved at cascadedddpm128NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/128to256NOSWIN/comparison_epoch_100.png\n",
      "Model saved at epoch 100\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the UNet model and DDPM\n",
    "in_channels = 2  # For grayscale images\n",
    "out_channels = 1  # For grayscale images\n",
    "emb_dim = 128\n",
    "num_timesteps = 1000\n",
    "\n",
    "\n",
    "unet = SuperResUNet(in_channels, out_channels, emb_dim).to(device)\n",
    "ddpm = SuperResDDPM(unet, num_timesteps).to(device)\n",
    "\n",
    "# Train the model\n",
    "train_diffusion_model(ddpm, train_loader, valid_loader, epochs=100, save_interval=10, checkpoint_path='Model_Savepoints/cascadedddpm128NOSWIN_checkpoint.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f0fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
