{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aacdcc6",
   "metadata": {},
   "source": [
    "# Super-Resolution Model Using Diffusion Model (Without SWIN Transformer)\n",
    "\n",
    "This notebook presents the implementation of super-resolution model SR1 that upscales images from 64x64 to 128x128 using a Diffusion Probabilistic Model (DDPM). The model's forward process involves concatenating the low-resolution image with its bilinearly upscaled version, which is then passed through the network to upscale and produce a high-resolution output.\n",
    "\n",
    "The notebook also includes the training routine for the model, which uses a combination of AdamW optimizer and a learning rate scheduler to minimize the mean squared error (MSE) between the predicted and target high-resolution images. The code saves the model's checkpoints periodically and generates comparison images that showcase the input, interpolated, upscaled, and target images side by side.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b5706a",
   "metadata": {},
   "source": [
    "##### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10cdffbe-5f0f-4217-bda5-f5bdbb19dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986139c",
   "metadata": {},
   "source": [
    "##### 2. Applying Transforms and Initializing Dataset Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e6a82d-697d-46ba-b8cd-b2be436fea17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for 64x64 and 128x128 images\n",
    "transform_64 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((64, 64)),  # Resize to 64x64\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "transform_128 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((128, 128)),  # Resize to 128x128\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "class MRNetUpscaleDataset(Dataset):\n",
    "    def __init__(self, slice_dir, label_files, transform_64=None, transform_128=None):\n",
    "        super().__init__()\n",
    "        self.slice_dir = slice_dir\n",
    "        self.transform_64 = transform_64\n",
    "        self.transform_128 = transform_128\n",
    "\n",
    "        self.labels_dict = {}\n",
    "        for label_file in label_files:\n",
    "            records = pd.read_csv(label_file, header=None, names=['id', 'label'])\n",
    "            records['id'] = records['id'].map(lambda i: '0' * (4 - len(str(i))) + str(i))\n",
    "            self.labels_dict.update(dict(zip(records['id'], records['label'])))\n",
    "\n",
    "        # List all slice files\n",
    "        self.slice_files = [os.path.join(slice_dir, fname) for fname in os.listdir(slice_dir) if fname.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        slice_path = self.slice_files[index]\n",
    "        image = Image.open(slice_path)\n",
    "        \n",
    "        if self.transform_64:\n",
    "            image_64 = self.transform_64(image)\n",
    "        if self.transform_128:\n",
    "            image_128 = self.transform_128(image)\n",
    "        \n",
    "        # Extract ID from filename to match with label\n",
    "        slice_id = os.path.basename(slice_path).split('_')[1]\n",
    "\n",
    "        if slice_id in self.labels_dict:\n",
    "            label = self.labels_dict[slice_id]\n",
    "            label = torch.FloatTensor([label])\n",
    "        else:\n",
    "            print(f\"Label for ID {slice_id} not found in the CSV file.\")\n",
    "            label = torch.FloatTensor([0])  # or raise an error if preferred\n",
    "\n",
    "        return {'data_64': image_64, 'data_128': image_128, 'label': label, 'id': slice_id}\n",
    "\n",
    "\n",
    "# Initialize datasets and data loaders\n",
    "root_dir = \"Raw_Images\"\n",
    "train_slice_dir = os.path.join(root_dir, \"train_slices_raw\")\n",
    "valid_slice_dir = os.path.join(root_dir, \"valid_slices_raw\")\n",
    "\n",
    "train_label_files = [\n",
    "    os.path.join(root_dir, \"train-acl.csv\"),\n",
    "    os.path.join(root_dir, \"train-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"train-meniscus.csv\")\n",
    "]\n",
    "\n",
    "valid_label_files = [\n",
    "    os.path.join(root_dir, \"valid-acl.csv\"),\n",
    "    os.path.join(root_dir, \"valid-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"valid-meniscus.csv\")\n",
    "]\n",
    "\n",
    "train_dataset = MRNetUpscaleDataset(slice_dir=train_slice_dir, label_files=train_label_files, transform_64=transform_64, transform_128=transform_128)\n",
    "valid_dataset = MRNetUpscaleDataset(slice_dir=valid_slice_dir, label_files=valid_label_files, transform_64=transform_64, transform_128=transform_128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bba8e6",
   "metadata": {},
   "source": [
    "##### 3. Implementing the UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f0c5ae1-260f-414f-afcb-5cd11acf3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sinusoidal positional embedding\n",
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=10000):\n",
    "        super(SinusoidalPositionalEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, timesteps):\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        emb = math.log(self.max_len) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n",
    "        emb = timesteps[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "        if self.embedding_dim % 2 == 1:\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=1)\n",
    "        return emb\n",
    "\n",
    "# Define the UNet model without the Swin Transformer\n",
    "class SuperResUNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=128):\n",
    "        super(SuperResUNet, self).__init__()\n",
    "\n",
    "        self.encoder1 = self.conv_block(in_channels + emb_dim, 64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self.conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.conv_block(256, 128)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self.conv_block(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "        self.timestep_embedding_layer = SinusoidalPositionalEmbedding(emb_dim)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.timestep_embedding_layer(t)\n",
    "        t_embed = t_embed.view(t.size(0), -1, 1, 1)\n",
    "        t_embed = t_embed.repeat(1, 1, x.size(2), x.size(3))\n",
    "\n",
    "        x = torch.cat((x, t_embed), dim=1)\n",
    "\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc1_pooled = self.pool1(enc1)\n",
    "\n",
    "        enc2 = self.encoder2(enc1_pooled)\n",
    "        enc2_pooled = self.pool2(enc2)\n",
    "\n",
    "        enc3 = self.encoder3(enc2_pooled)\n",
    "        enc3_pooled = self.pool3(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc3_pooled)\n",
    "\n",
    "        # Decoder\n",
    "        upconv3 = self.upconv3(bottleneck)\n",
    "        dec3 = torch.cat((upconv3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        upconv2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((upconv2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        upconv1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((upconv1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        final_output = self.final_conv(dec1)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd1bdc8",
   "metadata": {},
   "source": [
    "##### 4. Implementing the DDPM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "567beba2-9945-4ba3-a0dc-f76fa43d8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DDPM implementation adapted from:\n",
    "https://github.com/hojonathanho/diffusion/tree/master\n",
    "\"\"\"\n",
    "\n",
    "class SuperResDDPM(nn.Module):\n",
    "    def __init__(self, model, num_timesteps, beta_start=0.00085, beta_end=0.0120):\n",
    "        super(SuperResDDPM, self).__init__()\n",
    "        self.model = model\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', 1 - betas)\n",
    "        self.register_buffer('alphas_cumprod', torch.cumprod(1 - betas, dim=0))\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(self.alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - self.alphas_cumprod))\n",
    "\n",
    "    def forward(self, z_t, t, low_res_image):\n",
    "        # Concatenate low_res_image with z_t to condition the model\n",
    "        low_res_upsampled = F.interpolate(low_res_image, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        return self.model(torch.cat([z_t, low_res_upsampled], dim=1), t)\n",
    "\n",
    "    def sample_timesteps(self, batch_size):\n",
    "        return torch.randint(0, self.num_timesteps, (batch_size,)).to(device)\n",
    "\n",
    "    def forward_diffusion(self, target_high_res_img, t, noise):\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        return sqrt_alphas_cumprod_t * target_high_res_img + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_losses(self, input_low_res_img, target_high_res_img, t):\n",
    "        sqrt_alpha_t = self.sqrt_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        sqrt_one_minus_alpha_t = self.sqrt_one_minus_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "        input_high_res_img = F.interpolate(input_low_res_img, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        noise = torch.randn_like(target_high_res_img)\n",
    "        z_t = self.forward_diffusion(target_high_res_img, t, noise)\n",
    "\n",
    "        predicted_noise = self.forward(z_t, t, input_low_res_img)\n",
    "\n",
    "        return nn.MSELoss()(predicted_noise, noise)\n",
    "\n",
    "    def sample(self, low_res_image):\n",
    "        z_t = torch.randn_like(F.interpolate(low_res_image, scale_factor=2, mode='bilinear', align_corners=False))\n",
    "\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "            t_tensor = torch.tensor([t], device=z_t.device).long()\n",
    "            alpha_t = self.alphas[t]\n",
    "            sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "            sqrt_one_minus_alpha_t = torch.sqrt(1 - self.alphas_cumprod[t])\n",
    "            beta_t = self.betas[t]\n",
    "\n",
    "            predicted_noise = self.forward(z_t, t_tensor, low_res_image)\n",
    "\n",
    "            z_t = (z_t - (1 - self.alphas[t]) * predicted_noise / sqrt_one_minus_alpha_t) / sqrt_alpha_t\n",
    "\n",
    "            if t > 0:\n",
    "                z_t += torch.randn_like(z_t) * torch.sqrt(beta_t)\n",
    "\n",
    "        return z_t\n",
    "\n",
    "    def p_sample(self, z, t, low_res_image):\n",
    "        alpha_t = self.alphas[t]\n",
    "        sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "        sqrt_one_minus_alpha_t = torch.sqrt(1 - self.alphas_cumprod[t])\n",
    "        beta_t = self.betas[t]\n",
    "        predicted_noise = self.forward(z, t, low_res_image)\n",
    "\n",
    "        z = (z - beta_t / sqrt_one_minus_alpha_t * predicted_noise) / sqrt_alpha_t\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d0bcec",
   "metadata": {},
   "source": [
    "##### 5. Functions for Training and Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88b840d4-144c-4f99-a71b-9f9b1a5f2564",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model(ddpm, epoch, checkpoint_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': ddpm.state_dict()\n",
    "    }, checkpoint_path)\n",
    "    print(f'Model checkpoint saved at {checkpoint_path}')\n",
    "\n",
    "def load_model(ddpm, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    ddpm.load_state_dict(checkpoint['model_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    return ddpm, start_epoch\n",
    "\n",
    "def compare_and_save_images(ddpm, valid_loader, epoch, save_dir='generated_images/training'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    ddpm.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(valid_loader):\n",
    "            inputs = batch['data_64'].to(device)\n",
    "            targets = batch['data_128'].to(device)\n",
    "            \n",
    "            # Prepare lists to store images for comparison\n",
    "            inputs_list = []\n",
    "            interpolated_list = []\n",
    "            upscaled_list = []\n",
    "            targets_list = []\n",
    "            \n",
    "            # Process each image individually\n",
    "            for j in range(inputs.size(0)):\n",
    "                input_image = inputs[j].unsqueeze(0)\n",
    "                target_image = targets[j].unsqueeze(0)\n",
    "                \n",
    "                # Interpolate the low-resolution image\n",
    "                interpolated_image = F.interpolate(input_image, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "                \n",
    "                # Upscale the image using the DDPM model\n",
    "                upscaled_image = ddpm.sample(input_image)\n",
    "                \n",
    "                inputs_list.append(input_image.cpu().numpy().squeeze())\n",
    "                interpolated_list.append(interpolated_image.cpu().numpy().squeeze())\n",
    "                upscaled_list.append(upscaled_image.cpu().numpy().squeeze())\n",
    "                targets_list.append(target_image.cpu().numpy().squeeze())\n",
    "                \n",
    "            \n",
    "            # Plot comparison for the first 5 images\n",
    "            num_images = min(5, inputs.size(0))\n",
    "            fig, axes = plt.subplots(num_images, 4, figsize=(20, 5 * num_images))\n",
    "            \n",
    "            for j in range(num_images):\n",
    "                ax = axes[j, 0]\n",
    "                ax.imshow(inputs_list[j], cmap='gray')\n",
    "                ax.set_title(f'Input 64x64 Image {j+1}')\n",
    "                ax.axis('off')\n",
    "\n",
    "                ax = axes[j, 1]\n",
    "                ax.imshow(interpolated_list[j], cmap='gray')\n",
    "                ax.set_title(f'Interpolated 128x128 Image {j+1}')\n",
    "                ax.axis('off')\n",
    "                \n",
    "                ax = axes[j, 2]\n",
    "                ax.imshow(upscaled_list[j], cmap='gray')\n",
    "                ax.set_title(f'Upscaled 128x128 Image {j+1}')\n",
    "                ax.axis('off')\n",
    "\n",
    "                ax = axes[j, 3]\n",
    "                ax.imshow(targets_list[j], cmap='gray')\n",
    "                ax.set_title(f'Target 128x128 Image {j+1}')\n",
    "                ax.axis('off')\n",
    "            \n",
    "            save_path = os.path.join(save_dir, f'comparison_epoch_{epoch+1}.png')\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "            print(f'Comparison images saved at {save_path}')\n",
    "            break  # Only process the first batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01648d3",
   "metadata": {},
   "source": [
    "##### 6. Training routine for the Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ffb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diffusion_model(ddpm, train_loader, valid_loader, epochs=10, save_interval=10, checkpoint_path='cascadedddpm_checkpoint.pth'):\n",
    "    optimizer = optim.AdamW(ddpm.parameters(), lr=1e-6, weight_decay=1e-2)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "    start_epoch = 0\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        ddpm, start_epoch = load_model(ddpm, checkpoint_path)\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "        print(f\"Resuming training from epoch {start_epoch+1}\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}, starting from scratch.\")\n",
    "\n",
    "    ddpm.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        ddpm.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs = batch['data_64'].to(device)\n",
    "            targets = batch['data_128'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            t = ddpm.sample_timesteps(inputs.size(0))\n",
    "            loss = ddpm.p_losses(inputs, targets, t)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient checking\n",
    "            total_norm = 0\n",
    "            for p in ddpm.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "\n",
    "            if total_norm > 1e3:  # Threshold for exploding gradients\n",
    "                print(f\"Warning: Exploding gradients detected at epoch {epoch+1}, total norm: {total_norm:.2f}\")\n",
    "                continue\n",
    "            if total_norm < 1e-3:  # Threshold for vanishing gradients\n",
    "                print(f\"Warning: Vanishing gradients detected at epoch {epoch+1}, total norm: {total_norm:.2f}\")\n",
    "                continue\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Gradient Norm: {total_norm:.2f}')\n",
    "\n",
    "        ddpm.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                inputs = batch['data_64'].to(device)\n",
    "                targets = batch['data_128'].to(device)\n",
    "                t = ddpm.sample_timesteps(inputs.size(0))\n",
    "                loss = ddpm.p_losses(inputs, targets, t)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        avg_valid_loss = valid_loss / len(valid_loader)\n",
    "        valid_losses.append(avg_valid_loss)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_valid_loss:.4f}')\n",
    "\n",
    "        # Step the scheduler with the validation loss\n",
    "        scheduler.step(avg_valid_loss)\n",
    "\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            save_model(ddpm, epoch, checkpoint_path)\n",
    "            compare_and_save_images(ddpm, valid_loader, epoch)\n",
    "            print(f'Model saved at epoch {epoch+1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8b053",
   "metadata": {},
   "source": [
    "##### 7. Running the Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64b3455c-ccd2-484a-b93d-77a634be577e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found at cascadedddpm64NOSWIN_checkpoint.pth, starting from scratch.\n",
      "Epoch [1/100], Train Loss: 0.2075, Gradient Norm: 1.32\n",
      "Epoch [1/100], Validation Loss: 0.0295\n",
      "Epoch [2/100], Train Loss: 0.0174, Gradient Norm: 0.23\n",
      "Epoch [2/100], Validation Loss: 0.0100\n",
      "Epoch [3/100], Train Loss: 0.0103, Gradient Norm: 1.04\n",
      "Epoch [3/100], Validation Loss: 0.0108\n",
      "Epoch [4/100], Train Loss: 0.0079, Gradient Norm: 0.64\n",
      "Epoch [4/100], Validation Loss: 0.0080\n",
      "Epoch [5/100], Train Loss: 0.0075, Gradient Norm: 0.55\n",
      "Epoch [5/100], Validation Loss: 0.0067\n",
      "Epoch [6/100], Train Loss: 0.0064, Gradient Norm: 0.15\n",
      "Epoch [6/100], Validation Loss: 0.0080\n",
      "Epoch [7/100], Train Loss: 0.0069, Gradient Norm: 0.05\n",
      "Epoch [7/100], Validation Loss: 0.0047\n",
      "Epoch [8/100], Train Loss: 0.0057, Gradient Norm: 0.62\n",
      "Epoch [8/100], Validation Loss: 0.0047\n",
      "Epoch [9/100], Train Loss: 0.0061, Gradient Norm: 0.19\n",
      "Epoch [9/100], Validation Loss: 0.0053\n",
      "Epoch [10/100], Train Loss: 0.0056, Gradient Norm: 0.50\n",
      "Epoch [10/100], Validation Loss: 0.0044\n",
      "Model checkpoint saved at cascadedddpm64NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/64to128NOSWIN/comparison_epoch_10.png\n",
      "Model saved at epoch 10\n",
      "Epoch [11/100], Train Loss: 0.0053, Gradient Norm: 0.61\n",
      "Epoch [11/100], Validation Loss: 0.0048\n",
      "Epoch [12/100], Train Loss: 0.0047, Gradient Norm: 0.09\n",
      "Epoch [12/100], Validation Loss: 0.0042\n",
      "Epoch [13/100], Train Loss: 0.0057, Gradient Norm: 0.10\n",
      "Epoch [13/100], Validation Loss: 0.0057\n",
      "Epoch [14/100], Train Loss: 0.0043, Gradient Norm: 0.37\n",
      "Epoch [14/100], Validation Loss: 0.0042\n",
      "Epoch [15/100], Train Loss: 0.0045, Gradient Norm: 0.09\n",
      "Epoch [15/100], Validation Loss: 0.0038\n",
      "Epoch [16/100], Train Loss: 0.0042, Gradient Norm: 0.30\n",
      "Epoch [16/100], Validation Loss: 0.0049\n",
      "Epoch [17/100], Train Loss: 0.0059, Gradient Norm: 0.88\n",
      "Epoch [17/100], Validation Loss: 0.0048\n",
      "Epoch [18/100], Train Loss: 0.0038, Gradient Norm: 0.19\n",
      "Epoch [18/100], Validation Loss: 0.0044\n",
      "Epoch [19/100], Train Loss: 0.0039, Gradient Norm: 0.65\n",
      "Epoch [19/100], Validation Loss: 0.0040\n",
      "Epoch [20/100], Train Loss: 0.0040, Gradient Norm: 0.17\n",
      "Epoch [20/100], Validation Loss: 0.0036\n",
      "Model checkpoint saved at cascadedddpm64NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/64to128NOSWIN/comparison_epoch_20.png\n",
      "Model saved at epoch 20\n",
      "Epoch [21/100], Train Loss: 0.0043, Gradient Norm: 0.49\n",
      "Epoch [21/100], Validation Loss: 0.0037\n",
      "Epoch [22/100], Train Loss: 0.0034, Gradient Norm: 1.03\n",
      "Epoch [22/100], Validation Loss: 0.0041\n",
      "Epoch [23/100], Train Loss: 0.0037, Gradient Norm: 0.56\n",
      "Epoch [23/100], Validation Loss: 0.0039\n",
      "Epoch [24/100], Train Loss: 0.0036, Gradient Norm: 0.15\n",
      "Epoch [24/100], Validation Loss: 0.0031\n",
      "Epoch [25/100], Train Loss: 0.0049, Gradient Norm: 0.43\n",
      "Epoch [25/100], Validation Loss: 0.0031\n",
      "Epoch [26/100], Train Loss: 0.0038, Gradient Norm: 0.42\n",
      "Epoch [26/100], Validation Loss: 0.0041\n",
      "Epoch [27/100], Train Loss: 0.0036, Gradient Norm: 0.03\n",
      "Epoch [27/100], Validation Loss: 0.0043\n",
      "Epoch [28/100], Train Loss: 0.0034, Gradient Norm: 0.33\n",
      "Epoch [28/100], Validation Loss: 0.0037\n",
      "Epoch [29/100], Train Loss: 0.0037, Gradient Norm: 0.70\n",
      "Epoch [29/100], Validation Loss: 0.0065\n",
      "Epoch [30/100], Train Loss: 0.0040, Gradient Norm: 0.10\n",
      "Epoch [30/100], Validation Loss: 0.0032\n",
      "Model checkpoint saved at cascadedddpm64NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/64to128NOSWIN/comparison_epoch_30.png\n",
      "Model saved at epoch 30\n",
      "Epoch [31/100], Train Loss: 0.0032, Gradient Norm: 0.70\n",
      "Epoch [31/100], Validation Loss: 0.0033\n",
      "Epoch [32/100], Train Loss: 0.0039, Gradient Norm: 0.19\n",
      "Epoch [32/100], Validation Loss: 0.0036\n",
      "Epoch [33/100], Train Loss: 0.0033, Gradient Norm: 0.23\n",
      "Epoch [33/100], Validation Loss: 0.0051\n",
      "Epoch [34/100], Train Loss: 0.0032, Gradient Norm: 0.39\n",
      "Epoch [34/100], Validation Loss: 0.0035\n",
      "Epoch [35/100], Train Loss: 0.0055, Gradient Norm: 0.29\n",
      "Epoch [35/100], Validation Loss: 0.0027\n",
      "Epoch [36/100], Train Loss: 0.0034, Gradient Norm: 0.22\n",
      "Epoch [36/100], Validation Loss: 0.0042\n",
      "Epoch [37/100], Train Loss: 0.0034, Gradient Norm: 0.14\n",
      "Epoch [37/100], Validation Loss: 0.0043\n",
      "Epoch [38/100], Train Loss: 0.0037, Gradient Norm: 0.66\n",
      "Epoch [38/100], Validation Loss: 0.0033\n",
      "Epoch [39/100], Train Loss: 0.0030, Gradient Norm: 0.24\n",
      "Epoch [39/100], Validation Loss: 0.0030\n",
      "Epoch [40/100], Train Loss: 0.0029, Gradient Norm: 0.21\n",
      "Epoch [40/100], Validation Loss: 0.0037\n",
      "Model checkpoint saved at cascadedddpm64NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/64to128NOSWIN/comparison_epoch_40.png\n",
      "Model saved at epoch 40\n",
      "Epoch [41/100], Train Loss: 0.0031, Gradient Norm: 0.20\n",
      "Epoch [41/100], Validation Loss: 0.0032\n",
      "Epoch [42/100], Train Loss: 0.0033, Gradient Norm: 0.12\n",
      "Epoch [42/100], Validation Loss: 0.0038\n",
      "Epoch [43/100], Train Loss: 0.0032, Gradient Norm: 0.08\n",
      "Epoch [43/100], Validation Loss: 0.0032\n",
      "Epoch [44/100], Train Loss: 0.0031, Gradient Norm: 0.25\n",
      "Epoch [44/100], Validation Loss: 0.0031\n",
      "Epoch [45/100], Train Loss: 0.0025, Gradient Norm: 0.14\n",
      "Epoch [45/100], Validation Loss: 0.0029\n",
      "Epoch [46/100], Train Loss: 0.0029, Gradient Norm: 0.33\n",
      "Epoch [46/100], Validation Loss: 0.0032\n",
      "Epoch 00046: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch [47/100], Train Loss: 0.0024, Gradient Norm: 0.50\n",
      "Epoch [47/100], Validation Loss: 0.0025\n",
      "Epoch [48/100], Train Loss: 0.0023, Gradient Norm: 0.09\n",
      "Epoch [48/100], Validation Loss: 0.0027\n",
      "Epoch [49/100], Train Loss: 0.0026, Gradient Norm: 0.04\n",
      "Epoch [49/100], Validation Loss: 0.0032\n",
      "Epoch [50/100], Train Loss: 0.0024, Gradient Norm: 0.53\n",
      "Epoch [50/100], Validation Loss: 0.0022\n",
      "Model checkpoint saved at cascadedddpm64NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/64to128NOSWIN/comparison_epoch_50.png\n",
      "Model saved at epoch 50\n",
      "Epoch [51/100], Train Loss: 0.0025, Gradient Norm: 0.08\n",
      "Epoch [51/100], Validation Loss: 0.0024\n",
      "Epoch [52/100], Train Loss: 0.0025, Gradient Norm: 0.03\n",
      "Epoch [52/100], Validation Loss: 0.0023\n",
      "Epoch [53/100], Train Loss: 0.0027, Gradient Norm: 0.20\n",
      "Epoch [53/100], Validation Loss: 0.0028\n",
      "Epoch [54/100], Train Loss: 0.0025, Gradient Norm: 0.33\n",
      "Epoch [54/100], Validation Loss: 0.0028\n",
      "Epoch [55/100], Train Loss: 0.0023, Gradient Norm: 0.16\n",
      "Epoch [55/100], Validation Loss: 0.0020\n",
      "Epoch [56/100], Train Loss: 0.0025, Gradient Norm: 0.17\n",
      "Epoch [56/100], Validation Loss: 0.0016\n",
      "Epoch [57/100], Train Loss: 0.0024, Gradient Norm: 0.15\n",
      "Epoch [57/100], Validation Loss: 0.0024\n",
      "Epoch [58/100], Train Loss: 0.0025, Gradient Norm: 0.45\n",
      "Epoch [58/100], Validation Loss: 0.0022\n",
      "Epoch [59/100], Train Loss: 0.0021, Gradient Norm: 0.40\n",
      "Epoch [59/100], Validation Loss: 0.0021\n",
      "Epoch [60/100], Train Loss: 0.0022, Gradient Norm: 0.21\n",
      "Epoch [60/100], Validation Loss: 0.0020\n",
      "Model checkpoint saved at cascadedddpm64NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/64to128NOSWIN/comparison_epoch_60.png\n",
      "Model saved at epoch 60\n",
      "Epoch [61/100], Train Loss: 0.0025, Gradient Norm: 0.23\n",
      "Epoch [61/100], Validation Loss: 0.0019\n",
      "Epoch [62/100], Train Loss: 0.0024, Gradient Norm: 0.16\n",
      "Epoch [62/100], Validation Loss: 0.0025\n",
      "Epoch [63/100], Train Loss: 0.0021, Gradient Norm: 0.53\n",
      "Epoch [63/100], Validation Loss: 0.0026\n",
      "Epoch [64/100], Train Loss: 0.0024, Gradient Norm: 0.24\n",
      "Epoch [64/100], Validation Loss: 0.0017\n",
      "Epoch [65/100], Train Loss: 0.0025, Gradient Norm: 0.13\n",
      "Epoch [65/100], Validation Loss: 0.0020\n",
      "Epoch [66/100], Train Loss: 0.0023, Gradient Norm: 0.08\n",
      "Epoch [66/100], Validation Loss: 0.0020\n",
      "Epoch [67/100], Train Loss: 0.0020, Gradient Norm: 0.14\n",
      "Epoch [67/100], Validation Loss: 0.0021\n",
      "Epoch 00067: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch [68/100], Train Loss: 0.0022, Gradient Norm: 0.03\n",
      "Epoch [68/100], Validation Loss: 0.0017\n",
      "Epoch [69/100], Train Loss: 0.0020, Gradient Norm: 0.04\n",
      "Epoch [69/100], Validation Loss: 0.0021\n",
      "Epoch [70/100], Train Loss: 0.0021, Gradient Norm: 0.12\n",
      "Epoch [70/100], Validation Loss: 0.0019\n",
      "Model checkpoint saved at cascadedddpm64NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/64to128NOSWIN/comparison_epoch_70.png\n",
      "Model saved at epoch 70\n",
      "Epoch [71/100], Train Loss: 0.0020, Gradient Norm: 0.07\n",
      "Epoch [71/100], Validation Loss: 0.0025\n",
      "Epoch [72/100], Train Loss: 0.0022, Gradient Norm: 0.31\n",
      "Epoch [72/100], Validation Loss: 0.0022\n",
      "Epoch [73/100], Train Loss: 0.0020, Gradient Norm: 0.34\n",
      "Epoch [73/100], Validation Loss: 0.0018\n",
      "Epoch [74/100], Train Loss: 0.0020, Gradient Norm: 0.02\n",
      "Epoch [74/100], Validation Loss: 0.0020\n",
      "Epoch [75/100], Train Loss: 0.0020, Gradient Norm: 0.02\n",
      "Epoch [75/100], Validation Loss: 0.0018\n",
      "Epoch [76/100], Train Loss: 0.0019, Gradient Norm: 0.09\n",
      "Epoch [76/100], Validation Loss: 0.0022\n",
      "Epoch [77/100], Train Loss: 0.0022, Gradient Norm: 0.12\n",
      "Epoch [77/100], Validation Loss: 0.0019\n",
      "Epoch [78/100], Train Loss: 0.0020, Gradient Norm: 0.12\n",
      "Epoch [78/100], Validation Loss: 0.0024\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch [79/100], Train Loss: 0.0020, Gradient Norm: 0.03\n",
      "Epoch [79/100], Validation Loss: 0.0017\n",
      "Epoch [80/100], Train Loss: 0.0021, Gradient Norm: 0.22\n",
      "Epoch [80/100], Validation Loss: 0.0018\n",
      "Model checkpoint saved at cascadedddpm64NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/64to128NOSWIN/comparison_epoch_80.png\n",
      "Model saved at epoch 80\n",
      "Epoch [81/100], Train Loss: 0.0019, Gradient Norm: 0.08\n",
      "Epoch [81/100], Validation Loss: 0.0016\n",
      "Epoch [82/100], Train Loss: 0.0019, Gradient Norm: 0.30\n",
      "Epoch [82/100], Validation Loss: 0.0020\n",
      "Epoch [83/100], Train Loss: 0.0019, Gradient Norm: 0.07\n",
      "Epoch [83/100], Validation Loss: 0.0021\n",
      "Epoch [84/100], Train Loss: 0.0020, Gradient Norm: 0.05\n",
      "Epoch [84/100], Validation Loss: 0.0022\n",
      "Epoch [85/100], Train Loss: 0.0018, Gradient Norm: 0.12\n",
      "Epoch [85/100], Validation Loss: 0.0015\n",
      "Epoch [86/100], Train Loss: 0.0020, Gradient Norm: 0.03\n",
      "Epoch [86/100], Validation Loss: 0.0017\n",
      "Epoch [87/100], Train Loss: 0.0022, Gradient Norm: 0.12\n",
      "Epoch [87/100], Validation Loss: 0.0021\n",
      "Epoch [88/100], Train Loss: 0.0022, Gradient Norm: 0.03\n",
      "Epoch [88/100], Validation Loss: 0.0019\n",
      "Epoch [89/100], Train Loss: 0.0019, Gradient Norm: 0.07\n",
      "Epoch [89/100], Validation Loss: 0.0023\n",
      "Epoch [90/100], Train Loss: 0.0021, Gradient Norm: 0.06\n",
      "Epoch [90/100], Validation Loss: 0.0019\n",
      "Model checkpoint saved at cascadedddpm64NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/64to128NOSWIN/comparison_epoch_90.png\n",
      "Model saved at epoch 90\n",
      "Epoch [91/100], Train Loss: 0.0020, Gradient Norm: 0.37\n",
      "Epoch [91/100], Validation Loss: 0.0019\n",
      "Epoch [92/100], Train Loss: 0.0019, Gradient Norm: 0.06\n",
      "Epoch [92/100], Validation Loss: 0.0019\n",
      "Epoch [93/100], Train Loss: 0.0020, Gradient Norm: 0.17\n",
      "Epoch [93/100], Validation Loss: 0.0017\n",
      "Epoch [94/100], Train Loss: 0.0018, Gradient Norm: 0.05\n",
      "Epoch [94/100], Validation Loss: 0.0016\n",
      "Epoch [95/100], Train Loss: 0.0021, Gradient Norm: 0.19\n",
      "Epoch [95/100], Validation Loss: 0.0017\n",
      "Epoch [96/100], Train Loss: 0.0019, Gradient Norm: 0.15\n",
      "Epoch [96/100], Validation Loss: 0.0021\n",
      "Epoch 00096: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch [97/100], Train Loss: 0.0017, Gradient Norm: 0.09\n",
      "Epoch [97/100], Validation Loss: 0.0020\n",
      "Epoch [98/100], Train Loss: 0.0018, Gradient Norm: 0.07\n",
      "Epoch [98/100], Validation Loss: 0.0023\n",
      "Epoch [99/100], Train Loss: 0.0019, Gradient Norm: 0.18\n",
      "Epoch [99/100], Validation Loss: 0.0016\n",
      "Epoch [100/100], Train Loss: 0.0018, Gradient Norm: 0.03\n",
      "Epoch [100/100], Validation Loss: 0.0019\n",
      "Model checkpoint saved at cascadedddpm64NOSWIN_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/64to128NOSWIN/comparison_epoch_100.png\n",
      "Model saved at epoch 100\n"
     ]
    }
   ],
   "source": [
    "# Initialize the UNet model and DDPM\n",
    "in_channels = 2  # For grayscale images\n",
    "out_channels = 1  # For grayscale images\n",
    "emb_dim = 128\n",
    "num_timesteps = 1000\n",
    "\n",
    "unet = SuperResUNet(in_channels, out_channels, emb_dim).to(device)\n",
    "ddpm = SuperResDDPM(unet, num_timesteps).to(device)\n",
    "\n",
    "# Train the model\n",
    "train_diffusion_model(ddpm, train_loader, valid_loader, epochs=100, save_interval=10, checkpoint_path='Model_Savepoints/cascadedddpm64NOSWIN_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a98c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6971e05b",
   "metadata": {},
   "source": [
    "##### 8. Testing the model out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94b693f9-1565-4d8c-9e5c-aca323a69569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 64, 64])\n",
      "torch.Size([1, 1, 64, 64])\n",
      "torch.Size([1, 1, 128, 128])\n",
      "torch.Size([1, 1, 128, 128])\n",
      "Upscaled comparison image saved at generated_images/upscaled_from_validation/upscaled_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# Initialize the UNet model and DDPM\n",
    "in_channels = 2  # For grayscale images\n",
    "out_channels = 1  # For grayscale images\n",
    "emb_dim = 128\n",
    "num_timesteps = 1000\n",
    "\n",
    "unet = SuperResUNet(in_channels, out_channels, emb_dim).to(device)\n",
    "ddpm = SuperResDDPM(unet, num_timesteps).to(device)\n",
    "def load_model_and_upscale_from_validation(ddpm, checkpoint_path, valid_loader, save_dir='generated_images/upscaled_from_validation'):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    ddpm.load_state_dict(checkpoint['model_state_dict'])\n",
    "    ddpm.to(device)\n",
    "    ddpm.eval()\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            inputs = batch['data_64'].to(device)\n",
    "            targets = batch['data_128'].to(device)\n",
    "            print(inputs.shape)\n",
    "            # Select the first image in the batch\n",
    "            input_image = inputs[0].unsqueeze(0)\n",
    "            target_image = targets[0].unsqueeze(0)\n",
    "            print(input_image.shape)\n",
    "            # Interpolate the low-resolution image\n",
    "            interpolated_image = F.interpolate(input_image, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            print(interpolated_image.shape)\n",
    "            # Upscale the image using the DDPM model\n",
    "            upscaled_image = ddpm.sample(input_image)\n",
    "            print(upscaled_image.shape)\n",
    "            # Convert to numpy arrays for visualization\n",
    "            input_image_np = input_image.cpu().numpy().squeeze()\n",
    "            interpolated_image_np = interpolated_image.cpu().numpy().squeeze()\n",
    "            upscaled_image_np = upscaled_image.cpu().numpy().squeeze()\n",
    "            target_image_np = target_image.cpu().numpy().squeeze()\n",
    "            \n",
    "            # Plot and save the images for comparison\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "            \n",
    "            axes[0].imshow(input_image_np, cmap='gray')\n",
    "            axes[0].set_title('Input 64x64 Image')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            axes[1].imshow(interpolated_image_np, cmap='gray')\n",
    "            axes[1].set_title('Interpolated 128x128 Image')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            axes[2].imshow(upscaled_image_np, cmap='gray')\n",
    "            axes[2].set_title('Upscaled 128x128 Image')\n",
    "            axes[2].axis('off')\n",
    "            \n",
    "            axes[3].imshow(target_image_np, cmap='gray')\n",
    "            axes[3].set_title('Target 128x128 Image')\n",
    "            axes[3].axis('off')\n",
    "            \n",
    "            save_path = os.path.join(save_dir, 'upscaled_comparison.png')\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "            print(f'Upscaled comparison image saved at {save_path}')\n",
    "            break  # Only process the first batch\n",
    "\n",
    "\n",
    "load_model_and_upscale_from_validation(ddpm, 'cascadedddpm64(2)100epochs_checkpoint.pth', valid_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed09d04-8542-4c63-929c-cd448f26637b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
