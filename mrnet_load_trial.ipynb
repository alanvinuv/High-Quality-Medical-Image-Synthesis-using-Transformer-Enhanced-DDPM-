{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook provides a pipeline for preprocessing, visualizing, and saving MRI slices from the MRNet dataset, focusing on the coronal plane. \n",
    "\n",
    "Key steps include:\n",
    "\n",
    "- **Data Preprocessing**: MRI slices are normalized to the range [-1, 1] and resized to 256x256 pixels.\n",
    "- **Dataset Construction**: A custom PyTorch Dataset class loads, processes, and extracts slices from the center of MRI volumes.\n",
    "- **Slice Counting**: The total number of slices across tasks (ACL, abnormal, meniscus) is counted.\n",
    "- **Saving Preprocessed Slices**: Processed slices are saved as PNG images for both training and validation sets, facilitating their use in model training.\n",
    "\n",
    "This workflow standardizes MRI data, making it ready for machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchio as tio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Taking Count of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: acl\n",
      "Total number of slices in training set: 1040\n",
      "Total number of slices in validation set: 270\n",
      "Task: abnormal\n",
      "Total number of slices in training set: 4565\n",
      "Total number of slices in validation set: 475\n",
      "Task: meniscus\n",
      "Total number of slices in training set: 1985\n",
      "Total number of slices in validation set: 260\n",
      "Total number of slices across all tasks in training set: 7590\n",
      "Total number of slices across all tasks in validation set: 1005\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define preprocessing transformations\n",
    "preprocessing_transforms = tio.Compose([\n",
    "    tio.RescaleIntensity(out_min_max=(-1, 1)),  # Normalize intensity to [-1, 1]\n",
    "    tio.Resize((256, 256, 1))  # Resize to 256x256 and add dimension for z axis\n",
    "])\n",
    "\n",
    "class MRNetSinglePlaneDataset(Dataset):\n",
    "    def __init__(self, root_dir, task, plane='coronal', split='train', preprocessing_transforms=None):\n",
    "        super().__init__()\n",
    "        self.task = task\n",
    "        self.plane = plane\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.preprocessing_transforms = preprocessing_transforms\n",
    "\n",
    "        # Load labels\n",
    "        self.records = self._get_annotations()\n",
    "        self.records['id'] = self._remap_id_to_match_folder_name()\n",
    "        self.labels = self.records['label'].tolist()\n",
    "\n",
    "        # Filter to include only positive cases for the task\n",
    "        self.records = self.records[self.records['label'] == 1]\n",
    "        self.paths = self._get_file_paths()\n",
    "\n",
    "    def _get_file_paths(self):\n",
    "        file_paths = []\n",
    "        for filename in self.records['id'].tolist():\n",
    "            plane_path = os.path.join(self.root_dir, self.split, self.plane, f'{filename}.npy')\n",
    "            file_paths.append(plane_path)\n",
    "        return file_paths\n",
    "\n",
    "    def _remap_id_to_match_folder_name(self):\n",
    "        return self.records['id'].map(lambda i: '0' * (4 - len(str(i))) + str(i))\n",
    "\n",
    "    def _get_annotations(self):\n",
    "        csv_file = os.path.join(self.root_dir, f'{self.split}-{self.task}.csv')\n",
    "        records = pd.read_csv(csv_file, header=None, names=['id', 'label'])\n",
    "        return records\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load image data and label\n",
    "        plane_path = self.paths[index]\n",
    "        volume = np.load(plane_path).astype('float32')\n",
    "\n",
    "        # Calculate slice indices to get 5 slices from the center of the volume\n",
    "        num_slices = volume.shape[0]\n",
    "        center_slice = num_slices // 2\n",
    "        slice_indices = np.linspace(center_slice - 2, center_slice + 2, 5, dtype=int)\n",
    "\n",
    "        # Extract and preprocess slices\n",
    "        slices = []\n",
    "        for i in slice_indices:\n",
    "            slice_data = volume[i]  # Get the ith slice\n",
    "            slice_data = slice_data[None, :, :]  # Add channel dimension (1, H, W)\n",
    "            slice_data = slice_data[..., None]  # Add z dimension (1, H, W, 1)\n",
    "            slice_data = self.preprocessing_transforms(slice_data)  # Apply preprocessing\n",
    "            slice_data = slice_data.squeeze(-1)  # Remove the z dimension after preprocessing\n",
    "            slice_data = torch.tensor(slice_data)  # Convert to PyTorch tensor\n",
    "            slices.append(slice_data)\n",
    "\n",
    "        # Stack slices into a single tensor\n",
    "        data = torch.stack(slices)\n",
    "\n",
    "        label = self.labels[index]\n",
    "        label = torch.FloatTensor([label])\n",
    "\n",
    "        # Sample identifier\n",
    "        id = plane_path.split(os.sep)[-1].split('.')[0]\n",
    "\n",
    "        return {'data': data, 'label': label, 'id': id}\n",
    "\n",
    "# Paths and parameters\n",
    "root_dir = \"MRnet-v1.0\"\n",
    "tasks = ['acl', 'abnormal', 'meniscus']  # List of tasks\n",
    "plane = 'coronal'  # Focus on coronal plane\n",
    "\n",
    "# Function to count total slices in a dataset\n",
    "def count_total_slices(dataset):\n",
    "    total_slices = 0\n",
    "    for i in range(len(dataset)):\n",
    "        sample = dataset[i]\n",
    "        total_slices += sample['data'].shape[0]\n",
    "    return total_slices\n",
    "\n",
    "# Create and count slices for each task\n",
    "total_train_slices = 0\n",
    "total_valid_slices = 0\n",
    "\n",
    "for task in tasks:\n",
    "    # Create the dataset objects for train and valid splits\n",
    "    train_dataset = MRNetSinglePlaneDataset(root_dir=root_dir, task=task, plane=plane, split='train', preprocessing_transforms=preprocessing_transforms)\n",
    "    valid_dataset = MRNetSinglePlaneDataset(root_dir=root_dir, task=task, plane=plane, split='valid', preprocessing_transforms=preprocessing_transforms)\n",
    "\n",
    "    # Count the total number of slices in the training and validation sets\n",
    "    train_slices = count_total_slices(train_dataset)\n",
    "    valid_slices = count_total_slices(valid_dataset)\n",
    "\n",
    "    total_train_slices += train_slices\n",
    "    total_valid_slices += valid_slices\n",
    "\n",
    "    print(f\"Task: {task}\")\n",
    "    print(f\"Total number of slices in training set: {train_slices}\")\n",
    "    print(f\"Total number of slices in validation set: {valid_slices}\")\n",
    "\n",
    "print(f\"Total number of slices across all tasks in training set: {total_train_slices}\")\n",
    "print(f\"Total number of slices across all tasks in validation set: {total_valid_slices}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving Slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: acl\n",
      "Total number of slices in training set: 1040\n",
      "Total number of slices in validation set: 270\n",
      "Saving 1040 slices for training set of task acl...\n",
      "Saving 270 slices for validation set of task acl...\n",
      "Task: abnormal\n",
      "Total number of slices in training set: 4565\n",
      "Total number of slices in validation set: 475\n",
      "Saving 4565 slices for training set of task abnormal...\n",
      "Saving 475 slices for validation set of task abnormal...\n",
      "Task: meniscus\n",
      "Total number of slices in training set: 1985\n",
      "Total number of slices in validation set: 260\n",
      "Saving 1985 slices for training set of task meniscus...\n",
      "Saving 260 slices for validation set of task meniscus...\n",
      "Total number of slices across all tasks in training set: 7590\n",
      "Total number of slices across all tasks in validation set: 1005\n",
      "Train slices saved to C:\\Users\\ASUS\\Documents\\Uobd\\project\\Datasets\\MRNet-v1.0\\train_slices_raw\n",
      "Valid slices saved to C:\\Users\\ASUS\\Documents\\Uobd\\project\\Datasets\\MRNet-v1.0\\valid_slices_raw\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to display images\n",
    "def show_slices(slices):\n",
    "    \"\"\" Function to display a row of image slices \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(slices), figsize=(15, 15))\n",
    "    for i, slice in enumerate(slices):\n",
    "        axes[i].imshow(slice.numpy().squeeze(), cmap=\"gray\")\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize images from the dataset\n",
    "def visualize_sample(dataset, index=2):\n",
    "    sample = dataset[index]\n",
    "    data = sample['data']  # Get the 2D slices\n",
    "    label = sample['label'].item()\n",
    "    id = sample['id']\n",
    "\n",
    "    # Print the label and ID\n",
    "    print(\"Sample ID:\", id)\n",
    "    print(\"Sample label:\", label)\n",
    "\n",
    "    # Select slices to display\n",
    "    slices = [data[i] for i in range(data.shape[0])]\n",
    "\n",
    "    # Display the slices\n",
    "    show_slices(slices)\n",
    "\n",
    "# Paths and parameters\n",
    "root_dir = \"MRnet-v1.0\"\n",
    "tasks = ['acl', 'abnormal', 'meniscus']  # List of tasks\n",
    "plane = 'coronal'  # Focus on coronal plane\n",
    "\n",
    "# Save images to a directory\n",
    "def save_slices(dataset, save_dir, task):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    for i in range(len(dataset)):\n",
    "        sample = dataset[i]\n",
    "        data = sample['data']\n",
    "        id = sample['id']\n",
    "        for j in range(data.shape[0]):\n",
    "            slice_image = data[j].numpy().squeeze()\n",
    "            plt.imsave(os.path.join(save_dir, f'{task}_{id}_slice_{j}.png'), slice_image, cmap='gray')\n",
    "\n",
    "# Create and count slices for each task\n",
    "total_train_slices = 0\n",
    "total_valid_slices = 0\n",
    "\n",
    "train_save_dir = os.path.join(root_dir, 'train_slices_raw')\n",
    "valid_save_dir = os.path.join(root_dir, 'valid_slices_raw')\n",
    "\n",
    "for task in tasks:\n",
    "    # Create the dataset objects for train and valid splits\n",
    "    train_dataset = MRNetSinglePlaneDataset(root_dir=root_dir, task=task, plane=plane, split='train', preprocessing_transforms=preprocessing_transforms)\n",
    "    valid_dataset = MRNetSinglePlaneDataset(root_dir=root_dir, task=task, plane=plane, split='valid', preprocessing_transforms=preprocessing_transforms)\n",
    "\n",
    "    # Count the total number of slices in the training and validation sets\n",
    "    train_slices = 0\n",
    "    valid_slices = 0\n",
    "\n",
    "    for i in range(len(train_dataset)):\n",
    "        train_slices += train_dataset[i]['data'].shape[0]\n",
    "\n",
    "    for i in range(len(valid_dataset)):\n",
    "        valid_slices += valid_dataset[i]['data'].shape[0]\n",
    "\n",
    "    total_train_slices += train_slices\n",
    "    total_valid_slices += valid_slices\n",
    "\n",
    "    print(f\"Task: {task}\")\n",
    "    print(f\"Total number of slices in training set: {train_slices}\")\n",
    "    print(f\"Total number of slices in validation set: {valid_slices}\")\n",
    "\n",
    "    # Save slices to their respective directories\n",
    "    print(f\"Saving {train_slices} slices for training set of task {task}...\")\n",
    "    save_slices(train_dataset, train_save_dir, task)\n",
    "    print(f\"Saving {valid_slices} slices for validation set of task {task}...\")\n",
    "    save_slices(valid_dataset, valid_save_dir, task)\n",
    "\n",
    "print(f\"Total number of slices across all tasks in training set: {total_train_slices}\")\n",
    "print(f\"Total number of slices across all tasks in validation set: {total_valid_slices}\")\n",
    "print(f\"Train slices saved to {train_save_dir}\")\n",
    "print(f\"Valid slices saved to {valid_save_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
