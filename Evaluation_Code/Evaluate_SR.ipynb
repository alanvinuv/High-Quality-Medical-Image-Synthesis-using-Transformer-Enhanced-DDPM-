{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e70f57",
   "metadata": {},
   "source": [
    "# Evaluation of Super-Resolution Models with and without Swin Transformer Integration\n",
    "\n",
    "This notebook is designed to evaluate the performance of super-resolution (SR) models applied to medical images, specifically MRI slices. The main objective is to compare the effectiveness of SR models enhanced with Swin Transformer blocks against traditional interpolation methods. The evaluation process involves upscaling low-resolution images (64x64) to high-resolution images (256x256) using a cascaded super-resolution approach. The models tested include a direct application of DDPM for upscaling, as well as models that incorporate Swin Transformer blocks within a UNet architecture.\n",
    "\n",
    "Key metrics for comparison include Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), Mean Squared Error (MSE), Feature Similarity Index (FSIM), Visual Information Fidelity (VIF), Learned Perceptual Image Patch Similarity (LPIPS), and Average Gradient (AG). These metrics provide a comprehensive assessment of the models' ability to enhance image quality while preserving critical medical image features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a848e5",
   "metadata": {},
   "source": [
    "##### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f677ff-0059-4f9e-a7bc-d8f41540e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.stats import entropy\n",
    "from scipy.linalg import sqrtm\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f7561",
   "metadata": {},
   "source": [
    "##### Implementing Swin Transformer and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e26210-6b61-4daa-8e99-abac1f3acf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "<https://arxiv.org/abs/2103.14030>\n",
    "https://github.com/microsoft/Swin-Transformer\n",
    "\"\"\"\n",
    "\n",
    "# DropPath (Stochastic Depth) module to implement drop path regularization\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "        return output\n",
    "\n",
    "# Helper functions to handle tuple and truncation\n",
    "def to_2tuple(x):\n",
    "    if isinstance(x, (tuple, list)):\n",
    "        return x\n",
    "    return (x, x)\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1.):\n",
    "    with torch.no_grad():\n",
    "        size = tensor.shape\n",
    "        tmp = tensor.new_empty(size + (4,)).normal_()\n",
    "        valid = (tmp < 2) & (tmp > -2)\n",
    "        ind = valid.max(-1, keepdim=True)[1]\n",
    "        tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
    "        tensor.data.mul_(std).add_(mean)\n",
    "        return tensor\n",
    "\n",
    "# MLP module used within Swin Transformer\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "# Functions to partition and reverse windows in the Swin Transformer\n",
    "def window_partition(x, window_size):\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.view(B, C, H // window_size, window_size, W // window_size, window_size)\n",
    "    windows = x.permute(0, 2, 4, 3, 5, 1).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 5, 1, 3, 2, 4).contiguous().view(B, -1, H, W)\n",
    "    return x\n",
    "\n",
    "# Window-based multi-head self-attention (W-MSA) module\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # Relative position bias table for all windows\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n",
    "\n",
    "        # Get relative position index for each window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1)\n",
    "        attn = attn + relative_position_bias.unsqueeze(0).to(attn.dtype)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "# Swin Transformer block implementing the shifted window-based attention mechanism\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * self.mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, out_features=dim, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            attn_mask = self.calculate_mask(self.input_resolution)\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def calculate_mask(self, x_size):\n",
    "        H, W = x_size\n",
    "        img_mask = torch.zeros((1, H, W, 1))\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size).view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        return attn_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, C = x.shape\n",
    "        H, W = self.input_resolution\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        x_windows = window_partition(shifted_x, self.window_size)\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
    "\n",
    "        if self.attn_mask is not None:\n",
    "            attn_windows = self.attn(x_windows, mask=self.attn_mask.to(x.dtype))\n",
    "        else:\n",
    "            attn_windows = self.attn(x_windows)\n",
    "\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3e4cf",
   "metadata": {},
   "source": [
    "##### Implementing Self, Cross Attention and Sinusoidal Positional Embedding Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb2f1cb-c4fa-4ecd-ac2f-cd576a23900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-attention block for feature refinement\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SelfAttentionBlock, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, width, height = x.size()\n",
    "        proj_query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(batch_size, -1, width * height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "        proj_value = self.value_conv(x).view(batch_size, -1, width * height)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, C, width, height)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "# Cross-attention block for the decoder to focus on relevant features\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(CrossAttentionBlock, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "# Sinusoidal positional embedding for timestep encoding in DDPM\n",
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=10000):\n",
    "        super(SinusoidalPositionalEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, timesteps):\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        emb = math.log(self.max_len) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n",
    "        emb = timesteps[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "        if self.embedding_dim % 2 == 1:\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=1)\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87ae78f",
   "metadata": {},
   "source": [
    "##### 64x64 UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17430fa5-71d2-4390-81bd-4eea248ca41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionUNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=128):\n",
    "        super(AttentionUNet, self).__init__()\n",
    "\n",
    "        self.encoder1 = self.conv_block(in_channels + emb_dim, 64)\n",
    "        self.self_attention1 = SelfAttentionBlock(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.self_attention2 = SelfAttentionBlock(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.self_attention3 = SelfAttentionBlock(256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "        self.swin_block = SwinTransformerBlock(dim=512, input_resolution=(8, 8), num_heads=8, window_size=4) \n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.cross_attention3 = CrossAttentionBlock(256, 256, 128)\n",
    "        self.decoder3 = self.conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.cross_attention2 = CrossAttentionBlock(128, 128, 64)\n",
    "        self.decoder2 = self.conv_block(256, 128)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.cross_attention1 = CrossAttentionBlock(64, 64, 32)\n",
    "        self.decoder1 = self.conv_block(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "        self.timestep_embedding_layer = SinusoidalPositionalEmbedding(emb_dim)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.timestep_embedding_layer(t)\n",
    "        t_embed = t_embed.view(t.size(0), -1, 1, 1)\n",
    "        t_embed = t_embed.repeat(1, 1, x.size(2), x.size(3))\n",
    "        x = torch.cat((x, t_embed), dim=1)\n",
    "\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc1 = self.self_attention1(enc1)\n",
    "        enc1_pooled = self.pool1(enc1)\n",
    "\n",
    "        enc2 = self.encoder2(enc1_pooled)\n",
    "        enc2 = self.self_attention2(enc2)\n",
    "        enc2_pooled = self.pool2(enc2)\n",
    "\n",
    "        enc3 = self.encoder3(enc2_pooled)\n",
    "        enc3 = self.self_attention3(enc3)\n",
    "        enc3_pooled = self.pool3(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc3_pooled)\n",
    "        B, C, H, W = bottleneck.shape\n",
    "        bottleneck = bottleneck.view(B, H * W, C)\n",
    "        bottleneck = self.swin_block(bottleneck)\n",
    "        bottleneck = bottleneck.view(B, C, H, W)\n",
    "\n",
    "        # Decoder\n",
    "        upconv3 = self.upconv3(bottleneck)\n",
    "        enc3 = self.cross_attention3(upconv3, enc3)\n",
    "        dec3 = torch.cat((upconv3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        upconv2 = self.upconv2(dec3)\n",
    "        enc2 = self.cross_attention2(upconv2, enc2)\n",
    "        dec2 = torch.cat((upconv2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        upconv1 = self.upconv1(dec2)\n",
    "        enc1 = self.cross_attention1(upconv1, enc1)\n",
    "        dec1 = torch.cat((upconv1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        final_output = self.final_conv(dec1)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f414079d",
   "metadata": {},
   "source": [
    "##### 64x64 to 128x128 UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e95b21b5-0cca-4f18-8c93-4f9ca28b56b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the UNet model with attention\n",
    "class SuperResUNet128(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=128):\n",
    "        super(SuperResUNet128, self).__init__()\n",
    "\n",
    "        self.encoder1 = self.conv_block(in_channels + emb_dim, 64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "        self.swin_block = SwinTransformerBlock(dim=512, input_resolution=(16, 16), num_heads=8, window_size=4)\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self.conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.conv_block(256, 128)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self.conv_block(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "        self.timestep_embedding_layer = SinusoidalPositionalEmbedding(emb_dim)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.timestep_embedding_layer(t)\n",
    "        t_embed = t_embed.view(t.size(0), -1, 1, 1)\n",
    "        t_embed = t_embed.repeat(1, 1, x.size(2), x.size(3))\n",
    "\n",
    "        x = torch.cat((x, t_embed), dim=1)\n",
    "\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc1_pooled = self.pool1(enc1)\n",
    "\n",
    "        enc2 = self.encoder2(enc1_pooled)\n",
    "        enc2_pooled = self.pool2(enc2)\n",
    "\n",
    "        enc3 = self.encoder3(enc2_pooled)\n",
    "        enc3_pooled = self.pool3(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc3_pooled)\n",
    "        B, C, H, W = bottleneck.shape\n",
    "        bottleneck = bottleneck.view(B, H * W, C)\n",
    "        bottleneck = self.swin_block(bottleneck)\n",
    "        bottleneck = bottleneck.view(B, C, H, W)\n",
    "\n",
    "        # Decoder\n",
    "        upconv3 = self.upconv3(bottleneck)\n",
    "        dec3 = torch.cat((upconv3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        upconv2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((upconv2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        upconv1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((upconv1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        final_output = self.final_conv(dec1)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7827cb34",
   "metadata": {},
   "source": [
    "##### 128x128 to 256x256 UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "668a90bc-edd1-4a21-91d3-0fb89f5f0cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SuperResUNet256(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=128):\n",
    "        super(SuperResUNet256, self).__init__()\n",
    "\n",
    "        self.encoder1 = self.conv_block(in_channels + emb_dim, 64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "        self.swin_block = SwinTransformerBlock(dim=512, input_resolution=(32, 32), num_heads=8, window_size=4)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self.conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.conv_block(256, 128)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self.conv_block(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "        self.timestep_embedding_layer = SinusoidalPositionalEmbedding(emb_dim)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.timestep_embedding_layer(t)\n",
    "        t_embed = t_embed.view(t.size(0), -1, 1, 1)\n",
    "        t_embed = t_embed.repeat(1, 1, x.size(2), x.size(3))\n",
    "\n",
    "        x = torch.cat((x, t_embed), dim=1)\n",
    "\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc1_pooled = self.pool1(enc1)\n",
    "\n",
    "        enc2 = self.encoder2(enc1_pooled)\n",
    "        enc2_pooled = self.pool2(enc2)\n",
    "\n",
    "        enc3 = self.encoder3(enc2_pooled)\n",
    "        enc3_pooled = self.pool3(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc3_pooled)\n",
    "        B, C, H, W = bottleneck.shape\n",
    "        bottleneck = bottleneck.view(B, H * W, C)\n",
    "        bottleneck = self.swin_block(bottleneck)\n",
    "        bottleneck = bottleneck.view(B, C, H, W)\n",
    "\n",
    "        # Decoder\n",
    "        upconv3 = self.upconv3(bottleneck)\n",
    "        dec3 = torch.cat((upconv3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        upconv2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((upconv2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        upconv1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((upconv1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        final_output = self.final_conv(dec1)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76686f2",
   "metadata": {},
   "source": [
    "##### Implementing DDPM and SuperRes DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0edf78b1-1943-44c6-b102-14d9188c03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DDPM implementation adapted from:\n",
    "https://github.com/hojonathanho/diffusion/tree/master\n",
    "\"\"\"\n",
    "\n",
    "class SuperResDDPM(nn.Module):\n",
    "    def __init__(self, model, num_timesteps, beta_start=0.00085, beta_end=0.0120):\n",
    "        super(SuperResDDPM, self).__init__()\n",
    "        self.model = model\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', 1 - betas)\n",
    "        self.register_buffer('alphas_cumprod', torch.cumprod(1 - betas, dim=0))\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(self.alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - self.alphas_cumprod))\n",
    "\n",
    "    def forward(self, z_t, t, low_res_image):\n",
    "        # Concatenate low_res_image with z_t to condition the model\n",
    "        low_res_upsampled = F.interpolate(low_res_image, scale_factor=2, mode='bicubic', align_corners=False)\n",
    "        return self.model(torch.cat([z_t, low_res_upsampled], dim=1), t)\n",
    "\n",
    "    def sample_timesteps(self, batch_size):\n",
    "        return torch.randint(0, self.num_timesteps, (batch_size,)).to(device)\n",
    "\n",
    "    def forward_diffusion(self, target_high_res_img, t, noise):\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        return sqrt_alphas_cumprod_t * target_high_res_img + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_losses(self, input_low_res_img, target_high_res_img, t):\n",
    "        sqrt_alpha_t = self.sqrt_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        sqrt_one_minus_alpha_t = self.sqrt_one_minus_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "        #input_high_res_img = F.interpolate(input_low_res_img, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        noise = torch.randn_like(target_high_res_img)\n",
    "        z_t = self.forward_diffusion(target_high_res_img, t, noise)\n",
    "\n",
    "        predicted_noise = self.forward(z_t, t, input_low_res_img)\n",
    "\n",
    "        return nn.MSELoss()(predicted_noise, noise)\n",
    "\n",
    "    def sample(self, low_res_image):\n",
    "        z_t = torch.randn_like(F.interpolate(low_res_image, scale_factor=2, mode='bicubic', align_corners=False))\n",
    "\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "            t_tensor = torch.tensor([t], device=z_t.device).long()\n",
    "            alpha_t = self.alphas[t]\n",
    "            sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "            sqrt_one_minus_alpha_t = torch.sqrt(1 - self.alphas_cumprod[t])\n",
    "            beta_t = self.betas[t]\n",
    "\n",
    "            predicted_noise = self.forward(z_t, t_tensor, low_res_image)\n",
    "\n",
    "            z_t = (z_t - (1 - self.alphas[t]) * predicted_noise / sqrt_one_minus_alpha_t) / sqrt_alpha_t\n",
    "\n",
    "            if t > 0:\n",
    "                z_t += torch.randn_like(z_t) * torch.sqrt(beta_t)\n",
    "\n",
    "        return z_t\n",
    "\n",
    "    def p_sample(self, z, t, low_res_image):\n",
    "        alpha_t = self.alphas[t]\n",
    "        sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "        sqrt_one_minus_alpha_t = torch.sqrt(1 - self.alphas_cumprod[t])\n",
    "        beta_t = self.betas[t]\n",
    "        predicted_noise = self.forward(z, t, low_res_image)\n",
    "\n",
    "        z = (z - beta_t / sqrt_one_minus_alpha_t * predicted_noise) / sqrt_alpha_t\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b019948-3700-452d-980a-1a0784276725",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DDPM implementation adapted from:\n",
    "https://github.com/hojonathanho/diffusion/tree/master\n",
    "\"\"\"\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, model, num_timesteps, latent_dim, beta_start=0.00085, beta_end=0.0120):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.model = model\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', 1 - betas)\n",
    "        self.register_buffer('alphas_cumprod', torch.cumprod(1 - betas, dim=0))\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(self.alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - self.alphas_cumprod))\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def forward(self, z_t, t):\n",
    "        return self.model(z_t, t)\n",
    "\n",
    "    def sample_timesteps(self, batch_size):\n",
    "        return torch.randint(0, self.num_timesteps, (batch_size,)).to(device)\n",
    "\n",
    "    def forward_diffusion(self, z_0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(z_0)\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        return sqrt_alphas_cumprod_t * z_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_losses(self, z_0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(z_0)\n",
    "        z_t = self.forward_diffusion(z_0, t, noise)\n",
    "        predicted_noise = self.forward(z_t, t)\n",
    "        return nn.MSELoss()(noise, predicted_noise)\n",
    "\n",
    "    def sample(self, shape):\n",
    "        z_t = torch.randn(shape).to(device)\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "            t_tensor = torch.tensor([t], device=z_t.device).long()\n",
    "            alpha_t = self.alphas[t]\n",
    "            sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "            sqrt_one_minus_alpha_t = torch.sqrt(1 - self.alphas_cumprod[t])\n",
    "            beta_t = self.betas[t]\n",
    "\n",
    "            # Predict the noise\n",
    "            predicted_noise = self.forward(z_t, t_tensor)\n",
    "\n",
    "            # Remove the predicted noise\n",
    "            z_t = (z_t - beta_t / sqrt_one_minus_alpha_t * predicted_noise) / sqrt_alpha_t\n",
    "\n",
    "            # Add noise for non-final steps\n",
    "            if t > 0:\n",
    "                z_t += torch.randn_like(z_t) * torch.sqrt(beta_t)\n",
    "\n",
    "        return z_t\n",
    "\n",
    "    def p_sample(self, z, t):\n",
    "        predicted_noise = self.forward(z, t)\n",
    "        alpha_t = self.alphas[t]\n",
    "        sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "        sqrt_one_minus_alpha_t = torch.sqrt(1 - alpha_t)\n",
    "        z = (z - predicted_noise * (1 - alpha_t) / sqrt_one_minus_alpha_t) / sqrt_alpha_t\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d08320",
   "metadata": {},
   "source": [
    "### LOAD MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "298d41ea-42d6-4a57-9c64-7eaa5bc2748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_model(ddpm, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    ddpm.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return ddpm\n",
    "\n",
    "# Load the three models from checkpoints\n",
    "def load_models():\n",
    "    # Model 1: 64x64 Generation\n",
    "    unet_64 = AttentionUNet(in_channels=1, out_channels=1, emb_dim=128).to(device)\n",
    "    ddpm_64 = DDPM(unet_64, num_timesteps=2000, latent_dim=128).to(device)\n",
    "    ddpm_64 = load_model(ddpm_64, \"Model_Savepoints/swinddpm64RAW_checkpoint.pth\")\n",
    "\n",
    "    # Model 2: 64x64 to 128x128 Super-resolution\n",
    "    unet_128 = SuperResUNet128(in_channels=2, out_channels=1, emb_dim=128).to(device)\n",
    "    ddpm_128 = SuperResDDPM(unet_128, num_timesteps=1000).to(device)\n",
    "    ddpm_128 = load_model(ddpm_128, \"Model_Savepoints/cascadedddpm64200epochs_checkpoint.pth\")\n",
    "\n",
    "    # Model 3: 128x128 to 256x256 Super-resolution\n",
    "    unet_256 = SuperResUNet256(in_channels=2, out_channels=1, emb_dim=128).to(device)\n",
    "    ddpm_256 = SuperResDDPM(unet_256, num_timesteps=1000).to(device)\n",
    "    ddpm_256 = load_model(ddpm_256, \"Model_Savepoints/cascadedddpm128(225epoch)_checkpoint.pth\")\n",
    "\n",
    "    return ddpm_64, ddpm_128, ddpm_256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6d604",
   "metadata": {},
   "source": [
    "##### SSIM, FSIM, DSIM, MSE, PSNR, Visual Fidelity, LPIPS and Average Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1645bc2-b9f4-4f97-9461-d8cd7326c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install image-similarity-measures\n",
    "#pip install torchmetrics\n",
    "#pip install pyfftw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e4f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity as LPIPS\n",
    "from torchmetrics.image import VisualInformationFidelity\n",
    "from image_similarity_measures.quality_metrics import fsim\n",
    "from scipy.ndimage import sobel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c384dcf5-70ff-4d88-beeb-5aac31acda65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 5/5 [13:37<00:00, 163.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cascaded Super-resolution from 64x64 to 256x256:\n",
      "PSNR=27.174150257110597, SSIM=0.7387099087238311, DSSIM=0.1306450456380844, MSE=0.0035601869842503218, FSIM=0.5452157156335029\n",
      "VIF=0.8344831836223602, LPIPS=0.17526448905467987, AG=0.26342782378196716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MRNetUpscaleDataset(Dataset):\n",
    "    def __init__(self, slice_dir, label_files, transform_64=None, transform_256=None):\n",
    "        super().__init__()\n",
    "        self.slice_dir = slice_dir\n",
    "        self.transform_64 = transform_64\n",
    "        self.transform_256 = transform_256\n",
    "\n",
    "        self.labels_dict = {}\n",
    "        for label_file in label_files:\n",
    "            records = pd.read_csv(label_file, header=None, names=['id', 'label'])\n",
    "            records['id'] = records['id'].map(lambda i: '0' * (4 - len(str(i))) + str(i))\n",
    "            self.labels_dict.update(dict(zip(records['id'], records['label'])))\n",
    "\n",
    "        # List all slice files\n",
    "        self.slice_files = [os.path.join(slice_dir, fname) for fname in os.listdir(slice_dir) if fname.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        slice_path = self.slice_files[index]\n",
    "        image = Image.open(slice_path)\n",
    "        \n",
    "        if self.transform_64:\n",
    "            image_64 = self.transform_64(image)\n",
    "        if self.transform_256:\n",
    "            image_256 = self.transform_256(image)\n",
    "        \n",
    "        # Extract ID from filename to match with label\n",
    "        slice_id = os.path.basename(slice_path).split('_')[1]\n",
    "\n",
    "        if slice_id in self.labels_dict:\n",
    "            label = self.labels_dict[slice_id]\n",
    "            label = torch.FloatTensor([label])\n",
    "        else:\n",
    "            print(f\"Label for ID {slice_id} not found in the CSV file.\")\n",
    "            label = torch.FloatTensor([0]) \n",
    "\n",
    "        return {'data_64': image_64, 'data_256': image_256, 'label': label, 'id': slice_id}\n",
    "\n",
    "# Define transformations for 64x64 and 256x256\n",
    "transform_64 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((64, 64)),  # Resize to 64x64\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "transform_256 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "# Initialize the dataset\n",
    "root_dir = r\"/rds/homes/a/avv306/Raw_Images/Raw_Images\"\n",
    "valid_slice_dir = os.path.join(root_dir, \"valid_slices_raw\")\n",
    "\n",
    "valid_label_files = [\n",
    "    os.path.join(root_dir, \"valid-acl.csv\"),\n",
    "    os.path.join(root_dir, \"valid-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"valid-meniscus.csv\")\n",
    "]\n",
    "\n",
    "# Create a dataset for 64x64 and 256x256\n",
    "valid_dataset_64_256 = MRNetUpscaleDataset(\n",
    "    slice_dir=valid_slice_dir,\n",
    "    label_files=valid_label_files,\n",
    "    transform_64=transform_64,\n",
    "    transform_256=transform_256\n",
    ")\n",
    "\n",
    "# Create Subsets of the dataset for testing\n",
    "subset_indices = list(range(100))  # Use the first 160 images for example\n",
    "valid_subset_64_256 = Subset(valid_dataset_64_256, subset_indices)\n",
    "\n",
    "# Create DataLoader for the subset\n",
    "valid_loader_64_256 = DataLoader(valid_subset_64_256, batch_size=20, shuffle=True)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize PyTorch metric objects on the same device\n",
    "psnr_metric = PeakSignalNoiseRatio().to(device)\n",
    "ssim_metric = StructuralSimilarityIndexMeasure().to(device)\n",
    "vif_metric = VisualInformationFidelity().to(device)\n",
    "lpips_metric = LPIPS(net_type='vgg').to(device)  \n",
    "\n",
    "# Function to prepare images for LPIPS\n",
    "def prepare_for_lpips(img):\n",
    "    # Convert grayscale to 3 channels by repeating the channel 3 times\n",
    "    img_3c = img.repeat(1, 3, 1, 1)  # Repeat across the channel dimension\n",
    "    # Normalize the image to [-1, 1]\n",
    "    img_3c = (img_3c * 2) - 1\n",
    "    # Clip the values to ensure they stay within the expected range\n",
    "    img_3c = torch.clamp(img_3c, min=-1.0, max=1.0)\n",
    "    return img_3c\n",
    "\n",
    "# Function to compute PSNR using PyTorch\n",
    "def compute_psnr(img1, img2):\n",
    "    return psnr_metric(img1, img2).item()\n",
    "\n",
    "# Function to compute SSIM and DSSIM using PyTorch\n",
    "def compute_ssim_dssim(img1, img2):\n",
    "    ssim_value = ssim_metric(img1, img2).item()\n",
    "    dssim_value = (1 - ssim_value) / 2\n",
    "    return ssim_value, dssim_value\n",
    "\n",
    "# Function to compute MSE using PyTorch\n",
    "def compute_mse(img1, img2):\n",
    "    return torch.mean((img1 - img2) ** 2).item()\n",
    "\n",
    "# Function to compute FSIM using image_similarity_measures\n",
    "def compute_fsim(img1, img2):\n",
    "    # Convert PyTorch tensors to NumPy arrays and scale to 8-bit integer\n",
    "    img1_np = img1.cpu().numpy().squeeze() * 255\n",
    "    img2_np = img2.cpu().numpy().squeeze() * 255\n",
    "\n",
    "    # Ensure the images are 3D (add a channel dimension if they're 2D)\n",
    "    if img1_np.ndim == 2:\n",
    "        img1_np = np.expand_dims(img1_np, axis=-1)\n",
    "    if img2_np.ndim == 2:\n",
    "        img2_np = np.expand_dims(img2_np, axis=-1)\n",
    "    \n",
    "    img1_uint8 = img1_np.astype(np.uint8)\n",
    "    img2_uint8 = img2_np.astype(np.uint8)\n",
    "\n",
    "    # Compute FSIM using the image_similarity_measures library\n",
    "    return fsim(img1_uint8, img2_uint8)\n",
    "\n",
    "# Function to compute VIF using PyTorch\n",
    "def compute_vif(img1, img2):\n",
    "    return vif_metric(img1, img2).item()\n",
    "\n",
    "# Function to compute LPIPS using PyTorch\n",
    "def compute_lpips(img1, img2):\n",
    "    img1 = prepare_for_lpips(img1)\n",
    "    img2 = prepare_for_lpips(img2)\n",
    "    return lpips_metric(img1, img2).item()\n",
    "\n",
    "# Function to compute AG (Average Gradient)\n",
    "def compute_ag(img):\n",
    "    img_np = img.cpu().numpy().squeeze()\n",
    "    gx = sobel(img_np, axis=0)\n",
    "    gy = sobel(img_np, axis=1)\n",
    "    grad_magnitude = np.sqrt(gx**2 + gy**2)\n",
    "    return np.mean(grad_magnitude)\n",
    "\n",
    "# Function to evaluate super-resolution models\n",
    "def evaluate_super_resolution_cascaded(ddpm_128, ddpm_256, dataloader, device):\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    dssim_values = []\n",
    "    mse_values = []\n",
    "    fsim_values = []\n",
    "    vif_values = []\n",
    "    lpips_values = []\n",
    "    ag_values = []\n",
    "\n",
    "    ddpm_128.eval()\n",
    "    ddpm_256.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Processing batches\")):\n",
    "            low_res = batch['data_64'].to(device)\n",
    "            high_res = batch['data_256'].to(device)\n",
    "\n",
    "            for i in range(low_res.size(0)):\n",
    "                input_image = low_res[i].unsqueeze(0)\n",
    "                target_image = high_res[i].unsqueeze(0)\n",
    "\n",
    "                # Upscale from 64x64 to 256x256\n",
    "                super_res_image = ddpm_128.sample(input_image)\n",
    "                super_res_image = ddpm_256.sample(super_res_image)\n",
    "                \n",
    "                # Convert tensors back to images for evaluation\n",
    "                high_res_img = target_image\n",
    "                super_res_img = super_res_image\n",
    "\n",
    "                # Compute metrics\n",
    "                psnr_values.append(compute_psnr(high_res_img, super_res_img))\n",
    "                ssim_value, dssim_value = compute_ssim_dssim(high_res_img, super_res_img)\n",
    "                ssim_values.append(ssim_value)\n",
    "                dssim_values.append(dssim_value)\n",
    "                mse_values.append(compute_mse(high_res_img, super_res_img))\n",
    "                fsim_values.append(compute_fsim(high_res_img, super_res_img))\n",
    "                vif_values.append(compute_vif(high_res_img, super_res_img))\n",
    "                lpips_values.append(compute_lpips(high_res_img, super_res_img))\n",
    "                ag_values.append(compute_ag(super_res_img))\n",
    "                \n",
    "    return (np.mean(psnr_values), np.mean(ssim_values), np.mean(dssim_values), np.mean(mse_values), np.mean(fsim_values),\n",
    "            np.mean(vif_values), np.mean(lpips_values), np.mean(ag_values))\n",
    "\n",
    "# Load the models (ensure load_models is implemented as per your previous context)\n",
    "ddpm_64, ddpm_128, ddpm_256 = load_models()\n",
    "\n",
    "# Evaluate the cascaded super-resolution model from 64x64 to 256x256\n",
    "(psnr_256, ssim_256, dssim_256, mse_256, fsim_256, \n",
    " vif_256, lpips_256, ag_256) = evaluate_super_resolution_cascaded(ddpm_128, ddpm_256, valid_loader_64_256, device)\n",
    "\n",
    "print(f\"Cascaded Super-resolution from 64x64 to 256x256:\")\n",
    "print(f\"PSNR={psnr_256}, SSIM={ssim_256}, DSSIM={dssim_256}, MSE={mse_256}, FSIM={fsim_256}\")\n",
    "print(f\"VIF={vif_256}, LPIPS={lpips_256}, AG={ag_256}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe6ba0b",
   "metadata": {},
   "source": [
    "### NOSWIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed7b021",
   "metadata": {},
   "source": [
    "##### 64x64 UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e219f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionUNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=128):\n",
    "        super(AttentionUNet, self).__init__()\n",
    "\n",
    "        self.encoder1 = self.conv_block(in_channels + emb_dim, 64)\n",
    "        self.self_attention1 = SelfAttentionBlock(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.self_attention2 = SelfAttentionBlock(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.self_attention3 = SelfAttentionBlock(256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "       \n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.cross_attention3 = CrossAttentionBlock(256, 256, 128)\n",
    "        self.decoder3 = self.conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.cross_attention2 = CrossAttentionBlock(128, 128, 64)\n",
    "        self.decoder2 = self.conv_block(256, 128)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.cross_attention1 = CrossAttentionBlock(64, 64, 32)\n",
    "        self.decoder1 = self.conv_block(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "        self.timestep_embedding_layer = SinusoidalPositionalEmbedding(emb_dim)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.timestep_embedding_layer(t)\n",
    "        t_embed = t_embed.view(t.size(0), -1, 1, 1)\n",
    "        t_embed = t_embed.repeat(1, 1, x.size(2), x.size(3))\n",
    "        x = torch.cat((x, t_embed), dim=1)\n",
    "\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc1 = self.self_attention1(enc1)\n",
    "        enc1_pooled = self.pool1(enc1)\n",
    "\n",
    "        enc2 = self.encoder2(enc1_pooled)\n",
    "        enc2 = self.self_attention2(enc2)\n",
    "        enc2_pooled = self.pool2(enc2)\n",
    "\n",
    "        enc3 = self.encoder3(enc2_pooled)\n",
    "        enc3 = self.self_attention3(enc3)\n",
    "        enc3_pooled = self.pool3(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc3_pooled)\n",
    "\n",
    "        # Decoder\n",
    "        upconv3 = self.upconv3(bottleneck)\n",
    "        enc3 = self.cross_attention3(upconv3, enc3)\n",
    "        dec3 = torch.cat((upconv3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        upconv2 = self.upconv2(dec3)\n",
    "        enc2 = self.cross_attention2(upconv2, enc2)\n",
    "        dec2 = torch.cat((upconv2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        upconv1 = self.upconv1(dec2)\n",
    "        enc1 = self.cross_attention1(upconv1, enc1)\n",
    "        dec1 = torch.cat((upconv1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        final_output = self.final_conv(dec1)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3092029f",
   "metadata": {},
   "source": [
    "##### 64x64 to 128x128 UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11b52da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperResUNet128(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=128):\n",
    "        super(SuperResUNet128, self).__init__()\n",
    "\n",
    "        self.encoder1 = self.conv_block(in_channels + emb_dim, 64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self.conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.conv_block(256, 128)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self.conv_block(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "        self.timestep_embedding_layer = SinusoidalPositionalEmbedding(emb_dim)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.timestep_embedding_layer(t)\n",
    "        t_embed = t_embed.view(t.size(0), -1, 1, 1)\n",
    "        t_embed = t_embed.repeat(1, 1, x.size(2), x.size(3))\n",
    "\n",
    "        x = torch.cat((x, t_embed), dim=1)\n",
    "\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc1_pooled = self.pool1(enc1)\n",
    "\n",
    "        enc2 = self.encoder2(enc1_pooled)\n",
    "        enc2_pooled = self.pool2(enc2)\n",
    "\n",
    "        enc3 = self.encoder3(enc2_pooled)\n",
    "        enc3_pooled = self.pool3(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc3_pooled)\n",
    "\n",
    "        # Decoder\n",
    "        upconv3 = self.upconv3(bottleneck)\n",
    "        dec3 = torch.cat((upconv3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        upconv2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((upconv2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        upconv1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((upconv1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        final_output = self.final_conv(dec1)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52f0e7",
   "metadata": {},
   "source": [
    "##### 128x128 to 256x256 UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75e8f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperResUNet256(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=128):\n",
    "        super(SuperResUNet256, self).__init__()\n",
    "\n",
    "        self.encoder1 = self.conv_block(in_channels + emb_dim, 64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self.conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.conv_block(256, 128)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self.conv_block(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "        self.timestep_embedding_layer = SinusoidalPositionalEmbedding(emb_dim)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.timestep_embedding_layer(t)\n",
    "        t_embed = t_embed.view(t.size(0), -1, 1, 1)\n",
    "        t_embed = t_embed.repeat(1, 1, x.size(2), x.size(3))\n",
    "\n",
    "        x = torch.cat((x, t_embed), dim=1)\n",
    "\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc1_pooled = self.pool1(enc1)\n",
    "\n",
    "        enc2 = self.encoder2(enc1_pooled)\n",
    "        enc2_pooled = self.pool2(enc2)\n",
    "\n",
    "        enc3 = self.encoder3(enc2_pooled)\n",
    "        enc3_pooled = self.pool3(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc3_pooled)\n",
    "\n",
    "        # Decoder\n",
    "        upconv3 = self.upconv3(bottleneck)\n",
    "        dec3 = torch.cat((upconv3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        upconv2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((upconv2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        upconv1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((upconv1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        final_output = self.final_conv(dec1)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d81b0",
   "metadata": {},
   "source": [
    "##### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e615b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_model(ddpm, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    ddpm.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return ddpm\n",
    "\n",
    "# Load the three models from checkpoints\n",
    "def load_models():\n",
    "    # Model 1: 64x64 Generation\n",
    "    unet_64 = AttentionUNet(in_channels=1, out_channels=1, emb_dim=128).to(device)\n",
    "    ddpm_64 = DDPM(unet_64, num_timesteps=2000, latent_dim=128).to(device)\n",
    "    ddpm_64 = load_model(ddpm_64, \"Model_Savepoints/ddpm64NOSWIN_checkpoint.pth\")\n",
    "\n",
    "    # Model 2: 64x64 to 128x128 Super-resolution\n",
    "    unet_128 = SuperResUNet128(in_channels=2, out_channels=1, emb_dim=128).to(device)\n",
    "    ddpm_128 = SuperResDDPM(unet_128, num_timesteps=1000).to(device)\n",
    "    ddpm_128 = load_model(ddpm_128, \"Model_Savepoints/cascadedddpm64NOSWIN(100epoch)_checkpoint.pth\")\n",
    "\n",
    "    # Model 3: 128x128 to 256x256 Super-resolution\n",
    "    unet_256 = SuperResUNet256(in_channels=2, out_channels=1, emb_dim=128).to(device)\n",
    "    ddpm_256 = SuperResDDPM(unet_256, num_timesteps=1000).to(device)\n",
    "    ddpm_256 = load_model(ddpm_256, \"Model_Savepoints/cascadedddpm128NOSWIN_checkpoint.pth\")\n",
    "\n",
    "    return ddpm_64, ddpm_128, ddpm_256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ceade",
   "metadata": {},
   "source": [
    "##### SSIM, FSIM, DSIM, MSE, PSNR, Visual Fidelity, LPIPS and Average Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5652dc72-58c0-4bdd-846c-716364ac483e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 10/10 [10:26<00:00, 62.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cascaded NO SWIN Super-resolution from 64x64 to 256x256: PSNR=27.686824488639832, SSIM=0.6121361188590526, DSSIM=0.19393194057047367, MSE=0.004092157847480848, FSIM=0.45221029153800674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "from image_similarity_measures.quality_metrics import fsim  # Import FSIM from image_similarity_measures\n",
    "\n",
    "\n",
    "class MRNetUpscaleDataset(Dataset):\n",
    "    def __init__(self, slice_dir, label_files, transform_64=None, transform_256=None):\n",
    "        super().__init__()\n",
    "        self.slice_dir = slice_dir\n",
    "        self.transform_64 = transform_64\n",
    "        self.transform_256 = transform_256\n",
    "\n",
    "        self.labels_dict = {}\n",
    "        for label_file in label_files:\n",
    "            records = pd.read_csv(label_file, header=None, names=['id', 'label'])\n",
    "            records['id'] = records['id'].map(lambda i: '0' * (4 - len(str(i))) + str(i))\n",
    "            self.labels_dict.update(dict(zip(records['id'], records['label'])))\n",
    "\n",
    "        # List all slice files\n",
    "        self.slice_files = [os.path.join(slice_dir, fname) for fname in os.listdir(slice_dir) if fname.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        slice_path = self.slice_files[index]\n",
    "        image = Image.open(slice_path)\n",
    "        \n",
    "        if self.transform_64:\n",
    "            image_64 = self.transform_64(image)\n",
    "        if self.transform_256:\n",
    "            image_256 = self.transform_256(image)\n",
    "        \n",
    "        # Extract ID from filename to match with label\n",
    "        slice_id = os.path.basename(slice_path).split('_')[1]\n",
    "\n",
    "        if slice_id in self.labels_dict:\n",
    "            label = self.labels_dict[slice_id]\n",
    "            label = torch.FloatTensor([label])\n",
    "        else:\n",
    "            print(f\"Label for ID {slice_id} not found in the CSV file.\")\n",
    "            label = torch.FloatTensor([0])  \n",
    "\n",
    "        return {'data_64': image_64, 'data_256': image_256, 'label': label, 'id': slice_id}\n",
    "\n",
    "# Define transformations for 64x64 and 256x256\n",
    "transform_64 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((64, 64)),  # Resize to 64x64\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "transform_256 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "# Initialize the dataset\n",
    "root_dir = r\"/rds/homes/a/avv306/Raw_Images/Raw_Images\"\n",
    "valid_slice_dir = os.path.join(root_dir, \"valid_slices_raw\")\n",
    "\n",
    "valid_label_files = [\n",
    "    os.path.join(root_dir, \"valid-acl.csv\"),\n",
    "    os.path.join(root_dir, \"valid-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"valid-meniscus.csv\")\n",
    "]\n",
    "\n",
    "# Create a dataset for 64x64 and 256x256\n",
    "valid_dataset_64_256 = MRNetUpscaleDataset(\n",
    "    slice_dir=valid_slice_dir,\n",
    "    label_files=valid_label_files,\n",
    "    transform_64=transform_64,\n",
    "    transform_256=transform_256\n",
    ")\n",
    "\n",
    "# Create Subsets of the dataset for testing\n",
    "subset_indices = list(range(160))  # Use the first 160 images for example\n",
    "valid_subset_64_256 = Subset(valid_dataset_64_256, subset_indices)\n",
    "\n",
    "# Create DataLoader for the subset\n",
    "valid_loader_64_256 = DataLoader(valid_subset_64_256, batch_size=16, shuffle=True)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize PyTorch metric objects on the same device\n",
    "psnr_metric = PeakSignalNoiseRatio().to(device)\n",
    "ssim_metric = StructuralSimilarityIndexMeasure().to(device)\n",
    "\n",
    "# Function to compute PSNR using PyTorch\n",
    "def compute_psnr(img1, img2):\n",
    "    return psnr_metric(img1, img2).item()\n",
    "\n",
    "# Function to compute SSIM and DSSIM using PyTorch\n",
    "def compute_ssim_dssim(img1, img2):\n",
    "    ssim_value = ssim_metric(img1, img2).item()\n",
    "    dssim_value = (1 - ssim_value) / 2\n",
    "    return ssim_value, dssim_value\n",
    "\n",
    "# Function to compute MSE using PyTorch\n",
    "def compute_mse(img1, img2):\n",
    "    return torch.mean((img1 - img2) ** 2).item()\n",
    "\n",
    "# Function to compute FSIM using image_similarity_measures\n",
    "def compute_fsim(img1, img2):\n",
    "    # Convert PyTorch tensors to NumPy arrays and scale to 8-bit integer\n",
    "    img1_np = img1.cpu().numpy().squeeze() * 255\n",
    "    img2_np = img2.cpu().numpy().squeeze() * 255\n",
    "\n",
    "    # Ensure the images are 3D (add a channel dimension if they're 2D)\n",
    "    if img1_np.ndim == 2:\n",
    "        img1_np = np.expand_dims(img1_np, axis=-1)\n",
    "    if img2_np.ndim == 2:\n",
    "        img2_np = np.expand_dims(img2_np, axis=-1)\n",
    "    \n",
    "    img1_uint8 = img1_np.astype(np.uint8)\n",
    "    img2_uint8 = img2_np.astype(np.uint8)\n",
    "\n",
    "    # Compute FSIM using the image_similarity_measures library\n",
    "    return fsim(img1_uint8, img2_uint8)\n",
    "\n",
    "\n",
    "# Function to evaluate super-resolution models\n",
    "def evaluate_super_resolution_cascaded(ddpm_128, ddpm_256, dataloader, device):\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    dssim_values = []\n",
    "    mse_values = []\n",
    "    fsim_values = []\n",
    "\n",
    "    ddpm_128.eval()\n",
    "    ddpm_256.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Processing batches\")):\n",
    "            low_res = batch['data_64'].to(device)\n",
    "            high_res = batch['data_256'].to(device)\n",
    "\n",
    "            for i in range(low_res.size(0)):\n",
    "                input_image = low_res[i].unsqueeze(0)\n",
    "                #print(input_image.shape)\n",
    "                target_image = high_res[i].unsqueeze(0)\n",
    "                #print(target_image.shape)\n",
    "                # Upscale from 64x64 to 256x256\n",
    "                super_res_image = ddpm_128.sample(input_image)\n",
    "                super_res_image = ddpm_256.sample(super_res_image)\n",
    "                #print(super_res_image.shape)\n",
    "                # Convert tensors back to images for evaluation\n",
    "                high_res_img = target_image\n",
    "                super_res_img = super_res_image\n",
    "\n",
    "                psnr_values.append(compute_psnr(high_res_img, super_res_img))\n",
    "                ssim_value, dssim_value = compute_ssim_dssim(high_res_img, super_res_img)\n",
    "                ssim_values.append(ssim_value)\n",
    "                dssim_values.append(dssim_value)\n",
    "                mse_values.append(compute_mse(high_res_img, super_res_img))\n",
    "                fsim_values.append(compute_fsim(high_res_img, super_res_img))\n",
    "                \n",
    "    return np.mean(psnr_values), np.mean(ssim_values), np.mean(dssim_values), np.mean(mse_values), np.mean(fsim_values)\n",
    "\n",
    "# Load the models \n",
    "ddpm_64, ddpm_128, ddpm_256 = load_models()\n",
    "\n",
    "# Evaluate the cascaded super-resolution model from 64x64 to 256x256\n",
    "psnr_256, ssim_256, dssim_256, mse_256, fsim_256 = evaluate_super_resolution_cascaded(ddpm_128, ddpm_256, valid_loader_64_256, device)\n",
    "\n",
    "print(f\"Cascaded NO SWIN Super-resolution from 64x64 to 256x256: PSNR={psnr_256}, SSIM={ssim_256}, DSSIM={dssim_256}, MSE={mse_256}, FSIM={fsim_256}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e671d337-a00c-48d6-878d-e10c030f7b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 10/10 [09:57<00:00, 59.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cascaded NO SWIN Super-resolution from 64x64 to 256x256: VIF=0.8160529281944037, LPIPS=0.36456783562898637, AG=0.3209536373615265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity as LPIPS\n",
    "from torchmetrics.image import VisualInformationFidelity\n",
    "from scipy.ndimage import sobel\n",
    "\n",
    "\n",
    "class MRNetUpscaleDataset(Dataset):\n",
    "    def __init__(self, slice_dir, label_files, transform_64=None, transform_256=None):\n",
    "        super().__init__()\n",
    "        self.slice_dir = slice_dir\n",
    "        self.transform_64 = transform_64\n",
    "        self.transform_256 = transform_256\n",
    "\n",
    "        self.labels_dict = {}\n",
    "        for label_file in label_files:\n",
    "            records = pd.read_csv(label_file, header=None, names=['id', 'label'])\n",
    "            records['id'] = records['id'].map(lambda i: '0' * (4 - len(str(i))) + str(i))\n",
    "            self.labels_dict.update(dict(zip(records['id'], records['label'])))\n",
    "\n",
    "        # List all slice files\n",
    "        self.slice_files = [os.path.join(slice_dir, fname) for fname in os.listdir(slice_dir) if fname.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        slice_path = self.slice_files[index]\n",
    "        image = Image.open(slice_path)\n",
    "        \n",
    "        if self.transform_64:\n",
    "            image_64 = self.transform_64(image)\n",
    "        if self.transform_256:\n",
    "            image_256 = self.transform_256(image)\n",
    "        \n",
    "        # Extract ID from filename to match with label\n",
    "        slice_id = os.path.basename(slice_path).split('_')[1]\n",
    "\n",
    "        if slice_id in self.labels_dict:\n",
    "            label = self.labels_dict[slice_id]\n",
    "            label = torch.FloatTensor([label])\n",
    "        else:\n",
    "            print(f\"Label for ID {slice_id} not found in the CSV file.\")\n",
    "            label = torch.FloatTensor([0])  \n",
    "\n",
    "        return {'data_64': image_64, 'data_256': image_256, 'label': label, 'id': slice_id}\n",
    "\n",
    "# Define transformations for 64x64 and 256x256\n",
    "transform_64 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((64, 64)),  # Resize to 64x64\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "transform_256 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "# Initialize the dataset\n",
    "root_dir = r\"/rds/homes/a/avv306/Raw_Images/Raw_Images\"\n",
    "valid_slice_dir = os.path.join(root_dir, \"valid_slices_raw\")\n",
    "\n",
    "valid_label_files = [\n",
    "    os.path.join(root_dir, \"valid-acl.csv\"),\n",
    "    os.path.join(root_dir, \"valid-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"valid-meniscus.csv\")\n",
    "]\n",
    "\n",
    "# Create a dataset for 64x64 and 256x256\n",
    "valid_dataset_64_256 = MRNetUpscaleDataset(\n",
    "    slice_dir=valid_slice_dir,\n",
    "    label_files=valid_label_files,\n",
    "    transform_64=transform_64,\n",
    "    transform_256=transform_256\n",
    ")\n",
    "\n",
    "# Create Subsets of the dataset for testing\n",
    "subset_indices = list(range(160))  # Use the first 160 images for example\n",
    "valid_subset_64_256 = Subset(valid_dataset_64_256, subset_indices)\n",
    "\n",
    "# Create DataLoader for the subset\n",
    "valid_loader_64_256 = DataLoader(valid_subset_64_256, batch_size=16, shuffle=True)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize PyTorch metric objects on the same device\n",
    "vif_metric = VisualInformationFidelity().to(device)\n",
    "lpips_metric = LPIPS(net_type='vgg').to(device) \n",
    "\n",
    "# Function to prepare images for LPIPS\n",
    "def prepare_for_lpips(img):\n",
    "    # Convert grayscale to 3 channels by repeating the channel 3 times\n",
    "    img_3c = img.repeat(1, 3, 1, 1)  # Repeat across the channel dimension\n",
    "    # Normalize the image to [-1, 1]\n",
    "    img_3c = (img_3c * 2) - 1\n",
    "    # Clip the values to ensure they stay within the expected range\n",
    "    img_3c = torch.clamp(img_3c, min=-1.0, max=1.0)\n",
    "    return img_3c\n",
    "\n",
    "\n",
    "# Function to compute VIF using PyTorch\n",
    "def compute_vif(img1, img2):\n",
    "    return vif_metric(img1, img2).item()\n",
    "\n",
    "# Function to compute LPIPS using PyTorch\n",
    "def compute_lpips(img1, img2):\n",
    "    img1 = prepare_for_lpips(img1)\n",
    "    img2 = prepare_for_lpips(img2)\n",
    "    return lpips_metric(img1, img2).item()\n",
    "\n",
    "# Function to compute AG (Average Gradient)\n",
    "def compute_ag(img):\n",
    "    img_np = img.cpu().numpy().squeeze()\n",
    "    gx = sobel(img_np, axis=0)\n",
    "    gy = sobel(img_np, axis=1)\n",
    "    grad_magnitude = np.sqrt(gx**2 + gy**2)\n",
    "    return np.mean(grad_magnitude)\n",
    "\n",
    "# Function to evaluate super-resolution models\n",
    "def evaluate_super_resolution_cascaded(ddpm_128, ddpm_256, dataloader, device):\n",
    "    vif_values = []\n",
    "    lpips_values = []\n",
    "    ag_values = []\n",
    "\n",
    "    ddpm_128.eval()\n",
    "    ddpm_256.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Processing batches\")):\n",
    "            low_res = batch['data_64'].to(device)\n",
    "            high_res = batch['data_256'].to(device)\n",
    "\n",
    "            for i in range(low_res.size(0)):\n",
    "                input_image = low_res[i].unsqueeze(0)\n",
    "                target_image = high_res[i].unsqueeze(0)\n",
    "                # Upscale from 64x64 to 256x256\n",
    "                super_res_image = ddpm_128.sample(input_image)\n",
    "                super_res_image = ddpm_256.sample(super_res_image)\n",
    "                \n",
    "                # Convert tensors back to images for evaluation\n",
    "                high_res_img = target_image\n",
    "                super_res_img = super_res_image\n",
    "\n",
    "                vif_values.append(compute_vif(high_res_img, super_res_img))\n",
    "                lpips_values.append(compute_lpips(high_res_img, super_res_img))\n",
    "                ag_values.append(compute_ag(super_res_img))\n",
    "                \n",
    "    return np.mean(vif_values), np.mean(lpips_values), np.mean(ag_values)\n",
    "\n",
    "# Load the models \n",
    "ddpm_64, ddpm_128, ddpm_256 = load_models()\n",
    "\n",
    "# Evaluate the cascaded super-resolution model from 64x64 to 256x256\n",
    "vif_256, lpips_256, ag_256 = evaluate_super_resolution_cascaded(ddpm_128, ddpm_256, valid_loader_64_256, device)\n",
    "\n",
    "print(f\"Cascaded NO SWIN Super-resolution from 64x64 to 256x256: VIF={vif_256}, LPIPS={lpips_256}, AG={ag_256}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a23919",
   "metadata": {},
   "source": [
    "### INTERPOLATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d192b76-179b-4502-8488-bb4185704715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-40GB\n",
      "\n",
      "Evaluating Direct Interpolation Techniques for 64x64 to 256x256:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating nearest interpolation: 100%|██████████| 10/10 [00:30<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Interpolation from 64x64 to 256x256: PSNR=23.112764418125153, SSIM=0.6773493640124798, DSSIM=0.1613253179937601, MSE=0.004943166526209098, FSIM=0.5011008700408051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bilinear interpolation: 100%|██████████| 10/10 [00:30<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bilinear Interpolation from 64x64 to 256x256: PSNR=23.694603943824767, SSIM=0.700542651861906, DSSIM=0.14972867406904697, MSE=0.004306619871204021, FSIM=0.6232317244863224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bicubic interpolation: 100%|██████████| 10/10 [00:30<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bicubic Interpolation from 64x64 to 256x256: PSNR=25.035907018184663, SSIM=0.7310318142175675, DSSIM=0.13448409289121627, MSE=0.0035574150926549917, FSIM=0.6323735310671865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating lanczos interpolation: 100%|██████████| 10/10 [00:30<00:00,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lanczos Interpolation from 64x64 to 256x256: PSNR=25.097058725357055, SSIM=0.7344014767557383, DSSIM=0.13279926162213088, MSE=0.003449835801438894, FSIM=0.6401947585409061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class MRNetUpscaleDataset(Dataset):\n",
    "    def __init__(self, slice_dir, label_files, transform_64=None, transform_256=None):\n",
    "        super().__init__()\n",
    "        self.slice_dir = slice_dir\n",
    "        self.transform_64 = transform_64\n",
    "        self.transform_256 = transform_256\n",
    "\n",
    "        self.labels_dict = {}\n",
    "        for label_file in label_files:\n",
    "            records = pd.read_csv(label_file, header=None, names=['id', 'label'])\n",
    "            records['id'] = records['id'].map(lambda i: '0' * (4 - len(str(i))) + str(i))\n",
    "            self.labels_dict.update(dict(zip(records['id'], records['label'])))\n",
    "\n",
    "        # List all slice files\n",
    "        self.slice_files = [os.path.join(slice_dir, fname) for fname in os.listdir(slice_dir) if fname.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        slice_path = self.slice_files[index]\n",
    "        image = Image.open(slice_path)\n",
    "        \n",
    "        if self.transform_64:\n",
    "            image_64 = self.transform_64(image)\n",
    "        if self.transform_256:\n",
    "            image_256 = self.transform_256(image)\n",
    "        \n",
    "        # Extract ID from filename to match with label\n",
    "        slice_id = os.path.basename(slice_path).split('_')[1]\n",
    "\n",
    "        if slice_id in self.labels_dict:\n",
    "            label = self.labels_dict[slice_id]\n",
    "            label = torch.FloatTensor([label])\n",
    "        else:\n",
    "            print(f\"Label for ID {slice_id} not found in the CSV file.\")\n",
    "            label = torch.FloatTensor([0])  \n",
    "\n",
    "        return {'data_64': image_64, 'data_256': image_256, 'label': label, 'id': slice_id}\n",
    "\n",
    "# Define transformations for 64x64 and 256x256\n",
    "transform_64 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((64, 64)),  # Resize to 64x64\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "transform_256 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "# Initialize the dataset\n",
    "root_dir = r\"/rds/homes/a/avv306/Raw_Images/Raw_Images\"\n",
    "valid_slice_dir = os.path.join(root_dir, \"valid_slices_raw\")\n",
    "\n",
    "valid_label_files = [\n",
    "    os.path.join(root_dir, \"valid-acl.csv\"),\n",
    "    os.path.join(root_dir, \"valid-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"valid-meniscus.csv\")\n",
    "]\n",
    "\n",
    "# Create a dataset for 64x64 and 256x256\n",
    "valid_dataset_64_256 = MRNetUpscaleDataset(\n",
    "    slice_dir=valid_slice_dir,\n",
    "    label_files=valid_label_files,\n",
    "    transform_64=transform_64,\n",
    "    transform_256=transform_256\n",
    ")\n",
    "\n",
    "# Create Subsets of the dataset for testing\n",
    "subset_indices = list(range(160))  # Use the first 160 images for example\n",
    "valid_subset_64_256 = Subset(valid_dataset_64_256, subset_indices)\n",
    "\n",
    "# Create DataLoader for the subset\n",
    "valid_loader_64_256 = DataLoader(valid_subset_64_256, batch_size=16, shuffle=True)\n",
    "\n",
    "# Initialize the torchmetrics objects\n",
    "psnr_metric = PeakSignalNoiseRatio().to(device)\n",
    "ssim_metric = StructuralSimilarityIndexMeasure().to(device)\n",
    "\n",
    "# Function to compute PSNR using torchmetrics\n",
    "def compute_psnr(img1, img2):\n",
    "    return psnr_metric(img1, img2).item()\n",
    "\n",
    "# Function to compute SSIM and DSSIM using torchmetrics\n",
    "def compute_ssim_dssim(img1, img2):\n",
    "    ssim_value = ssim_metric(img1, img2).item()\n",
    "    dssim_value = (1 - ssim_value) / 2\n",
    "    return ssim_value, dssim_value\n",
    "\n",
    "# Function to compute MSE using PyTorch\n",
    "def compute_mse(img1, img2):\n",
    "    return torch.mean((img1 - img2) ** 2).item()\n",
    "\n",
    "# Function to compute FSIM using image_similarity_measures\n",
    "def compute_fsim(img1, img2):\n",
    "    # Convert PyTorch tensors to NumPy arrays and scale to 8-bit integer\n",
    "    img1_np = img1.cpu().numpy().squeeze() * 255\n",
    "    img2_np = img2.cpu().numpy().squeeze() * 255\n",
    "\n",
    "    # Ensure the images are 3D (add a channel dimension if they're 2D)\n",
    "    if img1_np.ndim == 2:\n",
    "        img1_np = np.expand_dims(img1_np, axis=-1)\n",
    "    if img2_np.ndim == 2:\n",
    "        img2_np = np.expand_dims(img2_np, axis=-1)\n",
    "    \n",
    "    img1_uint8 = img1_np.astype(np.uint8)\n",
    "    img2_uint8 = img2_np.astype(np.uint8)\n",
    "\n",
    "    # Compute FSIM using the image_similarity_measures library\n",
    "    return fsim(img1_uint8, img2_uint8)\n",
    "\n",
    "# Function to perform interpolation from 64x64 directly to 256x256 and evaluate metrics\n",
    "def evaluate_direct_interpolation(interpolation_method, dataloader, device):\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    dssim_values = []\n",
    "    mse_values = []\n",
    "    fsim_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"Evaluating {interpolation_method} interpolation\"):\n",
    "            low_res = batch['data_64'].to(device)\n",
    "            high_res = batch['data_256'].to(device)\n",
    "\n",
    "            for i in range(low_res.size(0)):\n",
    "                low_res_img = low_res[i].unsqueeze(0)  # Add batch dimension\n",
    "                high_res_img = high_res[i].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "                # Apply interpolation directly from 64x64 to 256x256\n",
    "                low_res_img_np = low_res_img.cpu().numpy().squeeze()\n",
    "                if interpolation_method == 'bilinear':\n",
    "                    upsampled_img_np = cv2.resize(low_res_img_np, (256, 256), interpolation=cv2.INTER_LINEAR)\n",
    "                elif interpolation_method == 'bicubic':\n",
    "                    upsampled_img_np = cv2.resize(low_res_img_np, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "                elif interpolation_method == 'lanczos':\n",
    "                    upsampled_img_np = cv2.resize(low_res_img_np, (256, 256), interpolation=cv2.INTER_LANCZOS4)\n",
    "                else:  # default to nearest-neighbor\n",
    "                    upsampled_img_np = cv2.resize(low_res_img_np, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                # Convert back to torch tensor\n",
    "                upsampled_img = torch.tensor(upsampled_img_np, device=device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "                # Evaluate metrics\n",
    "                psnr_values.append(compute_psnr(high_res_img, upsampled_img))\n",
    "                ssim_value, dssim_value = compute_ssim_dssim(high_res_img, upsampled_img)\n",
    "                ssim_values.append(ssim_value)\n",
    "                dssim_values.append(dssim_value)\n",
    "                mse_values.append(compute_mse(high_res_img, upsampled_img))\n",
    "                fsim_values.append(compute_fsim(high_res_img, upsampled_img))\n",
    "\n",
    "    return np.mean(psnr_values), np.mean(ssim_values), np.mean(dssim_values), np.mean(mse_values), np.mean(fsim_values)\n",
    "\n",
    "\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# List of interpolation methods\n",
    "interpolation_methods = ['nearest', 'bilinear', 'bicubic', 'lanczos']\n",
    "\n",
    "# Evaluate all interpolation techniques directly from 64x64 to 256x256\n",
    "print(\"\\nEvaluating Direct Interpolation Techniques for 64x64 to 256x256:\")\n",
    "for method in interpolation_methods:\n",
    "    psnr256, ssim256, dssim256, mse256, fsim256 = evaluate_direct_interpolation(method, valid_loader_64_256, device)\n",
    "    print(f\"{method.capitalize()} Interpolation from 64x64 to 256x256: PSNR={psnr256}, SSIM={ssim256}, DSSIM={dssim256}, MSE={mse256}, FSIM={fsim256}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a966bc33-1109-4b28-829f-de510f4d4767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-40GB\n",
      "\n",
      "Evaluating Direct Interpolation Techniques for 64x64 to 256x256:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating nearest interpolation: 100%|██████████| 10/10 [00:01<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Interpolation from 64x64 to 256x256: VIF=1.08458410538733, LPIPS=0.4279451141133904, AG=0.15417876839637756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bilinear interpolation: 100%|██████████| 10/10 [00:01<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bilinear Interpolation from 64x64 to 256x256: VIF=1.5611980214715004, LPIPS=0.49902590084820986, AG=0.13009855151176453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bicubic interpolation: 100%|██████████| 10/10 [00:01<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bicubic Interpolation from 64x64 to 256x256: VIF=1.3168386608362197, LPIPS=0.47253904230892657, AG=0.14681777358055115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating lanczos interpolation: 100%|██████████| 10/10 [00:01<00:00,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lanczos Interpolation from 64x64 to 256x256: VIF=1.3331476621329785, LPIPS=0.4727118736132979, AG=0.147621750831604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity as LPIPS\n",
    "from torchmetrics.image import VisualInformationFidelity\n",
    "from scipy.ndimage import sobel\n",
    "\n",
    "\n",
    "class MRNetUpscaleDataset(Dataset):\n",
    "    def __init__(self, slice_dir, label_files, transform_64=None, transform_256=None):\n",
    "        super().__init__()\n",
    "        self.slice_dir = slice_dir\n",
    "        self.transform_64 = transform_64\n",
    "        self.transform_256 = transform_256\n",
    "\n",
    "        self.labels_dict = {}\n",
    "        for label_file in label_files:\n",
    "            records = pd.read_csv(label_file, header=None, names=['id', 'label'])\n",
    "            records['id'] = records['id'].map(lambda i: '0' * (4 - len(str(i))) + str(i))\n",
    "            self.labels_dict.update(dict(zip(records['id'], records['label'])))\n",
    "\n",
    "        # List all slice files\n",
    "        self.slice_files = [os.path.join(slice_dir, fname) for fname in os.listdir(slice_dir) if fname.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        slice_path = self.slice_files[index]\n",
    "        image = Image.open(slice_path)\n",
    "        \n",
    "        if self.transform_64:\n",
    "            image_64 = self.transform_64(image)\n",
    "        if self.transform_256:\n",
    "            image_256 = self.transform_256(image)\n",
    "        \n",
    "        # Extract ID from filename to match with label\n",
    "        slice_id = os.path.basename(slice_path).split('_')[1]\n",
    "\n",
    "        if slice_id in self.labels_dict:\n",
    "            label = self.labels_dict[slice_id]\n",
    "            label = torch.FloatTensor([label])\n",
    "        else:\n",
    "            print(f\"Label for ID {slice_id} not found in the CSV file.\")\n",
    "            label = torch.FloatTensor([0])  \n",
    "\n",
    "        return {'data_64': image_64, 'data_256': image_256, 'label': label, 'id': slice_id}\n",
    "\n",
    "# Define transformations for 64x64 and 256x256\n",
    "transform_64 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((64, 64)),  # Resize to 64x64\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "transform_256 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "# Initialize the dataset\n",
    "root_dir = r\"/rds/homes/a/avv306/Raw_Images/Raw_Images\"\n",
    "valid_slice_dir = os.path.join(root_dir, \"valid_slices_raw\")\n",
    "\n",
    "valid_label_files = [\n",
    "    os.path.join(root_dir, \"valid-acl.csv\"),\n",
    "    os.path.join(root_dir, \"valid-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"valid-meniscus.csv\")\n",
    "]\n",
    "\n",
    "# Create a dataset for 64x64 and 256x256\n",
    "valid_dataset_64_256 = MRNetUpscaleDataset(\n",
    "    slice_dir=valid_slice_dir,\n",
    "    label_files=valid_label_files,\n",
    "    transform_64=transform_64,\n",
    "    transform_256=transform_256\n",
    ")\n",
    "\n",
    "# Create Subsets of the dataset for testing\n",
    "subset_indices = list(range(160))  # Use the first 160 images for example\n",
    "valid_subset_64_256 = Subset(valid_dataset_64_256, subset_indices)\n",
    "\n",
    "# Create DataLoader for the subset\n",
    "valid_loader_64_256 = DataLoader(valid_subset_64_256, batch_size=16, shuffle=True)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Initialize PyTorch metric objects on the same device\n",
    "vif_metric = VisualInformationFidelity().to(device)\n",
    "lpips_metric = LPIPS(net_type='vgg').to(device) \n",
    "\n",
    "# Function to prepare images for LPIPS\n",
    "def prepare_for_lpips(img):\n",
    "    # Convert grayscale to 3 channels by repeating the channel 3 times\n",
    "    img_3c = img.repeat(1, 3, 1, 1)  # Repeat across the channel dimension\n",
    "    # Normalize the image to [-1, 1]\n",
    "    img_3c = (img_3c * 2) - 1\n",
    "    # Clip the values to ensure they stay within the expected range\n",
    "    img_3c = torch.clamp(img_3c, min=-1.0, max=1.0)\n",
    "    return img_3c\n",
    "\n",
    "# Function to compute VIF using PyTorch\n",
    "def compute_vif(img1, img2):\n",
    "    return vif_metric(img1, img2).item()\n",
    "\n",
    "# Function to compute LPIPS using PyTorch\n",
    "def compute_lpips(img1, img2):\n",
    "    img1 = prepare_for_lpips(img1)\n",
    "    img2 = prepare_for_lpips(img2)\n",
    "    return lpips_metric(img1, img2).item()\n",
    "\n",
    "# Function to compute AG (Average Gradient)\n",
    "def compute_ag(img):\n",
    "    img_np = img.cpu().numpy().squeeze()\n",
    "    gx = sobel(img_np, axis=0)\n",
    "    gy = sobel(img_np, axis=1)\n",
    "    grad_magnitude = np.sqrt(gx**2 + gy**2)\n",
    "    return np.mean(grad_magnitude)\n",
    "\n",
    "# Function to perform interpolation from 64x64 directly to 256x256 and evaluate metrics\n",
    "def evaluate_direct_interpolation(interpolation_method, dataloader, device):\n",
    "    vif_values = []\n",
    "    lpips_values = []\n",
    "    ag_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"Evaluating {interpolation_method} interpolation\"):\n",
    "            low_res = batch['data_64'].to(device)\n",
    "            high_res = batch['data_256'].to(device)\n",
    "\n",
    "            for i in range(low_res.size(0)):\n",
    "                low_res_img = low_res[i].unsqueeze(0)  # Add batch dimension\n",
    "                high_res_img = high_res[i].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "                # Apply interpolation directly from 64x64 to 256x256\n",
    "                low_res_img_np = low_res_img.cpu().numpy().squeeze()\n",
    "                if interpolation_method == 'bilinear':\n",
    "                    upsampled_img_np = cv2.resize(low_res_img_np, (256, 256), interpolation=cv2.INTER_LINEAR)\n",
    "                elif interpolation_method == 'bicubic':\n",
    "                    upsampled_img_np = cv2.resize(low_res_img_np, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "                elif interpolation_method == 'lanczos':\n",
    "                    upsampled_img_np = cv2.resize(low_res_img_np, (256, 256), interpolation=cv2.INTER_LANCZOS4)\n",
    "                else:  # default to nearest-neighbor\n",
    "                    upsampled_img_np = cv2.resize(low_res_img_np, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                # Convert back to torch tensor\n",
    "                upsampled_img = torch.tensor(upsampled_img_np, device=device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "                # Evaluate metrics\n",
    "                vif_values.append(compute_vif(high_res_img, upsampled_img))\n",
    "                lpips_values.append(compute_lpips(high_res_img, upsampled_img))\n",
    "                ag_values.append(compute_ag(upsampled_img))\n",
    "\n",
    "    return np.mean(vif_values), np.mean(lpips_values), np.mean(ag_values)\n",
    "\n",
    "# List of interpolation methods\n",
    "interpolation_methods = ['nearest', 'bilinear', 'bicubic', 'lanczos']\n",
    "\n",
    "# Evaluate all interpolation techniques directly from 64x64 to 256x256\n",
    "print(\"\\nEvaluating Direct Interpolation Techniques for 64x64 to 256x256:\")\n",
    "for method in interpolation_methods:\n",
    "    vif_256, lpips_256, ag_256 = evaluate_direct_interpolation(method, valid_loader_64_256, device)\n",
    "    print(f\"{method.capitalize()} Interpolation from 64x64 to 256x256: VIF={vif_256}, LPIPS={lpips_256}, AG={ag_256}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15b52866-69a2-43cc-ba44-d4f9d5e736b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-40GB\n",
      "Saved comparison image with SR at comparison_images/comparison_with_sr_0.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MRNetUpscaleDataset(Dataset):\n",
    "    def __init__(self, slice_dir, label_files, transform_64=None, transform_256=None):\n",
    "        super().__init__()\n",
    "        self.slice_dir = slice_dir\n",
    "        self.transform_64 = transform_64\n",
    "        self.transform_256 = transform_256\n",
    "\n",
    "        self.labels_dict = {}\n",
    "        for label_file in label_files:\n",
    "            records = pd.read_csv(label_file, header=None, names=['id', 'label'])\n",
    "            records['id'] = records['id'].map(lambda i: '0' * (4 - len(str(i))) + str(i))\n",
    "            self.labels_dict.update(dict(zip(records['id'], records['label'])))\n",
    "\n",
    "        # List all slice files\n",
    "        self.slice_files = [os.path.join(slice_dir, fname) for fname in os.listdir(slice_dir) if fname.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        slice_path = self.slice_files[index]\n",
    "        image = Image.open(slice_path)\n",
    "        \n",
    "        if self.transform_64:\n",
    "            image_64 = self.transform_64(image)\n",
    "        if self.transform_256:\n",
    "            image_256 = self.transform_256(image)\n",
    "        \n",
    "        # Extract ID from filename to match with label\n",
    "        slice_id = os.path.basename(slice_path).split('_')[1]\n",
    "\n",
    "        if slice_id in self.labels_dict:\n",
    "            label = self.labels_dict[slice_id]\n",
    "            label = torch.FloatTensor([label])\n",
    "        else:\n",
    "            print(f\"Label for ID {slice_id} not found in the CSV file.\")\n",
    "            label = torch.FloatTensor([0])  \n",
    "\n",
    "        return {'data_64': image_64, 'data_256': image_256, 'label': label, 'id': slice_id}\n",
    "\n",
    "# Define transformations for 64x64 and 256x256\n",
    "transform_64 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((64, 64)),  # Resize to 64x64\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "transform_256 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "# Initialize the dataset\n",
    "root_dir = r\"C:\\Users\\ASUS\\Documents\\Uobd\\project\\Development\\Datasets\\MRNet-v1.0\\Raw_Images\"\n",
    "valid_slice_dir = os.path.join(root_dir, \"valid_slices_raw\")\n",
    "\n",
    "valid_label_files = [\n",
    "    os.path.join(root_dir, \"valid-acl.csv\"),\n",
    "    os.path.join(root_dir, \"valid-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"valid-meniscus.csv\")\n",
    "]\n",
    "\n",
    "# Create a dataset for 64x64 and 256x256\n",
    "valid_dataset_64_256 = MRNetUpscaleDataset(\n",
    "    slice_dir=valid_slice_dir,\n",
    "    label_files=valid_label_files,\n",
    "    transform_64=transform_64,\n",
    "    transform_256=transform_256\n",
    ")\n",
    "\n",
    "# Create Subsets of the dataset for testing\n",
    "subset_indices = list(range(100))  # Use the first 100 images for example\n",
    "valid_subset_64_256 = Subset(valid_dataset_64_256, subset_indices)\n",
    "\n",
    "# Create DataLoader for the subset\n",
    "valid_loader_64_256 = DataLoader(valid_subset_64_256, batch_size=20, shuffle=True)\n",
    "\n",
    "# Function to perform interpolation from 64x64 directly to 256x256, generate super-res image, and save comparison as an image\n",
    "def evaluate_and_save_comparison_with_sr(dataloader, ddpm_128, ddpm_256, device, save_dir='trial'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # We'll process only one batch for display\n",
    "    batch = next(iter(dataloader))\n",
    "    \n",
    "    low_res = batch['data_64'].to(device)\n",
    "    high_res = batch['data_256'].to(device)\n",
    "\n",
    "    interpolation_methods = {\n",
    "        'Nearest': cv2.INTER_NEAREST,\n",
    "        'Bilinear': cv2.INTER_LINEAR,\n",
    "        'Bicubic': cv2.INTER_CUBIC,\n",
    "        'Lanczos': cv2.INTER_LANCZOS4\n",
    "    }\n",
    "\n",
    "    for i in range(low_res.size(0)):\n",
    "        low_res_img = low_res[i].cpu().numpy().squeeze()\n",
    "        high_res_img = high_res[i].cpu().numpy().squeeze()\n",
    "\n",
    "        # Create a figure to hold the comparison table\n",
    "        fig, axs = plt.subplots(1, len(interpolation_methods) + 3, figsize=(25, 5))\n",
    "        axs[0].imshow(low_res_img, cmap='gray')\n",
    "        axs[0].set_title('64x64 Low Res')\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        # Perform and display interpolation methods\n",
    "        for idx, (name, method) in enumerate(interpolation_methods.items(), start=1):\n",
    "            upsampled_img = cv2.resize(low_res_img, (256, 256), interpolation=method)\n",
    "            axs[idx].imshow(upsampled_img, cmap='gray')\n",
    "            axs[idx].set_title(f'{name} 256x256')\n",
    "            axs[idx].axis('off')\n",
    "            plt.imsave(os.path.join(save_dir, f'{name}_upsampled_{i}.png'), upsampled_img, cmap='gray')\n",
    "\n",
    "        # Perform super-resolution with the model\n",
    "        with torch.no_grad():\n",
    "            input_tensor = low_res[i].unsqueeze(0).to(device)\n",
    "            intermediate_image = ddpm_128.sample(input_tensor)\n",
    "            super_res_image = ddpm_256.sample(intermediate_image).cpu().numpy().squeeze()\n",
    "\n",
    "        axs[-2].imshow(super_res_image, cmap='gray')\n",
    "        axs[-2].set_title('NO SWIN Super-Resolution Model')\n",
    "        axs[-2].axis('off')\n",
    "\n",
    "        axs[-1].imshow(high_res_img, cmap='gray')\n",
    "        axs[-1].set_title('256x256 High Res')\n",
    "        axs[-1].axis('off')\n",
    "\n",
    "        # Save the comparison table as an image\n",
    "        comparison_image_path = os.path.join(save_dir, f'comparison_with_sr_{i}.png')\n",
    "        plt.savefig(comparison_image_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"Saved comparison image with SR at {comparison_image_path}\")\n",
    "        break  # Process only one image for comparison\n",
    "\n",
    "# Load the super-resolution models\n",
    "ddpm_64, ddpm_128, ddpm_256 = load_models()  # Ensure you load the models correctly\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Save and display the comparison for one image with SR and interpolation methods\n",
    "evaluate_and_save_comparison_with_sr(valid_loader_64_256, ddpm_128, ddpm_256, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
