{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de919491",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook details the implementation of a DDPM model for medical image synthesis, which has a UNet architecture augmented with self-attention and cross-attention mechanisms. \n",
    "\n",
    "This notebook walks you through:\n",
    "- Loading the Dataset\n",
    "- Construction of the attention-enhanced UNet model\n",
    "- Training of the DDPM\n",
    "\n",
    "This approach showcases a powerful method for medical image synthesis without the complexity of transformer layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2890803",
   "metadata": {},
   "source": [
    "##### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1675cbed-a2b9-4977-acb2-47bc55388704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for building and training the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6830a9b5",
   "metadata": {},
   "source": [
    "##### 2. Applying Transforms and Initializing Dataset Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf403831-137a-4d53-b841-25ece3fab3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "# Define transformations to preprocess the MRI images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Convert images to grayscale\n",
    "    transforms.Resize((64, 64)),  # Resize images to 64x64\n",
    "    transforms.ToTensor()  # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "\n",
    "class MRNetSliceDataset(Dataset):\n",
    "    def __init__(self, slice_dir, label_files, transform=None):\n",
    "        super().__init__()\n",
    "        self.slice_dir = slice_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Dictionary to store labels for each image ID\n",
    "        self.labels_dict = {}\n",
    "        for label_file in label_files:\n",
    "            records = pd.read_csv(label_file, header=None, names=['id', 'label'])\n",
    "            records['id'] = records['id'].map(lambda i: '0' * (4 - len(str(i))) + str(i))\n",
    "            self.labels_dict.update(dict(zip(records['id'], records['label'])))\n",
    "\n",
    "        # List all slice files in the directory\n",
    "        self.slice_files = [os.path.join(slice_dir, fname) for fname in os.listdir(slice_dir) if fname.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        slice_path = self.slice_files[index]\n",
    "        image = Image.open(slice_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Extract ID from the filename to find the corresponding label\n",
    "        slice_id = os.path.basename(slice_path).split('_')[1]\n",
    "\n",
    "        if slice_id in self.labels_dict:\n",
    "            label = self.labels_dict[slice_id]\n",
    "            label = torch.FloatTensor([label])\n",
    "        else:\n",
    "            print(f\"Label for ID {slice_id} not found in the CSV file.\")\n",
    "            label = torch.FloatTensor([0])  # Default label or handle missing labels as needed\n",
    "\n",
    "        return {'data': image, 'label': label, 'id': slice_id}\n",
    "\n",
    "# Initialize training and validation datasets and data loaders\n",
    "root_dir = \"Raw_Images\"\n",
    "train_slice_dir = os.path.join(root_dir, \"train_slices_raw\")\n",
    "valid_slice_dir = os.path.join(root_dir, \"valid_slices_raw\")\n",
    "\n",
    "train_label_files = [\n",
    "    os.path.join(root_dir, \"train-acl.csv\"),\n",
    "    os.path.join(root_dir, \"train-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"train-meniscus.csv\")\n",
    "]\n",
    "\n",
    "valid_label_files = [\n",
    "    os.path.join(root_dir, \"valid-acl.csv\"),\n",
    "    os.path.join(root_dir, \"valid-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"valid-meniscus.csv\")\n",
    "]\n",
    "\n",
    "train_dataset = MRNetSliceDataset(slice_dir=train_slice_dir, label_files=train_label_files, transform=transform)\n",
    "valid_dataset = MRNetSliceDataset(slice_dir=valid_slice_dir, label_files=valid_label_files, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b87be10",
   "metadata": {},
   "source": [
    "##### 3. Implementing the Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-attention block for feature refinement within the model\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SelfAttentionBlock, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, width, height = x.size()\n",
    "        proj_query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(batch_size, -1, width * height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "        proj_value = self.value_conv(x).view(batch_size, -1, width * height)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, C, width, height)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "# Cross-attention block for the decoder to focus on relevant features from the encoder\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(CrossAttentionBlock, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3481b",
   "metadata": {},
   "source": [
    "##### 4. Implementing the Attention UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a10fab5-00ea-42bd-9e38-468a4bd32d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinusoidal positional embedding for encoding timesteps in the DDPM\n",
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=10000):\n",
    "        super(SinusoidalPositionalEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, timesteps):\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        emb = math.log(self.max_len) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n",
    "        emb = timesteps[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "        if self.embedding_dim % 2 == 1:\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=1)\n",
    "        return emb\n",
    "\n",
    "# UNet with self-attention and cross-attention mechanisms\n",
    "class AttentionUNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=128):\n",
    "        super(AttentionUNet, self).__init__()\n",
    "\n",
    "        self.encoder1 = self.conv_block(in_channels + emb_dim, 64)\n",
    "        self.self_attention1 = SelfAttentionBlock(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.self_attention2 = SelfAttentionBlock(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.self_attention3 = SelfAttentionBlock(256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.cross_attention3 = CrossAttentionBlock(256, 256, 128)\n",
    "        self.decoder3 = self.conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.cross_attention2 = CrossAttentionBlock(128, 128, 64)\n",
    "        self.decoder2 = self.conv_block(256, 128)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.cross_attention1 = CrossAttentionBlock(64, 64, 32)\n",
    "        self.decoder1 = self.conv_block(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "        self.timestep_embedding_layer = SinusoidalPositionalEmbedding(emb_dim)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.timestep_embedding_layer(t)\n",
    "        t_embed = t_embed.view(t.size(0), -1, 1, 1)\n",
    "        t_embed = t_embed.repeat(1, 1, x.size(2), x.size(3))\n",
    "        x = torch.cat((x, t_embed), dim=1)\n",
    "\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc1 = self.self_attention1(enc1)\n",
    "        enc1_pooled = self.pool1(enc1)\n",
    "\n",
    "        enc2 = self.encoder2(enc1_pooled)\n",
    "        enc2 = self.self_attention2(enc2)\n",
    "        enc2_pooled = self.pool2(enc2)\n",
    "\n",
    "        enc3 = self.encoder3(enc2_pooled)\n",
    "        enc3 = self.self_attention3(enc3)\n",
    "        enc3_pooled = self.pool3(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc3_pooled)\n",
    "\n",
    "        # Decoder\n",
    "        upconv3 = self.upconv3(bottleneck)\n",
    "        enc3 = self.cross_attention3(upconv3, enc3)\n",
    "        dec3 = torch.cat((upconv3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        upconv2 = self.upconv2(dec3)\n",
    "        enc2 = self.cross_attention2(upconv2, enc2)\n",
    "        dec2 = torch.cat((upconv2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        upconv1 = self.upconv1(dec2)\n",
    "        enc1 = self.cross_attention1(upconv1, enc1)\n",
    "        dec1 = torch.cat((upconv1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        final_output = self.final_conv(dec1)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a26656",
   "metadata": {},
   "source": [
    "##### 5. Implementing the DDPM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1778e06d-1c6b-43dc-9c7f-2f314c25a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DDPM implementation adapted from:\n",
    "https://github.com/hojonathanho/diffusion/tree/master\n",
    "\"\"\"\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, model, num_timesteps, latent_dim, beta_start=0.00085, beta_end=0.0120):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.model = model\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', 1 - betas)\n",
    "        self.register_buffer('alphas_cumprod', torch.cumprod(1 - betas, dim=0))\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(self.alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - self.alphas_cumprod))\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def forward(self, z_t, t):\n",
    "        return self.model(z_t, t)\n",
    "\n",
    "    def sample_timesteps(self, batch_size):\n",
    "        return torch.randint(0, self.num_timesteps, (batch_size,)).to(device)\n",
    "\n",
    "    def forward_diffusion(self, z_0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(z_0)\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        return sqrt_alphas_cumprod_t * z_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_losses(self, z_0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(z_0)\n",
    "        z_t = self.forward_diffusion(z_0, t, noise)\n",
    "        predicted_noise = self.forward(z_t, t)\n",
    "        return nn.MSELoss()(noise, predicted_noise)\n",
    "\n",
    "    def sample(self, shape):\n",
    "        z_t = torch.randn(shape).to(device)\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "            t_tensor = torch.tensor([t], device=z_t.device).long()\n",
    "            alpha_t = self.alphas[t]\n",
    "            sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "            sqrt_one_minus_alpha_t = torch.sqrt(1 - self.alphas_cumprod[t])\n",
    "            beta_t = self.betas[t]\n",
    "\n",
    "            # Predict the noise\n",
    "            predicted_noise = self.forward(z_t, t_tensor)\n",
    "\n",
    "            # Remove the predicted noise\n",
    "            z_t = (z_t - beta_t / sqrt_one_minus_alpha_t * predicted_noise) / sqrt_alpha_t\n",
    "\n",
    "            # Add noise for non-final steps\n",
    "            if t > 0:\n",
    "                z_t += torch.randn_like(z_t) * torch.sqrt(beta_t)\n",
    "\n",
    "        return z_t\n",
    "\n",
    "    def p_sample(self, z, t):\n",
    "        predicted_noise = self.forward(z, t)\n",
    "        alpha_t = self.alphas[t]\n",
    "        sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "        sqrt_one_minus_alpha_t = torch.sqrt(1 - alpha_t)\n",
    "        z = (z - predicted_noise * (1 - alpha_t) / sqrt_one_minus_alpha_t) / sqrt_alpha_t\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e0b9ec",
   "metadata": {},
   "source": [
    "##### 6. Functions for Training and Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327304db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "def save_model(ddpm, epoch, checkpoint_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': ddpm.state_dict()\n",
    "    }, checkpoint_path)\n",
    "    print(f'Model checkpoint saved at {checkpoint_path}')\n",
    "\n",
    "# Load the model checkpoint\n",
    "def load_model(ddpm, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    ddpm.load_state_dict(checkpoint['model_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    return ddpm, start_epoch\n",
    "\n",
    "# Function to generate and save images\n",
    "def generate_and_save_images(ddpm, epoch, save_dir='generated_images/training'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    latent_dim = 128\n",
    "    shape = (1, 1, 64, 64)  # Change to 1 channel if using grayscale images\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    ddpm.eval()\n",
    "    with torch.no_grad():\n",
    "        # Sample from the DDPM in smaller batches to avoid memory issues\n",
    "        samples = ddpm.sample(shape)\n",
    "\n",
    "        # Convert to numpy and save images\n",
    "        samples = samples.squeeze().cpu().detach().numpy()\n",
    "        samples = (samples * 255).astype(np.uint8)\n",
    "        save_path = os.path.join(save_dir, f'generated_image_epoch_{epoch+1}.png')\n",
    "        Image.fromarray(samples, mode='L').save(save_path)\n",
    "        print(f'Image saved at {save_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ef4ba",
   "metadata": {},
   "source": [
    "##### 7. Training routine for the Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f81216a-5b1a-4610-a564-99ce3e130220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_diffusion_model(ddpm, train_loader, valid_loader, epochs=10, save_interval=10, checkpoint_path='ddpm_checkpoint.pth'):\n",
    "    optimizer = optim.AdamW(ddpm.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, verbose=True)\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Check if a checkpoint exists\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        ddpm, start_epoch = load_model(ddpm, checkpoint_path)\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "        print(f\"Resuming training from epoch {start_epoch+1}\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}, starting from scratch.\")\n",
    "\n",
    "    ddpm.to(device)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        ddpm.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs = batch['data'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            noise = torch.randn_like(inputs).to(device)\n",
    "            t = ddpm.sample_timesteps(inputs.size(0))\n",
    "            loss = ddpm.p_losses(inputs, t, noise)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient checking\n",
    "            total_norm = 0\n",
    "            for p in ddpm.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "\n",
    "            if total_norm > 1e3:  # Threshold for exploding gradients\n",
    "                print(f\"Warning: Exploding gradients detected at epoch {epoch+1}, total norm: {total_norm:.2f}\")\n",
    "                continue\n",
    "            if total_norm < 1e-3:  # Threshold for vanishing gradients\n",
    "                print(f\"Warning: Vanishing gradients detected at epoch {epoch+1}, total norm: {total_norm:.2f}\")\n",
    "                continue\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Gradient Norm: {total_norm:.2f}')\n",
    "\n",
    "        ddpm.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                inputs = batch['data'].to(device)\n",
    "                noise = torch.randn_like(inputs).to(device)\n",
    "                t = ddpm.sample_timesteps(inputs.size(0))\n",
    "                loss = ddpm.p_losses(inputs, t, noise)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        avg_valid_loss = valid_loss / len(valid_loader)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_valid_loss:.4f}')\n",
    "\n",
    "         # Learning Rate Scheduler\n",
    "        scheduler.step(avg_valid_loss)\n",
    "        \n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            save_model(ddpm, epoch, checkpoint_path)\n",
    "            generate_and_save_images(ddpm, epoch)\n",
    "            print(f'Model saved at epoch {epoch+1}')\n",
    "\n",
    "        # Special checkpoint save\n",
    "        if (epoch + 1) % 200 == 0:\n",
    "            special_checkpoint_path = checkpoint_path.replace(\".pth\", f\"_{epoch+1}.pth\")\n",
    "            save_model(ddpm, epoch, special_checkpoint_path)\n",
    "            print(f'Special checkpoint saved at epoch {epoch+1}')\n",
    "            \n",
    "    print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f9aba",
   "metadata": {},
   "source": [
    "##### 8. Running the Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a9ddc2f-4a6f-4872-8207-108ad49a3649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 110\n",
      "Epoch [110/400], Train Loss: 0.0069, Gradient Norm: 0.05\n",
      "Epoch [110/400], Validation Loss: 0.0067\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_110.png\n",
      "Model saved at epoch 110\n",
      "Epoch [111/400], Train Loss: 0.0064, Gradient Norm: 0.24\n",
      "Epoch [111/400], Validation Loss: 0.0074\n",
      "Epoch [112/400], Train Loss: 0.0068, Gradient Norm: 0.04\n",
      "Epoch [112/400], Validation Loss: 0.0072\n",
      "Epoch [113/400], Train Loss: 0.0073, Gradient Norm: 0.18\n",
      "Epoch [113/400], Validation Loss: 0.0068\n",
      "Epoch [114/400], Train Loss: 0.0064, Gradient Norm: 0.09\n",
      "Epoch [114/400], Validation Loss: 0.0071\n",
      "Epoch [115/400], Train Loss: 0.0066, Gradient Norm: 0.07\n",
      "Epoch [115/400], Validation Loss: 0.0074\n",
      "Epoch [116/400], Train Loss: 0.0068, Gradient Norm: 0.21\n",
      "Epoch [116/400], Validation Loss: 0.0076\n",
      "Epoch [117/400], Train Loss: 0.0067, Gradient Norm: 0.24\n",
      "Epoch [117/400], Validation Loss: 0.0071\n",
      "Epoch [118/400], Train Loss: 0.0066, Gradient Norm: 0.19\n",
      "Epoch [118/400], Validation Loss: 0.0086\n",
      "Epoch [119/400], Train Loss: 0.0066, Gradient Norm: 0.19\n",
      "Epoch [119/400], Validation Loss: 0.0082\n",
      "Epoch [120/400], Train Loss: 0.0067, Gradient Norm: 0.27\n",
      "Epoch [120/400], Validation Loss: 0.0084\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_120.png\n",
      "Model saved at epoch 120\n",
      "Epoch [121/400], Train Loss: 0.0064, Gradient Norm: 0.08\n",
      "Epoch [121/400], Validation Loss: 0.0069\n",
      "Epoch [122/400], Train Loss: 0.0066, Gradient Norm: 0.08\n",
      "Epoch [122/400], Validation Loss: 0.0062\n",
      "Epoch [123/400], Train Loss: 0.0064, Gradient Norm: 0.33\n",
      "Epoch [123/400], Validation Loss: 0.0075\n",
      "Epoch [124/400], Train Loss: 0.0064, Gradient Norm: 0.13\n",
      "Epoch [124/400], Validation Loss: 0.0075\n",
      "Epoch [125/400], Train Loss: 0.0068, Gradient Norm: 0.08\n",
      "Epoch [125/400], Validation Loss: 0.0081\n",
      "Epoch [126/400], Train Loss: 0.0063, Gradient Norm: 0.34\n",
      "Epoch [126/400], Validation Loss: 0.0070\n",
      "Epoch [127/400], Train Loss: 0.0070, Gradient Norm: 0.17\n",
      "Epoch [127/400], Validation Loss: 0.0058\n",
      "Epoch [128/400], Train Loss: 0.0062, Gradient Norm: 0.22\n",
      "Epoch [128/400], Validation Loss: 0.0084\n",
      "Epoch [129/400], Train Loss: 0.0063, Gradient Norm: 0.20\n",
      "Epoch [129/400], Validation Loss: 0.0064\n",
      "Epoch [130/400], Train Loss: 0.0065, Gradient Norm: 0.17\n",
      "Epoch [130/400], Validation Loss: 0.0080\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_130.png\n",
      "Model saved at epoch 130\n",
      "Epoch [131/400], Train Loss: 0.0065, Gradient Norm: 0.36\n",
      "Epoch [131/400], Validation Loss: 0.0074\n",
      "Epoch [132/400], Train Loss: 0.0061, Gradient Norm: 0.34\n",
      "Epoch [132/400], Validation Loss: 0.0069\n",
      "Epoch [133/400], Train Loss: 0.0063, Gradient Norm: 0.26\n",
      "Epoch [133/400], Validation Loss: 0.0072\n",
      "Epoch [134/400], Train Loss: 0.0061, Gradient Norm: 0.08\n",
      "Epoch [134/400], Validation Loss: 0.0065\n",
      "Epoch [135/400], Train Loss: 0.0062, Gradient Norm: 0.18\n",
      "Epoch [135/400], Validation Loss: 0.0060\n",
      "Epoch [136/400], Train Loss: 0.0063, Gradient Norm: 0.15\n",
      "Epoch [136/400], Validation Loss: 0.0075\n",
      "Epoch [137/400], Train Loss: 0.0067, Gradient Norm: 0.14\n",
      "Epoch [137/400], Validation Loss: 0.0063\n",
      "Epoch [138/400], Train Loss: 0.0065, Gradient Norm: 0.15\n",
      "Epoch [138/400], Validation Loss: 0.0063\n",
      "Epoch [139/400], Train Loss: 0.0062, Gradient Norm: 0.05\n",
      "Epoch [139/400], Validation Loss: 0.0071\n",
      "Epoch [140/400], Train Loss: 0.0059, Gradient Norm: 0.49\n",
      "Epoch [140/400], Validation Loss: 0.0069\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_140.png\n",
      "Model saved at epoch 140\n",
      "Epoch [141/400], Train Loss: 0.0063, Gradient Norm: 0.05\n",
      "Epoch [141/400], Validation Loss: 0.0072\n",
      "Epoch [142/400], Train Loss: 0.0062, Gradient Norm: 0.17\n",
      "Epoch [142/400], Validation Loss: 0.0068\n",
      "Epoch [143/400], Train Loss: 0.0064, Gradient Norm: 0.10\n",
      "Epoch [143/400], Validation Loss: 0.0075\n",
      "Epoch 00034: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch [144/400], Train Loss: 0.0055, Gradient Norm: 0.09\n",
      "Epoch [144/400], Validation Loss: 0.0061\n",
      "Epoch [145/400], Train Loss: 0.0059, Gradient Norm: 0.05\n",
      "Epoch [145/400], Validation Loss: 0.0060\n",
      "Epoch [146/400], Train Loss: 0.0058, Gradient Norm: 0.20\n",
      "Epoch [146/400], Validation Loss: 0.0068\n",
      "Epoch [147/400], Train Loss: 0.0058, Gradient Norm: 0.19\n",
      "Epoch [147/400], Validation Loss: 0.0065\n",
      "Epoch [148/400], Train Loss: 0.0058, Gradient Norm: 0.05\n",
      "Epoch [148/400], Validation Loss: 0.0064\n",
      "Epoch [149/400], Train Loss: 0.0061, Gradient Norm: 0.12\n",
      "Epoch [149/400], Validation Loss: 0.0067\n",
      "Epoch [150/400], Train Loss: 0.0060, Gradient Norm: 0.07\n",
      "Epoch [150/400], Validation Loss: 0.0071\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_150.png\n",
      "Model saved at epoch 150\n",
      "Epoch [151/400], Train Loss: 0.0062, Gradient Norm: 0.13\n",
      "Epoch [151/400], Validation Loss: 0.0059\n",
      "Epoch [152/400], Train Loss: 0.0060, Gradient Norm: 0.08\n",
      "Epoch [152/400], Validation Loss: 0.0064\n",
      "Epoch [153/400], Train Loss: 0.0059, Gradient Norm: 0.13\n",
      "Epoch [153/400], Validation Loss: 0.0070\n",
      "Epoch [154/400], Train Loss: 0.0061, Gradient Norm: 0.15\n",
      "Epoch [154/400], Validation Loss: 0.0069\n",
      "Epoch [155/400], Train Loss: 0.0056, Gradient Norm: 0.08\n",
      "Epoch [155/400], Validation Loss: 0.0064\n",
      "Epoch [156/400], Train Loss: 0.0057, Gradient Norm: 0.19\n",
      "Epoch [156/400], Validation Loss: 0.0064\n",
      "Epoch [157/400], Train Loss: 0.0057, Gradient Norm: 0.06\n",
      "Epoch [157/400], Validation Loss: 0.0061\n",
      "Epoch [158/400], Train Loss: 0.0056, Gradient Norm: 0.15\n",
      "Epoch [158/400], Validation Loss: 0.0070\n",
      "Epoch [159/400], Train Loss: 0.0057, Gradient Norm: 0.40\n",
      "Epoch [159/400], Validation Loss: 0.0063\n",
      "Epoch 00050: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch [160/400], Train Loss: 0.0057, Gradient Norm: 0.33\n",
      "Epoch [160/400], Validation Loss: 0.0070\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_160.png\n",
      "Model saved at epoch 160\n",
      "Epoch [161/400], Train Loss: 0.0059, Gradient Norm: 0.07\n",
      "Epoch [161/400], Validation Loss: 0.0064\n",
      "Epoch [162/400], Train Loss: 0.0058, Gradient Norm: 0.09\n",
      "Epoch [162/400], Validation Loss: 0.0065\n",
      "Epoch [163/400], Train Loss: 0.0058, Gradient Norm: 0.08\n",
      "Epoch [163/400], Validation Loss: 0.0064\n",
      "Epoch [164/400], Train Loss: 0.0056, Gradient Norm: 0.06\n",
      "Epoch [164/400], Validation Loss: 0.0064\n",
      "Epoch [165/400], Train Loss: 0.0056, Gradient Norm: 0.12\n",
      "Epoch [165/400], Validation Loss: 0.0063\n",
      "Epoch [166/400], Train Loss: 0.0055, Gradient Norm: 0.19\n",
      "Epoch [166/400], Validation Loss: 0.0057\n",
      "Epoch [167/400], Train Loss: 0.0058, Gradient Norm: 0.18\n",
      "Epoch [167/400], Validation Loss: 0.0056\n",
      "Epoch [168/400], Train Loss: 0.0056, Gradient Norm: 0.04\n",
      "Epoch [168/400], Validation Loss: 0.0080\n",
      "Epoch [169/400], Train Loss: 0.0054, Gradient Norm: 0.09\n",
      "Epoch [169/400], Validation Loss: 0.0065\n",
      "Epoch [170/400], Train Loss: 0.0057, Gradient Norm: 0.12\n",
      "Epoch [170/400], Validation Loss: 0.0056\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_170.png\n",
      "Model saved at epoch 170\n",
      "Epoch [171/400], Train Loss: 0.0052, Gradient Norm: 0.05\n",
      "Epoch [171/400], Validation Loss: 0.0065\n",
      "Epoch [172/400], Train Loss: 0.0053, Gradient Norm: 0.07\n",
      "Epoch [172/400], Validation Loss: 0.0072\n",
      "Epoch [173/400], Train Loss: 0.0057, Gradient Norm: 0.09\n",
      "Epoch [173/400], Validation Loss: 0.0070\n",
      "Epoch [174/400], Train Loss: 0.0055, Gradient Norm: 0.11\n",
      "Epoch [174/400], Validation Loss: 0.0052\n",
      "Epoch [175/400], Train Loss: 0.0055, Gradient Norm: 0.05\n",
      "Epoch [175/400], Validation Loss: 0.0070\n",
      "Epoch [176/400], Train Loss: 0.0054, Gradient Norm: 0.11\n",
      "Epoch [176/400], Validation Loss: 0.0069\n",
      "Epoch [177/400], Train Loss: 0.0055, Gradient Norm: 0.14\n",
      "Epoch [177/400], Validation Loss: 0.0072\n",
      "Epoch [178/400], Train Loss: 0.0057, Gradient Norm: 0.07\n",
      "Epoch [178/400], Validation Loss: 0.0067\n",
      "Epoch [179/400], Train Loss: 0.0059, Gradient Norm: 0.17\n",
      "Epoch [179/400], Validation Loss: 0.0066\n",
      "Epoch [180/400], Train Loss: 0.0054, Gradient Norm: 0.14\n",
      "Epoch [180/400], Validation Loss: 0.0065\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_180.png\n",
      "Model saved at epoch 180\n",
      "Epoch [181/400], Train Loss: 0.0059, Gradient Norm: 0.10\n",
      "Epoch [181/400], Validation Loss: 0.0065\n",
      "Epoch [182/400], Train Loss: 0.0058, Gradient Norm: 0.07\n",
      "Epoch [182/400], Validation Loss: 0.0068\n",
      "Epoch [183/400], Train Loss: 0.0053, Gradient Norm: 0.15\n",
      "Epoch [183/400], Validation Loss: 0.0063\n",
      "Epoch [184/400], Train Loss: 0.0055, Gradient Norm: 0.07\n",
      "Epoch [184/400], Validation Loss: 0.0067\n",
      "Epoch [185/400], Train Loss: 0.0055, Gradient Norm: 0.07\n",
      "Epoch [185/400], Validation Loss: 0.0069\n",
      "Epoch [186/400], Train Loss: 0.0056, Gradient Norm: 0.15\n",
      "Epoch [186/400], Validation Loss: 0.0064\n",
      "Epoch [187/400], Train Loss: 0.0054, Gradient Norm: 0.06\n",
      "Epoch [187/400], Validation Loss: 0.0063\n",
      "Epoch [188/400], Train Loss: 0.0056, Gradient Norm: 0.21\n",
      "Epoch [188/400], Validation Loss: 0.0063\n",
      "Epoch [189/400], Train Loss: 0.0055, Gradient Norm: 0.20\n",
      "Epoch [189/400], Validation Loss: 0.0062\n",
      "Epoch [190/400], Train Loss: 0.0054, Gradient Norm: 0.11\n",
      "Epoch [190/400], Validation Loss: 0.0064\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_190.png\n",
      "Model saved at epoch 190\n",
      "Epoch [191/400], Train Loss: 0.0053, Gradient Norm: 0.05\n",
      "Epoch [191/400], Validation Loss: 0.0074\n",
      "Epoch [192/400], Train Loss: 0.0053, Gradient Norm: 0.04\n",
      "Epoch [192/400], Validation Loss: 0.0069\n",
      "Epoch [193/400], Train Loss: 0.0054, Gradient Norm: 0.04\n",
      "Epoch [193/400], Validation Loss: 0.0070\n",
      "Epoch [194/400], Train Loss: 0.0054, Gradient Norm: 0.24\n",
      "Epoch [194/400], Validation Loss: 0.0068\n",
      "Epoch [195/400], Train Loss: 0.0055, Gradient Norm: 0.10\n",
      "Epoch [195/400], Validation Loss: 0.0067\n",
      "Epoch [196/400], Train Loss: 0.0056, Gradient Norm: 0.04\n",
      "Epoch [196/400], Validation Loss: 0.0073\n",
      "Epoch [197/400], Train Loss: 0.0053, Gradient Norm: 0.03\n",
      "Epoch [197/400], Validation Loss: 0.0071\n",
      "Epoch [198/400], Train Loss: 0.0054, Gradient Norm: 0.04\n",
      "Epoch [198/400], Validation Loss: 0.0066\n",
      "Epoch [199/400], Train Loss: 0.0052, Gradient Norm: 0.07\n",
      "Epoch [199/400], Validation Loss: 0.0066\n",
      "Epoch [200/400], Train Loss: 0.0055, Gradient Norm: 0.10\n",
      "Epoch [200/400], Validation Loss: 0.0067\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_200.png\n",
      "Model saved at epoch 200\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint_200.pth\n",
      "Special checkpoint saved at epoch 200\n",
      "Epoch [201/400], Train Loss: 0.0053, Gradient Norm: 0.10\n",
      "Epoch [201/400], Validation Loss: 0.0065\n",
      "Epoch [202/400], Train Loss: 0.0052, Gradient Norm: 0.13\n",
      "Epoch [202/400], Validation Loss: 0.0064\n",
      "Epoch [203/400], Train Loss: 0.0053, Gradient Norm: 0.12\n",
      "Epoch [203/400], Validation Loss: 0.0060\n",
      "Epoch [204/400], Train Loss: 0.0053, Gradient Norm: 0.09\n",
      "Epoch [204/400], Validation Loss: 0.0065\n",
      "Epoch [205/400], Train Loss: 0.0053, Gradient Norm: 0.08\n",
      "Epoch [205/400], Validation Loss: 0.0061\n",
      "Epoch [206/400], Train Loss: 0.0055, Gradient Norm: 0.07\n",
      "Epoch [206/400], Validation Loss: 0.0069\n",
      "Epoch 00097: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch [207/400], Train Loss: 0.0054, Gradient Norm: 0.18\n",
      "Epoch [207/400], Validation Loss: 0.0070\n",
      "Epoch [208/400], Train Loss: 0.0053, Gradient Norm: 0.08\n",
      "Epoch [208/400], Validation Loss: 0.0069\n",
      "Epoch [209/400], Train Loss: 0.0053, Gradient Norm: 0.10\n",
      "Epoch [209/400], Validation Loss: 0.0069\n",
      "Epoch [210/400], Train Loss: 0.0054, Gradient Norm: 0.08\n",
      "Epoch [210/400], Validation Loss: 0.0066\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_210.png\n",
      "Model saved at epoch 210\n",
      "Epoch [211/400], Train Loss: 0.0053, Gradient Norm: 0.05\n",
      "Epoch [211/400], Validation Loss: 0.0070\n",
      "Epoch [212/400], Train Loss: 0.0051, Gradient Norm: 0.10\n",
      "Epoch [212/400], Validation Loss: 0.0064\n",
      "Epoch [213/400], Train Loss: 0.0054, Gradient Norm: 0.10\n",
      "Epoch [213/400], Validation Loss: 0.0061\n",
      "Epoch [214/400], Train Loss: 0.0052, Gradient Norm: 0.10\n",
      "Epoch [214/400], Validation Loss: 0.0063\n",
      "Epoch [215/400], Train Loss: 0.0054, Gradient Norm: 0.11\n",
      "Epoch [215/400], Validation Loss: 0.0077\n",
      "Epoch [216/400], Train Loss: 0.0055, Gradient Norm: 0.11\n",
      "Epoch [216/400], Validation Loss: 0.0065\n",
      "Epoch [217/400], Train Loss: 0.0053, Gradient Norm: 0.10\n",
      "Epoch [217/400], Validation Loss: 0.0065\n",
      "Epoch [218/400], Train Loss: 0.0050, Gradient Norm: 0.03\n",
      "Epoch [218/400], Validation Loss: 0.0060\n",
      "Epoch [219/400], Train Loss: 0.0052, Gradient Norm: 0.23\n",
      "Epoch [219/400], Validation Loss: 0.0057\n",
      "Epoch [220/400], Train Loss: 0.0051, Gradient Norm: 0.05\n",
      "Epoch [220/400], Validation Loss: 0.0074\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_220.png\n",
      "Model saved at epoch 220\n",
      "Epoch [221/400], Train Loss: 0.0051, Gradient Norm: 0.13\n",
      "Epoch [221/400], Validation Loss: 0.0075\n",
      "Epoch [222/400], Train Loss: 0.0052, Gradient Norm: 0.07\n",
      "Epoch [222/400], Validation Loss: 0.0064\n",
      "Epoch 00113: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Epoch [223/400], Train Loss: 0.0051, Gradient Norm: 0.10\n",
      "Epoch [223/400], Validation Loss: 0.0075\n",
      "Epoch [224/400], Train Loss: 0.0051, Gradient Norm: 0.17\n",
      "Epoch [224/400], Validation Loss: 0.0070\n",
      "Epoch [225/400], Train Loss: 0.0048, Gradient Norm: 0.08\n",
      "Epoch [225/400], Validation Loss: 0.0059\n",
      "Epoch [226/400], Train Loss: 0.0051, Gradient Norm: 0.04\n",
      "Epoch [226/400], Validation Loss: 0.0065\n",
      "Epoch [227/400], Train Loss: 0.0052, Gradient Norm: 0.07\n",
      "Epoch [227/400], Validation Loss: 0.0071\n",
      "Epoch [228/400], Train Loss: 0.0051, Gradient Norm: 0.07\n",
      "Epoch [228/400], Validation Loss: 0.0060\n",
      "Epoch [229/400], Train Loss: 0.0051, Gradient Norm: 0.12\n",
      "Epoch [229/400], Validation Loss: 0.0060\n",
      "Epoch [230/400], Train Loss: 0.0050, Gradient Norm: 0.10\n",
      "Epoch [230/400], Validation Loss: 0.0062\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_230.png\n",
      "Model saved at epoch 230\n",
      "Epoch [231/400], Train Loss: 0.0053, Gradient Norm: 0.06\n",
      "Epoch [231/400], Validation Loss: 0.0061\n",
      "Epoch [232/400], Train Loss: 0.0055, Gradient Norm: 0.12\n",
      "Epoch [232/400], Validation Loss: 0.0066\n",
      "Epoch [233/400], Train Loss: 0.0052, Gradient Norm: 0.24\n",
      "Epoch [233/400], Validation Loss: 0.0052\n",
      "Epoch [234/400], Train Loss: 0.0052, Gradient Norm: 0.12\n",
      "Epoch [234/400], Validation Loss: 0.0065\n",
      "Epoch [235/400], Train Loss: 0.0054, Gradient Norm: 0.37\n",
      "Epoch [235/400], Validation Loss: 0.0063\n",
      "Epoch [236/400], Train Loss: 0.0054, Gradient Norm: 0.05\n",
      "Epoch [236/400], Validation Loss: 0.0068\n",
      "Epoch [237/400], Train Loss: 0.0051, Gradient Norm: 0.11\n",
      "Epoch [237/400], Validation Loss: 0.0057\n",
      "Epoch [238/400], Train Loss: 0.0053, Gradient Norm: 0.12\n",
      "Epoch [238/400], Validation Loss: 0.0071\n",
      "Epoch [239/400], Train Loss: 0.0054, Gradient Norm: 0.05\n",
      "Epoch [239/400], Validation Loss: 0.0056\n",
      "Epoch [240/400], Train Loss: 0.0051, Gradient Norm: 0.09\n",
      "Epoch [240/400], Validation Loss: 0.0062\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_240.png\n",
      "Model saved at epoch 240\n",
      "Epoch [241/400], Train Loss: 0.0050, Gradient Norm: 0.03\n",
      "Epoch [241/400], Validation Loss: 0.0067\n",
      "Epoch [242/400], Train Loss: 0.0053, Gradient Norm: 0.08\n",
      "Epoch [242/400], Validation Loss: 0.0076\n",
      "Epoch [243/400], Train Loss: 0.0053, Gradient Norm: 0.05\n",
      "Epoch [243/400], Validation Loss: 0.0073\n",
      "Epoch [244/400], Train Loss: 0.0050, Gradient Norm: 0.14\n",
      "Epoch [244/400], Validation Loss: 0.0063\n",
      "Epoch [245/400], Train Loss: 0.0056, Gradient Norm: 0.05\n",
      "Epoch [245/400], Validation Loss: 0.0066\n",
      "Epoch [246/400], Train Loss: 0.0055, Gradient Norm: 0.14\n",
      "Epoch [246/400], Validation Loss: 0.0059\n",
      "Epoch [247/400], Train Loss: 0.0049, Gradient Norm: 0.12\n",
      "Epoch [247/400], Validation Loss: 0.0068\n",
      "Epoch [248/400], Train Loss: 0.0051, Gradient Norm: 0.14\n",
      "Epoch [248/400], Validation Loss: 0.0062\n",
      "Epoch [249/400], Train Loss: 0.0051, Gradient Norm: 0.06\n",
      "Epoch [249/400], Validation Loss: 0.0064\n",
      "Epoch 00140: reducing learning rate of group 0 to 1.5625e-06.\n",
      "Epoch [250/400], Train Loss: 0.0051, Gradient Norm: 0.05\n",
      "Epoch [250/400], Validation Loss: 0.0065\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_250.png\n",
      "Model saved at epoch 250\n",
      "Epoch [251/400], Train Loss: 0.0051, Gradient Norm: 0.04\n",
      "Epoch [251/400], Validation Loss: 0.0067\n",
      "Epoch [252/400], Train Loss: 0.0052, Gradient Norm: 0.05\n",
      "Epoch [252/400], Validation Loss: 0.0063\n",
      "Epoch [253/400], Train Loss: 0.0054, Gradient Norm: 0.11\n",
      "Epoch [253/400], Validation Loss: 0.0070\n",
      "Epoch [254/400], Train Loss: 0.0055, Gradient Norm: 0.11\n",
      "Epoch [254/400], Validation Loss: 0.0054\n",
      "Epoch [255/400], Train Loss: 0.0056, Gradient Norm: 0.08\n",
      "Epoch [255/400], Validation Loss: 0.0057\n",
      "Epoch [256/400], Train Loss: 0.0051, Gradient Norm: 0.05\n",
      "Epoch [256/400], Validation Loss: 0.0062\n",
      "Epoch [257/400], Train Loss: 0.0052, Gradient Norm: 0.06\n",
      "Epoch [257/400], Validation Loss: 0.0068\n",
      "Epoch [258/400], Train Loss: 0.0053, Gradient Norm: 0.04\n",
      "Epoch [258/400], Validation Loss: 0.0064\n",
      "Epoch [259/400], Train Loss: 0.0050, Gradient Norm: 0.06\n",
      "Epoch [259/400], Validation Loss: 0.0062\n",
      "Epoch [260/400], Train Loss: 0.0052, Gradient Norm: 0.12\n",
      "Epoch [260/400], Validation Loss: 0.0062\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_260.png\n",
      "Model saved at epoch 260\n",
      "Epoch [261/400], Train Loss: 0.0053, Gradient Norm: 0.07\n",
      "Epoch [261/400], Validation Loss: 0.0062\n",
      "Epoch [262/400], Train Loss: 0.0052, Gradient Norm: 0.02\n",
      "Epoch [262/400], Validation Loss: 0.0069\n",
      "Epoch [263/400], Train Loss: 0.0053, Gradient Norm: 0.08\n",
      "Epoch [263/400], Validation Loss: 0.0059\n",
      "Epoch [264/400], Train Loss: 0.0052, Gradient Norm: 0.20\n",
      "Epoch [264/400], Validation Loss: 0.0069\n",
      "Epoch [265/400], Train Loss: 0.0054, Gradient Norm: 0.07\n",
      "Epoch [265/400], Validation Loss: 0.0068\n",
      "Epoch 00156: reducing learning rate of group 0 to 7.8125e-07.\n",
      "Epoch [266/400], Train Loss: 0.0053, Gradient Norm: 0.04\n",
      "Epoch [266/400], Validation Loss: 0.0076\n",
      "Epoch [267/400], Train Loss: 0.0052, Gradient Norm: 0.07\n",
      "Epoch [267/400], Validation Loss: 0.0066\n",
      "Epoch [268/400], Train Loss: 0.0053, Gradient Norm: 0.07\n",
      "Epoch [268/400], Validation Loss: 0.0068\n",
      "Epoch [269/400], Train Loss: 0.0053, Gradient Norm: 0.07\n",
      "Epoch [269/400], Validation Loss: 0.0059\n",
      "Epoch [270/400], Train Loss: 0.0051, Gradient Norm: 0.08\n",
      "Epoch [270/400], Validation Loss: 0.0067\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_270.png\n",
      "Model saved at epoch 270\n",
      "Epoch [271/400], Train Loss: 0.0049, Gradient Norm: 0.11\n",
      "Epoch [271/400], Validation Loss: 0.0065\n",
      "Epoch [272/400], Train Loss: 0.0050, Gradient Norm: 0.16\n",
      "Epoch [272/400], Validation Loss: 0.0055\n",
      "Epoch [273/400], Train Loss: 0.0053, Gradient Norm: 0.21\n",
      "Epoch [273/400], Validation Loss: 0.0075\n",
      "Epoch [274/400], Train Loss: 0.0053, Gradient Norm: 0.03\n",
      "Epoch [274/400], Validation Loss: 0.0064\n",
      "Epoch [275/400], Train Loss: 0.0052, Gradient Norm: 0.07\n",
      "Epoch [275/400], Validation Loss: 0.0059\n",
      "Epoch [276/400], Train Loss: 0.0056, Gradient Norm: 0.09\n",
      "Epoch [276/400], Validation Loss: 0.0065\n",
      "Epoch [277/400], Train Loss: 0.0052, Gradient Norm: 0.16\n",
      "Epoch [277/400], Validation Loss: 0.0059\n",
      "Epoch [278/400], Train Loss: 0.0050, Gradient Norm: 0.07\n",
      "Epoch [278/400], Validation Loss: 0.0059\n",
      "Epoch [279/400], Train Loss: 0.0052, Gradient Norm: 0.03\n",
      "Epoch [279/400], Validation Loss: 0.0073\n",
      "Epoch [280/400], Train Loss: 0.0051, Gradient Norm: 0.07\n",
      "Epoch [280/400], Validation Loss: 0.0061\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_280.png\n",
      "Model saved at epoch 280\n",
      "Epoch [281/400], Train Loss: 0.0051, Gradient Norm: 0.08\n",
      "Epoch [281/400], Validation Loss: 0.0061\n",
      "Epoch 00172: reducing learning rate of group 0 to 3.9063e-07.\n",
      "Epoch [282/400], Train Loss: 0.0053, Gradient Norm: 0.07\n",
      "Epoch [282/400], Validation Loss: 0.0066\n",
      "Epoch [283/400], Train Loss: 0.0054, Gradient Norm: 0.08\n",
      "Epoch [283/400], Validation Loss: 0.0072\n",
      "Epoch [284/400], Train Loss: 0.0052, Gradient Norm: 0.08\n",
      "Epoch [284/400], Validation Loss: 0.0071\n",
      "Epoch [285/400], Train Loss: 0.0051, Gradient Norm: 0.17\n",
      "Epoch [285/400], Validation Loss: 0.0069\n",
      "Epoch [286/400], Train Loss: 0.0052, Gradient Norm: 0.11\n",
      "Epoch [286/400], Validation Loss: 0.0059\n",
      "Epoch [287/400], Train Loss: 0.0055, Gradient Norm: 0.03\n",
      "Epoch [287/400], Validation Loss: 0.0067\n",
      "Epoch [288/400], Train Loss: 0.0053, Gradient Norm: 0.06\n",
      "Epoch [288/400], Validation Loss: 0.0061\n",
      "Epoch [289/400], Train Loss: 0.0052, Gradient Norm: 0.04\n",
      "Epoch [289/400], Validation Loss: 0.0058\n",
      "Epoch [290/400], Train Loss: 0.0051, Gradient Norm: 0.09\n",
      "Epoch [290/400], Validation Loss: 0.0057\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_290.png\n",
      "Model saved at epoch 290\n",
      "Epoch [291/400], Train Loss: 0.0054, Gradient Norm: 0.04\n",
      "Epoch [291/400], Validation Loss: 0.0061\n",
      "Epoch [292/400], Train Loss: 0.0052, Gradient Norm: 0.09\n",
      "Epoch [292/400], Validation Loss: 0.0072\n",
      "Epoch [293/400], Train Loss: 0.0051, Gradient Norm: 0.21\n",
      "Epoch [293/400], Validation Loss: 0.0068\n",
      "Epoch [294/400], Train Loss: 0.0053, Gradient Norm: 0.12\n",
      "Epoch [294/400], Validation Loss: 0.0067\n",
      "Epoch [295/400], Train Loss: 0.0054, Gradient Norm: 0.04\n",
      "Epoch [295/400], Validation Loss: 0.0062\n",
      "Epoch [296/400], Train Loss: 0.0050, Gradient Norm: 0.09\n",
      "Epoch [296/400], Validation Loss: 0.0071\n",
      "Epoch [297/400], Train Loss: 0.0050, Gradient Norm: 0.11\n",
      "Epoch [297/400], Validation Loss: 0.0066\n",
      "Epoch 00188: reducing learning rate of group 0 to 1.9531e-07.\n",
      "Epoch [298/400], Train Loss: 0.0055, Gradient Norm: 0.06\n",
      "Epoch [298/400], Validation Loss: 0.0061\n",
      "Epoch [299/400], Train Loss: 0.0050, Gradient Norm: 0.09\n",
      "Epoch [299/400], Validation Loss: 0.0071\n",
      "Epoch [300/400], Train Loss: 0.0051, Gradient Norm: 0.07\n",
      "Epoch [300/400], Validation Loss: 0.0064\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_300.png\n",
      "Model saved at epoch 300\n",
      "Epoch [301/400], Train Loss: 0.0055, Gradient Norm: 0.07\n",
      "Epoch [301/400], Validation Loss: 0.0064\n",
      "Epoch [302/400], Train Loss: 0.0054, Gradient Norm: 0.07\n",
      "Epoch [302/400], Validation Loss: 0.0067\n",
      "Epoch [303/400], Train Loss: 0.0053, Gradient Norm: 0.14\n",
      "Epoch [303/400], Validation Loss: 0.0058\n",
      "Epoch [304/400], Train Loss: 0.0052, Gradient Norm: 0.07\n",
      "Epoch [304/400], Validation Loss: 0.0074\n",
      "Epoch [305/400], Train Loss: 0.0050, Gradient Norm: 0.10\n",
      "Epoch [305/400], Validation Loss: 0.0055\n",
      "Epoch [306/400], Train Loss: 0.0054, Gradient Norm: 0.05\n",
      "Epoch [306/400], Validation Loss: 0.0065\n",
      "Epoch [307/400], Train Loss: 0.0053, Gradient Norm: 0.10\n",
      "Epoch [307/400], Validation Loss: 0.0066\n",
      "Epoch [308/400], Train Loss: 0.0052, Gradient Norm: 0.21\n",
      "Epoch [308/400], Validation Loss: 0.0070\n",
      "Epoch [309/400], Train Loss: 0.0052, Gradient Norm: 0.02\n",
      "Epoch [309/400], Validation Loss: 0.0075\n",
      "Epoch [310/400], Train Loss: 0.0051, Gradient Norm: 0.05\n",
      "Epoch [310/400], Validation Loss: 0.0068\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_310.png\n",
      "Model saved at epoch 310\n",
      "Epoch [311/400], Train Loss: 0.0054, Gradient Norm: 0.12\n",
      "Epoch [311/400], Validation Loss: 0.0061\n",
      "Epoch [312/400], Train Loss: 0.0052, Gradient Norm: 0.08\n",
      "Epoch [312/400], Validation Loss: 0.0060\n",
      "Epoch [313/400], Train Loss: 0.0047, Gradient Norm: 0.05\n",
      "Epoch [313/400], Validation Loss: 0.0068\n",
      "Epoch 00204: reducing learning rate of group 0 to 9.7656e-08.\n",
      "Epoch [314/400], Train Loss: 0.0051, Gradient Norm: 0.07\n",
      "Epoch [314/400], Validation Loss: 0.0070\n",
      "Epoch [315/400], Train Loss: 0.0052, Gradient Norm: 0.04\n",
      "Epoch [315/400], Validation Loss: 0.0065\n",
      "Epoch [316/400], Train Loss: 0.0049, Gradient Norm: 0.08\n",
      "Epoch [316/400], Validation Loss: 0.0058\n",
      "Epoch [317/400], Train Loss: 0.0051, Gradient Norm: 0.08\n",
      "Epoch [317/400], Validation Loss: 0.0060\n",
      "Epoch [318/400], Train Loss: 0.0051, Gradient Norm: 0.05\n",
      "Epoch [318/400], Validation Loss: 0.0071\n",
      "Epoch [319/400], Train Loss: 0.0055, Gradient Norm: 0.05\n",
      "Epoch [319/400], Validation Loss: 0.0065\n",
      "Epoch [320/400], Train Loss: 0.0053, Gradient Norm: 0.12\n",
      "Epoch [320/400], Validation Loss: 0.0065\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_320.png\n",
      "Model saved at epoch 320\n",
      "Epoch [321/400], Train Loss: 0.0054, Gradient Norm: 0.17\n",
      "Epoch [321/400], Validation Loss: 0.0058\n",
      "Epoch [322/400], Train Loss: 0.0054, Gradient Norm: 0.14\n",
      "Epoch [322/400], Validation Loss: 0.0073\n",
      "Epoch [323/400], Train Loss: 0.0052, Gradient Norm: 0.03\n",
      "Epoch [323/400], Validation Loss: 0.0074\n",
      "Epoch [324/400], Train Loss: 0.0052, Gradient Norm: 0.07\n",
      "Epoch [324/400], Validation Loss: 0.0072\n",
      "Epoch [325/400], Train Loss: 0.0052, Gradient Norm: 0.03\n",
      "Epoch [325/400], Validation Loss: 0.0072\n",
      "Epoch [326/400], Train Loss: 0.0050, Gradient Norm: 0.06\n",
      "Epoch [326/400], Validation Loss: 0.0061\n",
      "Epoch [327/400], Train Loss: 0.0054, Gradient Norm: 0.03\n",
      "Epoch [327/400], Validation Loss: 0.0061\n",
      "Epoch [328/400], Train Loss: 0.0054, Gradient Norm: 0.03\n",
      "Epoch [328/400], Validation Loss: 0.0065\n",
      "Epoch [329/400], Train Loss: 0.0053, Gradient Norm: 0.04\n",
      "Epoch [329/400], Validation Loss: 0.0063\n",
      "Epoch 00220: reducing learning rate of group 0 to 4.8828e-08.\n",
      "Epoch [330/400], Train Loss: 0.0052, Gradient Norm: 0.13\n",
      "Epoch [330/400], Validation Loss: 0.0069\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_330.png\n",
      "Model saved at epoch 330\n",
      "Epoch [331/400], Train Loss: 0.0052, Gradient Norm: 0.12\n",
      "Epoch [331/400], Validation Loss: 0.0064\n",
      "Epoch [332/400], Train Loss: 0.0051, Gradient Norm: 0.07\n",
      "Epoch [332/400], Validation Loss: 0.0057\n",
      "Epoch [333/400], Train Loss: 0.0056, Gradient Norm: 0.06\n",
      "Epoch [333/400], Validation Loss: 0.0068\n",
      "Epoch [334/400], Train Loss: 0.0051, Gradient Norm: 0.06\n",
      "Epoch [334/400], Validation Loss: 0.0071\n",
      "Epoch [335/400], Train Loss: 0.0054, Gradient Norm: 0.35\n",
      "Epoch [335/400], Validation Loss: 0.0065\n",
      "Epoch [336/400], Train Loss: 0.0051, Gradient Norm: 0.07\n",
      "Epoch [336/400], Validation Loss: 0.0059\n",
      "Epoch [337/400], Train Loss: 0.0053, Gradient Norm: 0.07\n",
      "Epoch [337/400], Validation Loss: 0.0064\n",
      "Epoch [338/400], Train Loss: 0.0050, Gradient Norm: 0.06\n",
      "Epoch [338/400], Validation Loss: 0.0063\n",
      "Epoch [339/400], Train Loss: 0.0053, Gradient Norm: 0.12\n",
      "Epoch [339/400], Validation Loss: 0.0057\n",
      "Epoch [340/400], Train Loss: 0.0050, Gradient Norm: 0.06\n",
      "Epoch [340/400], Validation Loss: 0.0081\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_340.png\n",
      "Model saved at epoch 340\n",
      "Epoch [341/400], Train Loss: 0.0052, Gradient Norm: 0.05\n",
      "Epoch [341/400], Validation Loss: 0.0054\n",
      "Epoch [342/400], Train Loss: 0.0052, Gradient Norm: 0.04\n",
      "Epoch [342/400], Validation Loss: 0.0076\n",
      "Epoch [343/400], Train Loss: 0.0050, Gradient Norm: 0.12\n",
      "Epoch [343/400], Validation Loss: 0.0063\n",
      "Epoch [344/400], Train Loss: 0.0054, Gradient Norm: 0.04\n",
      "Epoch [344/400], Validation Loss: 0.0071\n",
      "Epoch [345/400], Train Loss: 0.0054, Gradient Norm: 0.05\n",
      "Epoch [345/400], Validation Loss: 0.0063\n",
      "Epoch 00236: reducing learning rate of group 0 to 2.4414e-08.\n",
      "Epoch [346/400], Train Loss: 0.0052, Gradient Norm: 0.07\n",
      "Epoch [346/400], Validation Loss: 0.0067\n",
      "Epoch [347/400], Train Loss: 0.0051, Gradient Norm: 0.09\n",
      "Epoch [347/400], Validation Loss: 0.0063\n",
      "Epoch [348/400], Train Loss: 0.0054, Gradient Norm: 0.06\n",
      "Epoch [348/400], Validation Loss: 0.0061\n",
      "Epoch [349/400], Train Loss: 0.0052, Gradient Norm: 0.02\n",
      "Epoch [349/400], Validation Loss: 0.0059\n",
      "Epoch [350/400], Train Loss: 0.0051, Gradient Norm: 0.07\n",
      "Epoch [350/400], Validation Loss: 0.0060\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_350.png\n",
      "Model saved at epoch 350\n",
      "Epoch [351/400], Train Loss: 0.0054, Gradient Norm: 0.06\n",
      "Epoch [351/400], Validation Loss: 0.0061\n",
      "Epoch [352/400], Train Loss: 0.0053, Gradient Norm: 0.05\n",
      "Epoch [352/400], Validation Loss: 0.0075\n",
      "Epoch [353/400], Train Loss: 0.0050, Gradient Norm: 0.07\n",
      "Epoch [353/400], Validation Loss: 0.0067\n",
      "Epoch [354/400], Train Loss: 0.0052, Gradient Norm: 0.11\n",
      "Epoch [354/400], Validation Loss: 0.0067\n",
      "Epoch [355/400], Train Loss: 0.0050, Gradient Norm: 0.03\n",
      "Epoch [355/400], Validation Loss: 0.0060\n",
      "Epoch [356/400], Train Loss: 0.0052, Gradient Norm: 0.09\n",
      "Epoch [356/400], Validation Loss: 0.0073\n",
      "Epoch [357/400], Train Loss: 0.0053, Gradient Norm: 0.11\n",
      "Epoch [357/400], Validation Loss: 0.0065\n",
      "Epoch [358/400], Train Loss: 0.0055, Gradient Norm: 0.03\n",
      "Epoch [358/400], Validation Loss: 0.0063\n",
      "Epoch [359/400], Train Loss: 0.0049, Gradient Norm: 0.11\n",
      "Epoch [359/400], Validation Loss: 0.0063\n",
      "Epoch [360/400], Train Loss: 0.0050, Gradient Norm: 0.05\n",
      "Epoch [360/400], Validation Loss: 0.0071\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_360.png\n",
      "Model saved at epoch 360\n",
      "Epoch [361/400], Train Loss: 0.0052, Gradient Norm: 0.07\n",
      "Epoch [361/400], Validation Loss: 0.0060\n",
      "Epoch 00252: reducing learning rate of group 0 to 1.2207e-08.\n",
      "Epoch [362/400], Train Loss: 0.0053, Gradient Norm: 0.05\n",
      "Epoch [362/400], Validation Loss: 0.0060\n",
      "Epoch [363/400], Train Loss: 0.0051, Gradient Norm: 0.04\n",
      "Epoch [363/400], Validation Loss: 0.0069\n",
      "Epoch [364/400], Train Loss: 0.0055, Gradient Norm: 0.23\n",
      "Epoch [364/400], Validation Loss: 0.0067\n",
      "Epoch [365/400], Train Loss: 0.0050, Gradient Norm: 0.08\n",
      "Epoch [365/400], Validation Loss: 0.0061\n",
      "Epoch [366/400], Train Loss: 0.0053, Gradient Norm: 0.02\n",
      "Epoch [366/400], Validation Loss: 0.0078\n",
      "Epoch [367/400], Train Loss: 0.0048, Gradient Norm: 0.03\n",
      "Epoch [367/400], Validation Loss: 0.0061\n",
      "Epoch [368/400], Train Loss: 0.0052, Gradient Norm: 0.05\n",
      "Epoch [368/400], Validation Loss: 0.0068\n",
      "Epoch [369/400], Train Loss: 0.0052, Gradient Norm: 0.03\n",
      "Epoch [369/400], Validation Loss: 0.0066\n",
      "Epoch [370/400], Train Loss: 0.0052, Gradient Norm: 0.12\n",
      "Epoch [370/400], Validation Loss: 0.0066\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_370.png\n",
      "Model saved at epoch 370\n",
      "Epoch [371/400], Train Loss: 0.0054, Gradient Norm: 0.08\n",
      "Epoch [371/400], Validation Loss: 0.0060\n",
      "Epoch [372/400], Train Loss: 0.0051, Gradient Norm: 0.11\n",
      "Epoch [372/400], Validation Loss: 0.0058\n",
      "Epoch [373/400], Train Loss: 0.0053, Gradient Norm: 0.10\n",
      "Epoch [373/400], Validation Loss: 0.0056\n",
      "Epoch [374/400], Train Loss: 0.0053, Gradient Norm: 0.14\n",
      "Epoch [374/400], Validation Loss: 0.0060\n",
      "Epoch [375/400], Train Loss: 0.0051, Gradient Norm: 0.02\n",
      "Epoch [375/400], Validation Loss: 0.0057\n",
      "Epoch [376/400], Train Loss: 0.0051, Gradient Norm: 0.09\n",
      "Epoch [376/400], Validation Loss: 0.0059\n",
      "Epoch [377/400], Train Loss: 0.0051, Gradient Norm: 0.15\n",
      "Epoch [377/400], Validation Loss: 0.0067\n",
      "Epoch [378/400], Train Loss: 0.0051, Gradient Norm: 0.08\n",
      "Epoch [378/400], Validation Loss: 0.0059\n",
      "Epoch [379/400], Train Loss: 0.0056, Gradient Norm: 0.10\n",
      "Epoch [379/400], Validation Loss: 0.0068\n",
      "Epoch [380/400], Train Loss: 0.0052, Gradient Norm: 0.05\n",
      "Epoch [380/400], Validation Loss: 0.0072\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_380.png\n",
      "Model saved at epoch 380\n",
      "Epoch [381/400], Train Loss: 0.0054, Gradient Norm: 0.06\n",
      "Epoch [381/400], Validation Loss: 0.0073\n",
      "Epoch [382/400], Train Loss: 0.0052, Gradient Norm: 0.07\n",
      "Epoch [382/400], Validation Loss: 0.0065\n",
      "Epoch [383/400], Train Loss: 0.0052, Gradient Norm: 0.10\n",
      "Epoch [383/400], Validation Loss: 0.0075\n",
      "Epoch [384/400], Train Loss: 0.0051, Gradient Norm: 0.04\n",
      "Epoch [384/400], Validation Loss: 0.0065\n",
      "Epoch [385/400], Train Loss: 0.0051, Gradient Norm: 0.04\n",
      "Epoch [385/400], Validation Loss: 0.0061\n",
      "Epoch [386/400], Train Loss: 0.0053, Gradient Norm: 0.13\n",
      "Epoch [386/400], Validation Loss: 0.0071\n",
      "Epoch [387/400], Train Loss: 0.0052, Gradient Norm: 0.06\n",
      "Epoch [387/400], Validation Loss: 0.0066\n",
      "Epoch [388/400], Train Loss: 0.0052, Gradient Norm: 0.06\n",
      "Epoch [388/400], Validation Loss: 0.0068\n",
      "Epoch [389/400], Train Loss: 0.0054, Gradient Norm: 0.09\n",
      "Epoch [389/400], Validation Loss: 0.0069\n",
      "Epoch [390/400], Train Loss: 0.0054, Gradient Norm: 0.03\n",
      "Epoch [390/400], Validation Loss: 0.0068\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_390.png\n",
      "Model saved at epoch 390\n",
      "Epoch [391/400], Train Loss: 0.0052, Gradient Norm: 0.11\n",
      "Epoch [391/400], Validation Loss: 0.0060\n",
      "Epoch [392/400], Train Loss: 0.0053, Gradient Norm: 0.04\n",
      "Epoch [392/400], Validation Loss: 0.0065\n",
      "Epoch [393/400], Train Loss: 0.0052, Gradient Norm: 0.03\n",
      "Epoch [393/400], Validation Loss: 0.0065\n",
      "Epoch [394/400], Train Loss: 0.0055, Gradient Norm: 0.20\n",
      "Epoch [394/400], Validation Loss: 0.0059\n",
      "Epoch [395/400], Train Loss: 0.0052, Gradient Norm: 0.04\n",
      "Epoch [395/400], Validation Loss: 0.0061\n",
      "Epoch [396/400], Train Loss: 0.0053, Gradient Norm: 0.09\n",
      "Epoch [396/400], Validation Loss: 0.0062\n",
      "Epoch [397/400], Train Loss: 0.0052, Gradient Norm: 0.02\n",
      "Epoch [397/400], Validation Loss: 0.0064\n",
      "Epoch [398/400], Train Loss: 0.0052, Gradient Norm: 0.09\n",
      "Epoch [398/400], Validation Loss: 0.0060\n",
      "Epoch [399/400], Train Loss: 0.0051, Gradient Norm: 0.07\n",
      "Epoch [399/400], Validation Loss: 0.0069\n",
      "Epoch [400/400], Train Loss: 0.0053, Gradient Norm: 0.09\n",
      "Epoch [400/400], Validation Loss: 0.0067\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint.pth\n",
      "Image saved at generated_images/training/mk5NOSWIN/generated_image_epoch_400.png\n",
      "Model saved at epoch 400\n",
      "Model checkpoint saved at ddpm64NOSWIN_checkpoint_400.pth\n",
      "Special checkpoint saved at epoch 400\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DDPM model\n",
    "in_channels = 1  # For grayscale images\n",
    "out_channels = 1  # For grayscale images\n",
    "emb_dim = 128\n",
    "num_timesteps = 2000\n",
    "latent_dim = 128\n",
    "\n",
    "unet = AttentionUNet(in_channels, out_channels, emb_dim).to(device)\n",
    "ddpm = DDPM(unet, num_timesteps, latent_dim).to(device)\n",
    "\n",
    "# Train the model\n",
    "train_diffusion_model(ddpm, train_loader, valid_loader, epochs=400, save_interval=10, checkpoint_path=\"Model_Savepoints/ddpm64NOSWIN_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39939a06",
   "metadata": {},
   "source": [
    "##### 9. Generating Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdbc68be-e879-42f7-9a63-b283231e58f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 images saved in a table format at generated_images/epoch600/generated_images_table.png\n"
     ]
    }
   ],
   "source": [
    "# Function to generate and save images after training\n",
    "def generate_and_save_images_post_training(ddpm, num_images=10, save_dir='generated_images/DDPM_images'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    sample_shape = (1, 1, 64, 64)  # Generate 1 image at a time, 1 channel, 64x64 images\n",
    "\n",
    "    # Create a figure to plot images\n",
    "    fig, axes = plt.subplots(10, 5, figsize=(15, 30))  # Adjust the layout for 10 images in a 2x5 grid\n",
    "\n",
    "    ddpm.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_images):\n",
    "            samples = ddpm.sample(sample_shape)\n",
    "            # Convert to numpy\n",
    "            samples = samples.cpu().numpy().squeeze()\n",
    "            \n",
    "            # Plot the image in the grid\n",
    "            ax = axes[i // 5, i % 5]\n",
    "            ax.imshow(samples, cmap='gray')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    # Save the figure\n",
    "    save_path = os.path.join(save_dir, 'generated_images_table.png')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f'{num_images} images saved in a table format at {save_path}')\n",
    "\n",
    "# Load the trained model\n",
    "checkpoint_path = \"Model_Savepoints/ddpm_checkpoint_600.pth\"  \n",
    "ddpm, start_epoch = load_model(ddpm, checkpoint_path)\n",
    "\n",
    "# Generate and save images after training\n",
    "generate_and_save_images_post_training(ddpm, num_images=50, save_dir='generated_images/epoch600')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b73092-c90b-46f6-b11b-253652f813d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
