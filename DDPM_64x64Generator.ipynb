{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8d0b2a9",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook presents a sophisticated DDPM model designed for medical image synthesis, leveraging the power of the Swin Transformer in conjunction with a UNet architecture. The Swin Transformer, a hierarchical Vision Transformer that utilizes shifted windows, is integrated into the bottleneck of the UNet to enhance the model's capability to capture both local and global features effectively. This hybrid approach is particularly advantageous in handling the complex structures and fine details present in medical images.\n",
    "\n",
    "This notebook guides you through:\n",
    "\n",
    "- Loading the MRnet Dataset\n",
    "- Implementation of the Swin Transformer-enhanced UNet\n",
    "- Training and evaluation of the DDPM model\n",
    "\n",
    "Overall, it demonstrates the potential of transformers in advancing medical image synthesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d679b519",
   "metadata": {},
   "source": [
    "##### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1675cbed-a2b9-4977-acb2-47bc55388704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for building and training the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1df0a2",
   "metadata": {},
   "source": [
    "##### 2. Applying Transforms and Initializing Dataset Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf403831-137a-4d53-b841-25ece3fab3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "# Define transformations to preprocess the MRI images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Convert images to grayscale\n",
    "    transforms.Resize((64, 64)),  # Resize images to 64x64\n",
    "    transforms.ToTensor()  # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "\n",
    "class MRNetSliceDataset(Dataset):\n",
    "    def __init__(self, slice_dir, label_files, transform=None):\n",
    "        super().__init__()\n",
    "        self.slice_dir = slice_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Dictionary to store labels for each image ID\n",
    "        self.labels_dict = {}\n",
    "        for label_file in label_files:\n",
    "            records = pd.read_csv(label_file, header=None, names=['id', 'label'])\n",
    "            records['id'] = records['id'].map(lambda i: '0' * (4 - len(str(i))) + str(i))\n",
    "            self.labels_dict.update(dict(zip(records['id'], records['label'])))\n",
    "\n",
    "        # List all slice files in the directory\n",
    "        self.slice_files = [os.path.join(slice_dir, fname) for fname in os.listdir(slice_dir) if fname.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        slice_path = self.slice_files[index]\n",
    "        image = Image.open(slice_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Extract ID from the filename to find the corresponding label\n",
    "        slice_id = os.path.basename(slice_path).split('_')[1]\n",
    "\n",
    "        if slice_id in self.labels_dict:\n",
    "            label = self.labels_dict[slice_id]\n",
    "            label = torch.FloatTensor([label])\n",
    "        else:\n",
    "            print(f\"Label for ID {slice_id} not found in the CSV file.\")\n",
    "            label = torch.FloatTensor([0])  # Default label or handle missing labels as needed\n",
    "\n",
    "        return {'data': image, 'label': label, 'id': slice_id}\n",
    "\n",
    "# Initialize training and validation datasets and data loaders\n",
    "root_dir = \"Raw_Images\"\n",
    "train_slice_dir = os.path.join(root_dir, \"train_slices_raw\")\n",
    "valid_slice_dir = os.path.join(root_dir, \"valid_slices_raw\")\n",
    "\n",
    "train_label_files = [\n",
    "    os.path.join(root_dir, \"train-acl.csv\"),\n",
    "    os.path.join(root_dir, \"train-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"train-meniscus.csv\")\n",
    "]\n",
    "\n",
    "valid_label_files = [\n",
    "    os.path.join(root_dir, \"valid-acl.csv\"),\n",
    "    os.path.join(root_dir, \"valid-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"valid-meniscus.csv\")\n",
    "]\n",
    "\n",
    "train_dataset = MRNetSliceDataset(slice_dir=train_slice_dir, label_files=train_label_files, transform=transform)\n",
    "valid_dataset = MRNetSliceDataset(slice_dir=valid_slice_dir, label_files=valid_label_files, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbea110",
   "metadata": {},
   "source": [
    "##### 3. Implementing Swin Transformer and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26b0a914-c38c-47c8-8e8b-704433cb5b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "<https://arxiv.org/abs/2103.14030>\n",
    "https://github.com/microsoft/Swin-Transformer\n",
    "\"\"\"\n",
    "\n",
    "# DropPath (Stochastic Depth) module to implement drop path regularization\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "        return output\n",
    "\n",
    "# Helper functions to handle tuple and truncation\n",
    "def to_2tuple(x):\n",
    "    if isinstance(x, (tuple, list)):\n",
    "        return x\n",
    "    return (x, x)\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1.):\n",
    "    with torch.no_grad():\n",
    "        size = tensor.shape\n",
    "        tmp = tensor.new_empty(size + (4,)).normal_()\n",
    "        valid = (tmp < 2) & (tmp > -2)\n",
    "        ind = valid.max(-1, keepdim=True)[1]\n",
    "        tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
    "        tensor.data.mul_(std).add_(mean)\n",
    "        return tensor\n",
    "\n",
    "# MLP module used within Swin Transformer\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "# Functions to partition and reverse windows in the Swin Transformer\n",
    "def window_partition(x, window_size):\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.view(B, C, H // window_size, window_size, W // window_size, window_size)\n",
    "    windows = x.permute(0, 2, 4, 3, 5, 1).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 5, 1, 3, 2, 4).contiguous().view(B, -1, H, W)\n",
    "    return x\n",
    "\n",
    "# Window-based multi-head self-attention (W-MSA) module\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # Relative position bias table for all windows\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n",
    "\n",
    "        # Get relative position index for each window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1)\n",
    "        attn = attn + relative_position_bias.unsqueeze(0).to(attn.dtype)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "# Swin Transformer block implementing the shifted window-based attention mechanism\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * self.mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, out_features=dim, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            attn_mask = self.calculate_mask(self.input_resolution)\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def calculate_mask(self, x_size):\n",
    "        H, W = x_size\n",
    "        img_mask = torch.zeros((1, H, W, 1))\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size).view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        return attn_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, C = x.shape\n",
    "        H, W = self.input_resolution\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        x_windows = window_partition(shifted_x, self.window_size)\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
    "\n",
    "        if self.attn_mask is not None:\n",
    "            attn_windows = self.attn(x_windows, mask=self.attn_mask.to(x.dtype))\n",
    "        else:\n",
    "            attn_windows = self.attn(x_windows)\n",
    "\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd200e2",
   "metadata": {},
   "source": [
    "##### 4. Implementing the Attention UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a10fab5-00ea-42bd-9e38-468a4bd32d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-attention block for feature refinement\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SelfAttentionBlock, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, width, height = x.size()\n",
    "        proj_query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(batch_size, -1, width * height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "        proj_value = self.value_conv(x).view(batch_size, -1, width * height)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, C, width, height)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "# Cross-attention block for the decoder to focus on relevant features\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(CrossAttentionBlock, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "# Sinusoidal positional embedding for timestep encoding in DDPM\n",
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=10000):\n",
    "        super(SinusoidalPositionalEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, timesteps):\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        emb = math.log(self.max_len) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n",
    "        emb = timesteps[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "        if self.embedding_dim % 2 == 1:\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=1)\n",
    "        return emb\n",
    "\n",
    "# UNet with self-attention, cross-attention mechanisms and SWIN \n",
    "class AttentionUNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=128):\n",
    "        super(AttentionUNet, self).__init__()\n",
    "\n",
    "        self.encoder1 = self.conv_block(in_channels + emb_dim, 64)\n",
    "        self.self_attention1 = SelfAttentionBlock(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.self_attention2 = SelfAttentionBlock(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.self_attention3 = SelfAttentionBlock(256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "        self.swin_block = SwinTransformerBlock(dim=512, input_resolution=(8, 8), num_heads=8, window_size=4)  \n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.cross_attention3 = CrossAttentionBlock(256, 256, 128)\n",
    "        self.decoder3 = self.conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.cross_attention2 = CrossAttentionBlock(128, 128, 64)\n",
    "        self.decoder2 = self.conv_block(256, 128)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.cross_attention1 = CrossAttentionBlock(64, 64, 32)\n",
    "        self.decoder1 = self.conv_block(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "        self.timestep_embedding_layer = SinusoidalPositionalEmbedding(emb_dim)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.timestep_embedding_layer(t)\n",
    "        t_embed = t_embed.view(t.size(0), -1, 1, 1)\n",
    "        t_embed = t_embed.repeat(1, 1, x.size(2), x.size(3))\n",
    "        x = torch.cat((x, t_embed), dim=1)\n",
    "\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc1 = self.self_attention1(enc1)\n",
    "        enc1_pooled = self.pool1(enc1)\n",
    "\n",
    "        enc2 = self.encoder2(enc1_pooled)\n",
    "        enc2 = self.self_attention2(enc2)\n",
    "        enc2_pooled = self.pool2(enc2)\n",
    "\n",
    "        enc3 = self.encoder3(enc2_pooled)\n",
    "        enc3 = self.self_attention3(enc3)\n",
    "        enc3_pooled = self.pool3(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc3_pooled)\n",
    "        B, C, H, W = bottleneck.shape\n",
    "        bottleneck = bottleneck.view(B, H * W, C)\n",
    "        bottleneck = self.swin_block(bottleneck)\n",
    "        bottleneck = bottleneck.view(B, C, H, W)\n",
    "\n",
    "        # Decoder\n",
    "        upconv3 = self.upconv3(bottleneck)\n",
    "        enc3 = self.cross_attention3(upconv3, enc3)\n",
    "        dec3 = torch.cat((upconv3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        upconv2 = self.upconv2(dec3)\n",
    "        enc2 = self.cross_attention2(upconv2, enc2)\n",
    "        dec2 = torch.cat((upconv2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        upconv1 = self.upconv1(dec2)\n",
    "        enc1 = self.cross_attention1(upconv1, enc1)\n",
    "        dec1 = torch.cat((upconv1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        final_output = self.final_conv(dec1)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4154c88",
   "metadata": {},
   "source": [
    "##### 5. Implementing the DDPM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1778e06d-1c6b-43dc-9c7f-2f314c25a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DDPM implementation adapted from:\n",
    "https://github.com/hojonathanho/diffusion/tree/master\n",
    "\"\"\"\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, model, num_timesteps, latent_dim, beta_start=0.00085, beta_end=0.0120):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.model = model\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', 1 - betas)\n",
    "        self.register_buffer('alphas_cumprod', torch.cumprod(1 - betas, dim=0))\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(self.alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - self.alphas_cumprod))\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def forward(self, z_t, t):\n",
    "        return self.model(z_t, t)\n",
    "\n",
    "    def sample_timesteps(self, batch_size):\n",
    "        return torch.randint(0, self.num_timesteps, (batch_size,)).to(device)\n",
    "\n",
    "    def forward_diffusion(self, z_0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(z_0)\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        return sqrt_alphas_cumprod_t * z_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_losses(self, z_0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(z_0)\n",
    "        z_t = self.forward_diffusion(z_0, t, noise)\n",
    "        predicted_noise = self.forward(z_t, t)\n",
    "        return nn.MSELoss()(noise, predicted_noise)\n",
    "\n",
    "    def sample(self, shape):\n",
    "        z_t = torch.randn(shape).to(device)\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "            t_tensor = torch.tensor([t], device=z_t.device).long()\n",
    "            alpha_t = self.alphas[t]\n",
    "            sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "            sqrt_one_minus_alpha_t = torch.sqrt(1 - self.alphas_cumprod[t])\n",
    "            beta_t = self.betas[t]\n",
    "\n",
    "            # Predict the noise\n",
    "            predicted_noise = self.forward(z_t, t_tensor)\n",
    "\n",
    "            # Remove the predicted noise\n",
    "            z_t = (z_t - beta_t / sqrt_one_minus_alpha_t * predicted_noise) / sqrt_alpha_t\n",
    "\n",
    "            # Add noise for non-final steps\n",
    "            if t > 0:\n",
    "                z_t += torch.randn_like(z_t) * torch.sqrt(beta_t)\n",
    "\n",
    "        return z_t\n",
    "\n",
    "    def p_sample(self, z, t):\n",
    "        predicted_noise = self.forward(z, t)\n",
    "        alpha_t = self.alphas[t]\n",
    "        sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "        sqrt_one_minus_alpha_t = torch.sqrt(1 - alpha_t)\n",
    "        z = (z - predicted_noise * (1 - alpha_t) / sqrt_one_minus_alpha_t) / sqrt_alpha_t\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c67a797",
   "metadata": {},
   "source": [
    "##### 6. Functions for Training and Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8775e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "def save_model(ddpm, epoch, checkpoint_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': ddpm.state_dict()\n",
    "    }, checkpoint_path)\n",
    "    print(f'Model checkpoint saved at {checkpoint_path}')\n",
    "\n",
    "# Load the model checkpoint\n",
    "def load_model(ddpm, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    ddpm.load_state_dict(checkpoint['model_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    return ddpm, start_epoch\n",
    "\n",
    "# Function to generate and save images\n",
    "def generate_and_save_images(ddpm, epoch, save_dir='generated_images/training'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    latent_dim = 128\n",
    "    shape = (1, 1, 64, 64)  # Change to 1 channel if using grayscale images\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    ddpm.eval()\n",
    "    with torch.no_grad():\n",
    "        # Sample from the DDPM in smaller batches to avoid memory issues\n",
    "        samples = ddpm.sample(shape)\n",
    "\n",
    "        # Convert to numpy and save images\n",
    "        samples = samples.squeeze().cpu().detach().numpy()\n",
    "        samples = (samples * 255).astype(np.uint8)\n",
    "        save_path = os.path.join(save_dir, f'generated_image_epoch_{epoch+1}.png')\n",
    "        Image.fromarray(samples, mode='L').save(save_path)\n",
    "        print(f'Image saved at {save_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857510b0",
   "metadata": {},
   "source": [
    "##### 7. Training Routine for the Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f81216a-5b1a-4610-a564-99ce3e130220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_diffusion_model(ddpm, train_loader, valid_loader, epochs=10, save_interval=10, checkpoint_path='ddpm_checkpoint.pth'):\n",
    "    optimizer = optim.AdamW(ddpm.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, verbose=True)\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Check if a checkpoint exists\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        ddpm, start_epoch = load_model(ddpm, checkpoint_path)\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "        print(f\"Resuming training from epoch {start_epoch+1}\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}, starting from scratch.\")\n",
    "\n",
    "    ddpm.to(device)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        ddpm.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs = batch['data'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            noise = torch.randn_like(inputs).to(device)\n",
    "            t = ddpm.sample_timesteps(inputs.size(0))\n",
    "            loss = ddpm.p_losses(inputs, t, noise)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient checking\n",
    "            total_norm = 0\n",
    "            for p in ddpm.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "\n",
    "            if total_norm > 1e3:  # Threshold for exploding gradients\n",
    "                print(f\"Warning: Exploding gradients detected at epoch {epoch+1}, total norm: {total_norm:.2f}\")\n",
    "                continue\n",
    "            if total_norm < 1e-3:  # Threshold for vanishing gradients\n",
    "                print(f\"Warning: Vanishing gradients detected at epoch {epoch+1}, total norm: {total_norm:.2f}\")\n",
    "                continue\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Gradient Norm: {total_norm:.2f}')\n",
    "\n",
    "        ddpm.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                inputs = batch['data'].to(device)\n",
    "                noise = torch.randn_like(inputs).to(device)\n",
    "                t = ddpm.sample_timesteps(inputs.size(0))\n",
    "                loss = ddpm.p_losses(inputs, t, noise)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        avg_valid_loss = valid_loss / len(valid_loader)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_valid_loss:.4f}')\n",
    "\n",
    "         # Learning Rate Scheduler\n",
    "        scheduler.step(avg_valid_loss)\n",
    "        \n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            save_model(ddpm, epoch, checkpoint_path)\n",
    "            generate_and_save_images(ddpm, epoch)\n",
    "            print(f'Model saved at epoch {epoch+1}')\n",
    "\n",
    "        # Special checkpoint save\n",
    "        if (epoch + 1) % 200 == 0:\n",
    "            special_checkpoint_path = checkpoint_path.replace(\".pth\", f\"_{epoch+1}.pth\")\n",
    "            save_model(ddpm, epoch, special_checkpoint_path)\n",
    "            print(f'Special checkpoint saved at epoch {epoch+1}')\n",
    "            \n",
    "    print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46799969",
   "metadata": {},
   "source": [
    "##### 8. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a9ddc2f-4a6f-4872-8207-108ad49a3649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found at swinddpm64MRnet_checkpoint.pth, starting from scratch.\n",
      "Epoch [1/600], Train Loss: 0.1768, Gradient Norm: 0.90\n",
      "Epoch [1/600], Validation Loss: 0.0393\n",
      "Epoch [2/600], Train Loss: 0.0284, Gradient Norm: 0.52\n",
      "Epoch [2/600], Validation Loss: 0.0266\n",
      "Epoch [3/600], Train Loss: 0.0217, Gradient Norm: 0.68\n",
      "Epoch [3/600], Validation Loss: 0.0515\n",
      "Epoch [4/600], Train Loss: 0.0178, Gradient Norm: 1.47\n",
      "Epoch [4/600], Validation Loss: 0.0167\n",
      "Epoch [5/600], Train Loss: 0.0157, Gradient Norm: 0.38\n",
      "Epoch [5/600], Validation Loss: 0.0153\n",
      "Epoch [6/600], Train Loss: 0.0154, Gradient Norm: 1.06\n",
      "Epoch [6/600], Validation Loss: 0.0172\n",
      "Epoch [7/600], Train Loss: 0.0142, Gradient Norm: 0.96\n",
      "Epoch [7/600], Validation Loss: 0.0145\n",
      "Epoch [8/600], Train Loss: 0.0127, Gradient Norm: 1.09\n",
      "Epoch [8/600], Validation Loss: 0.0125\n",
      "Epoch [9/600], Train Loss: 0.0139, Gradient Norm: 0.95\n",
      "Epoch [9/600], Validation Loss: 0.0101\n",
      "Epoch [10/600], Train Loss: 0.0120, Gradient Norm: 0.31\n",
      "Epoch [10/600], Validation Loss: 0.0145\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_10.png\n",
      "Model saved at epoch 10\n",
      "Epoch [11/600], Train Loss: 0.0119, Gradient Norm: 0.24\n",
      "Epoch [11/600], Validation Loss: 0.0167\n",
      "Epoch [12/600], Train Loss: 0.0118, Gradient Norm: 0.27\n",
      "Epoch [12/600], Validation Loss: 0.0160\n",
      "Epoch [13/600], Train Loss: 0.0118, Gradient Norm: 1.96\n",
      "Epoch [13/600], Validation Loss: 0.0095\n",
      "Epoch [14/600], Train Loss: 0.0112, Gradient Norm: 0.08\n",
      "Epoch [14/600], Validation Loss: 0.0089\n",
      "Epoch [15/600], Train Loss: 0.0106, Gradient Norm: 1.65\n",
      "Epoch [15/600], Validation Loss: 0.0128\n",
      "Epoch [16/600], Train Loss: 0.0105, Gradient Norm: 1.38\n",
      "Epoch [16/600], Validation Loss: 0.0097\n",
      "Epoch [17/600], Train Loss: 0.0106, Gradient Norm: 0.14\n",
      "Epoch [17/600], Validation Loss: 0.0109\n",
      "Epoch [18/600], Train Loss: 0.0095, Gradient Norm: 0.18\n",
      "Epoch [18/600], Validation Loss: 0.0087\n",
      "Epoch [19/600], Train Loss: 0.0094, Gradient Norm: 0.41\n",
      "Epoch [19/600], Validation Loss: 0.0092\n",
      "Epoch [20/600], Train Loss: 0.0092, Gradient Norm: 1.46\n",
      "Epoch [20/600], Validation Loss: 0.0104\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_20.png\n",
      "Model saved at epoch 20\n",
      "Epoch [21/600], Train Loss: 0.0101, Gradient Norm: 0.22\n",
      "Epoch [21/600], Validation Loss: 0.0113\n",
      "Epoch [22/600], Train Loss: 0.0091, Gradient Norm: 1.08\n",
      "Epoch [22/600], Validation Loss: 0.0111\n",
      "Epoch [23/600], Train Loss: 0.0089, Gradient Norm: 1.05\n",
      "Epoch [23/600], Validation Loss: 0.0092\n",
      "Epoch [24/600], Train Loss: 0.0090, Gradient Norm: 0.63\n",
      "Epoch [24/600], Validation Loss: 0.0123\n",
      "Epoch [25/600], Train Loss: 0.0087, Gradient Norm: 0.16\n",
      "Epoch [25/600], Validation Loss: 0.0095\n",
      "Epoch [26/600], Train Loss: 0.0090, Gradient Norm: 0.37\n",
      "Epoch [26/600], Validation Loss: 0.0085\n",
      "Epoch [27/600], Train Loss: 0.0088, Gradient Norm: 0.09\n",
      "Epoch [27/600], Validation Loss: 0.0091\n",
      "Epoch [28/600], Train Loss: 0.0085, Gradient Norm: 0.50\n",
      "Epoch [28/600], Validation Loss: 0.0113\n",
      "Epoch [29/600], Train Loss: 0.0082, Gradient Norm: 0.42\n",
      "Epoch [29/600], Validation Loss: 0.0085\n",
      "Epoch [30/600], Train Loss: 0.0086, Gradient Norm: 0.17\n",
      "Epoch [30/600], Validation Loss: 0.0089\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_30.png\n",
      "Model saved at epoch 30\n",
      "Epoch [31/600], Train Loss: 0.0081, Gradient Norm: 0.60\n",
      "Epoch [31/600], Validation Loss: 0.0095\n",
      "Epoch [32/600], Train Loss: 0.0079, Gradient Norm: 0.09\n",
      "Epoch [32/600], Validation Loss: 0.0077\n",
      "Epoch [33/600], Train Loss: 0.0076, Gradient Norm: 0.14\n",
      "Epoch [33/600], Validation Loss: 0.0069\n",
      "Epoch [34/600], Train Loss: 0.0079, Gradient Norm: 0.45\n",
      "Epoch [34/600], Validation Loss: 0.0102\n",
      "Epoch [35/600], Train Loss: 0.0082, Gradient Norm: 0.17\n",
      "Epoch [35/600], Validation Loss: 0.0085\n",
      "Epoch [36/600], Train Loss: 0.0075, Gradient Norm: 0.65\n",
      "Epoch [36/600], Validation Loss: 0.0086\n",
      "Epoch [37/600], Train Loss: 0.0074, Gradient Norm: 0.22\n",
      "Epoch [37/600], Validation Loss: 0.0081\n",
      "Epoch [38/600], Train Loss: 0.0077, Gradient Norm: 0.07\n",
      "Epoch [38/600], Validation Loss: 0.0082\n",
      "Epoch [39/600], Train Loss: 0.0073, Gradient Norm: 0.17\n",
      "Epoch [39/600], Validation Loss: 0.0075\n",
      "Epoch [40/600], Train Loss: 0.0077, Gradient Norm: 0.45\n",
      "Epoch [40/600], Validation Loss: 0.0073\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_40.png\n",
      "Model saved at epoch 40\n",
      "Epoch [41/600], Train Loss: 0.0068, Gradient Norm: 0.16\n",
      "Epoch [41/600], Validation Loss: 0.0105\n",
      "Epoch [42/600], Train Loss: 0.0075, Gradient Norm: 0.34\n",
      "Epoch [42/600], Validation Loss: 0.0086\n",
      "Epoch [43/600], Train Loss: 0.0071, Gradient Norm: 0.14\n",
      "Epoch [43/600], Validation Loss: 0.0084\n",
      "Epoch [44/600], Train Loss: 0.0071, Gradient Norm: 0.18\n",
      "Epoch [44/600], Validation Loss: 0.0078\n",
      "Epoch [45/600], Train Loss: 0.0070, Gradient Norm: 0.15\n",
      "Epoch [45/600], Validation Loss: 0.0088\n",
      "Epoch [46/600], Train Loss: 0.0074, Gradient Norm: 0.25\n",
      "Epoch [46/600], Validation Loss: 0.0093\n",
      "Epoch [47/600], Train Loss: 0.0078, Gradient Norm: 0.08\n",
      "Epoch [47/600], Validation Loss: 0.0082\n",
      "Epoch [48/600], Train Loss: 0.0065, Gradient Norm: 0.25\n",
      "Epoch [48/600], Validation Loss: 0.0075\n",
      "Epoch [49/600], Train Loss: 0.0065, Gradient Norm: 0.19\n",
      "Epoch [49/600], Validation Loss: 0.0082\n",
      "Epoch 00049: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch [50/600], Train Loss: 0.0063, Gradient Norm: 0.07\n",
      "Epoch [50/600], Validation Loss: 0.0072\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_50.png\n",
      "Model saved at epoch 50\n",
      "Epoch [51/600], Train Loss: 0.0069, Gradient Norm: 0.08\n",
      "Epoch [51/600], Validation Loss: 0.0085\n",
      "Epoch [52/600], Train Loss: 0.0065, Gradient Norm: 0.38\n",
      "Epoch [52/600], Validation Loss: 0.0087\n",
      "Epoch [53/600], Train Loss: 0.0064, Gradient Norm: 0.17\n",
      "Epoch [53/600], Validation Loss: 0.0085\n",
      "Epoch [54/600], Train Loss: 0.0063, Gradient Norm: 0.09\n",
      "Epoch [54/600], Validation Loss: 0.0080\n",
      "Epoch [55/600], Train Loss: 0.0062, Gradient Norm: 0.48\n",
      "Epoch [55/600], Validation Loss: 0.0084\n",
      "Epoch [56/600], Train Loss: 0.0063, Gradient Norm: 0.12\n",
      "Epoch [56/600], Validation Loss: 0.0072\n",
      "Epoch [57/600], Train Loss: 0.0064, Gradient Norm: 0.18\n",
      "Epoch [57/600], Validation Loss: 0.0068\n",
      "Epoch [58/600], Train Loss: 0.0066, Gradient Norm: 0.13\n",
      "Epoch [58/600], Validation Loss: 0.0081\n",
      "Epoch [59/600], Train Loss: 0.0063, Gradient Norm: 0.18\n",
      "Epoch [59/600], Validation Loss: 0.0071\n",
      "Epoch [60/600], Train Loss: 0.0059, Gradient Norm: 0.07\n",
      "Epoch [60/600], Validation Loss: 0.0072\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_60.png\n",
      "Model saved at epoch 60\n",
      "Epoch [61/600], Train Loss: 0.0059, Gradient Norm: 0.17\n",
      "Epoch [61/600], Validation Loss: 0.0073\n",
      "Epoch [62/600], Train Loss: 0.0062, Gradient Norm: 0.12\n",
      "Epoch [62/600], Validation Loss: 0.0077\n",
      "Epoch [63/600], Train Loss: 0.0058, Gradient Norm: 0.43\n",
      "Epoch [63/600], Validation Loss: 0.0073\n",
      "Epoch [64/600], Train Loss: 0.0059, Gradient Norm: 0.30\n",
      "Epoch [64/600], Validation Loss: 0.0071\n",
      "Epoch [65/600], Train Loss: 0.0060, Gradient Norm: 0.18\n",
      "Epoch [65/600], Validation Loss: 0.0073\n",
      "Epoch [66/600], Train Loss: 0.0061, Gradient Norm: 0.09\n",
      "Epoch [66/600], Validation Loss: 0.0071\n",
      "Epoch [67/600], Train Loss: 0.0058, Gradient Norm: 0.53\n",
      "Epoch [67/600], Validation Loss: 0.0070\n",
      "Epoch [68/600], Train Loss: 0.0059, Gradient Norm: 0.09\n",
      "Epoch [68/600], Validation Loss: 0.0071\n",
      "Epoch [69/600], Train Loss: 0.0058, Gradient Norm: 0.10\n",
      "Epoch [69/600], Validation Loss: 0.0080\n",
      "Epoch [70/600], Train Loss: 0.0055, Gradient Norm: 0.07\n",
      "Epoch [70/600], Validation Loss: 0.0080\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_70.png\n",
      "Model saved at epoch 70\n",
      "Epoch [71/600], Train Loss: 0.0059, Gradient Norm: 0.20\n",
      "Epoch [71/600], Validation Loss: 0.0066\n",
      "Epoch [72/600], Train Loss: 0.0059, Gradient Norm: 0.11\n",
      "Epoch [72/600], Validation Loss: 0.0066\n",
      "Epoch [73/600], Train Loss: 0.0058, Gradient Norm: 0.27\n",
      "Epoch [73/600], Validation Loss: 0.0078\n",
      "Epoch [74/600], Train Loss: 0.0058, Gradient Norm: 0.28\n",
      "Epoch [74/600], Validation Loss: 0.0071\n",
      "Epoch [75/600], Train Loss: 0.0059, Gradient Norm: 0.09\n",
      "Epoch [75/600], Validation Loss: 0.0079\n",
      "Epoch [76/600], Train Loss: 0.0058, Gradient Norm: 0.18\n",
      "Epoch [76/600], Validation Loss: 0.0069\n",
      "Epoch [77/600], Train Loss: 0.0057, Gradient Norm: 0.08\n",
      "Epoch [77/600], Validation Loss: 0.0087\n",
      "Epoch [78/600], Train Loss: 0.0057, Gradient Norm: 0.22\n",
      "Epoch [78/600], Validation Loss: 0.0081\n",
      "Epoch [79/600], Train Loss: 0.0059, Gradient Norm: 0.14\n",
      "Epoch [79/600], Validation Loss: 0.0077\n",
      "Epoch [80/600], Train Loss: 0.0055, Gradient Norm: 0.17\n",
      "Epoch [80/600], Validation Loss: 0.0079\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_80.png\n",
      "Model saved at epoch 80\n",
      "Epoch [81/600], Train Loss: 0.0054, Gradient Norm: 0.09\n",
      "Epoch [81/600], Validation Loss: 0.0073\n",
      "Epoch [82/600], Train Loss: 0.0058, Gradient Norm: 0.08\n",
      "Epoch [82/600], Validation Loss: 0.0083\n",
      "Epoch [83/600], Train Loss: 0.0057, Gradient Norm: 0.15\n",
      "Epoch [83/600], Validation Loss: 0.0075\n",
      "Epoch [84/600], Train Loss: 0.0055, Gradient Norm: 0.06\n",
      "Epoch [84/600], Validation Loss: 0.0076\n",
      "Epoch [85/600], Train Loss: 0.0055, Gradient Norm: 0.17\n",
      "Epoch [85/600], Validation Loss: 0.0091\n",
      "Epoch [86/600], Train Loss: 0.0054, Gradient Norm: 0.07\n",
      "Epoch [86/600], Validation Loss: 0.0083\n",
      "Epoch [87/600], Train Loss: 0.0057, Gradient Norm: 0.07\n",
      "Epoch [87/600], Validation Loss: 0.0072\n",
      "Epoch 00087: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch [88/600], Train Loss: 0.0056, Gradient Norm: 0.25\n",
      "Epoch [88/600], Validation Loss: 0.0074\n",
      "Epoch [89/600], Train Loss: 0.0050, Gradient Norm: 0.08\n",
      "Epoch [89/600], Validation Loss: 0.0071\n",
      "Epoch [90/600], Train Loss: 0.0054, Gradient Norm: 0.14\n",
      "Epoch [90/600], Validation Loss: 0.0066\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_90.png\n",
      "Model saved at epoch 90\n",
      "Epoch [91/600], Train Loss: 0.0054, Gradient Norm: 0.32\n",
      "Epoch [91/600], Validation Loss: 0.0077\n",
      "Epoch [92/600], Train Loss: 0.0051, Gradient Norm: 0.24\n",
      "Epoch [92/600], Validation Loss: 0.0076\n",
      "Epoch [93/600], Train Loss: 0.0052, Gradient Norm: 0.07\n",
      "Epoch [93/600], Validation Loss: 0.0077\n",
      "Epoch [94/600], Train Loss: 0.0052, Gradient Norm: 0.16\n",
      "Epoch [94/600], Validation Loss: 0.0082\n",
      "Epoch [95/600], Train Loss: 0.0055, Gradient Norm: 0.10\n",
      "Epoch [95/600], Validation Loss: 0.0074\n",
      "Epoch [96/600], Train Loss: 0.0053, Gradient Norm: 0.17\n",
      "Epoch [96/600], Validation Loss: 0.0087\n",
      "Epoch [97/600], Train Loss: 0.0050, Gradient Norm: 0.12\n",
      "Epoch [97/600], Validation Loss: 0.0074\n",
      "Epoch [98/600], Train Loss: 0.0050, Gradient Norm: 0.12\n",
      "Epoch [98/600], Validation Loss: 0.0076\n",
      "Epoch [99/600], Train Loss: 0.0052, Gradient Norm: 0.07\n",
      "Epoch [99/600], Validation Loss: 0.0078\n",
      "Epoch [100/600], Train Loss: 0.0048, Gradient Norm: 0.18\n",
      "Epoch [100/600], Validation Loss: 0.0068\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_100.png\n",
      "Model saved at epoch 100\n",
      "Epoch [101/600], Train Loss: 0.0051, Gradient Norm: 0.05\n",
      "Epoch [101/600], Validation Loss: 0.0094\n",
      "Epoch [102/600], Train Loss: 0.0050, Gradient Norm: 0.27\n",
      "Epoch [102/600], Validation Loss: 0.0071\n",
      "Epoch [103/600], Train Loss: 0.0048, Gradient Norm: 0.29\n",
      "Epoch [103/600], Validation Loss: 0.0088\n",
      "Epoch 00103: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch [104/600], Train Loss: 0.0053, Gradient Norm: 0.10\n",
      "Epoch [104/600], Validation Loss: 0.0076\n",
      "Epoch [105/600], Train Loss: 0.0049, Gradient Norm: 0.12\n",
      "Epoch [105/600], Validation Loss: 0.0081\n",
      "Epoch [106/600], Train Loss: 0.0048, Gradient Norm: 0.18\n",
      "Epoch [106/600], Validation Loss: 0.0086\n",
      "Epoch [107/600], Train Loss: 0.0047, Gradient Norm: 0.17\n",
      "Epoch [107/600], Validation Loss: 0.0076\n",
      "Epoch [108/600], Train Loss: 0.0050, Gradient Norm: 0.13\n",
      "Epoch [108/600], Validation Loss: 0.0081\n",
      "Epoch [109/600], Train Loss: 0.0048, Gradient Norm: 0.10\n",
      "Epoch [109/600], Validation Loss: 0.0076\n",
      "Epoch [110/600], Train Loss: 0.0048, Gradient Norm: 0.18\n",
      "Epoch [110/600], Validation Loss: 0.0072\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_110.png\n",
      "Model saved at epoch 110\n",
      "Epoch [111/600], Train Loss: 0.0050, Gradient Norm: 0.68\n",
      "Epoch [111/600], Validation Loss: 0.0085\n",
      "Epoch [112/600], Train Loss: 0.0046, Gradient Norm: 0.15\n",
      "Epoch [112/600], Validation Loss: 0.0084\n",
      "Epoch [113/600], Train Loss: 0.0047, Gradient Norm: 0.06\n",
      "Epoch [113/600], Validation Loss: 0.0083\n",
      "Epoch [114/600], Train Loss: 0.0051, Gradient Norm: 0.06\n",
      "Epoch [114/600], Validation Loss: 0.0078\n",
      "Epoch [115/600], Train Loss: 0.0050, Gradient Norm: 0.12\n",
      "Epoch [115/600], Validation Loss: 0.0085\n",
      "Epoch [116/600], Train Loss: 0.0048, Gradient Norm: 0.07\n",
      "Epoch [116/600], Validation Loss: 0.0070\n",
      "Epoch [117/600], Train Loss: 0.0045, Gradient Norm: 0.13\n",
      "Epoch [117/600], Validation Loss: 0.0083\n",
      "Epoch [118/600], Train Loss: 0.0047, Gradient Norm: 0.17\n",
      "Epoch [118/600], Validation Loss: 0.0077\n",
      "Epoch [119/600], Train Loss: 0.0049, Gradient Norm: 0.18\n",
      "Epoch [119/600], Validation Loss: 0.0084\n",
      "Epoch 00119: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch [120/600], Train Loss: 0.0047, Gradient Norm: 0.08\n",
      "Epoch [120/600], Validation Loss: 0.0090\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_120.png\n",
      "Model saved at epoch 120\n",
      "Epoch [121/600], Train Loss: 0.0047, Gradient Norm: 0.12\n",
      "Epoch [121/600], Validation Loss: 0.0071\n",
      "Epoch [122/600], Train Loss: 0.0046, Gradient Norm: 0.12\n",
      "Epoch [122/600], Validation Loss: 0.0090\n",
      "Epoch [123/600], Train Loss: 0.0047, Gradient Norm: 0.05\n",
      "Epoch [123/600], Validation Loss: 0.0088\n",
      "Epoch [124/600], Train Loss: 0.0046, Gradient Norm: 0.12\n",
      "Epoch [124/600], Validation Loss: 0.0087\n",
      "Epoch [125/600], Train Loss: 0.0047, Gradient Norm: 0.08\n",
      "Epoch [125/600], Validation Loss: 0.0082\n",
      "Epoch [126/600], Train Loss: 0.0048, Gradient Norm: 0.17\n",
      "Epoch [126/600], Validation Loss: 0.0077\n",
      "Epoch [127/600], Train Loss: 0.0050, Gradient Norm: 0.22\n",
      "Epoch [127/600], Validation Loss: 0.0084\n",
      "Epoch [128/600], Train Loss: 0.0046, Gradient Norm: 0.18\n",
      "Epoch [128/600], Validation Loss: 0.0087\n",
      "Epoch [129/600], Train Loss: 0.0046, Gradient Norm: 0.10\n",
      "Epoch [129/600], Validation Loss: 0.0081\n",
      "Epoch [130/600], Train Loss: 0.0050, Gradient Norm: 0.08\n",
      "Epoch [130/600], Validation Loss: 0.0075\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_130.png\n",
      "Model saved at epoch 130\n",
      "Epoch [131/600], Train Loss: 0.0045, Gradient Norm: 0.15\n",
      "Epoch [131/600], Validation Loss: 0.0085\n",
      "Epoch [132/600], Train Loss: 0.0045, Gradient Norm: 0.22\n",
      "Epoch [132/600], Validation Loss: 0.0074\n",
      "Epoch [133/600], Train Loss: 0.0047, Gradient Norm: 0.42\n",
      "Epoch [133/600], Validation Loss: 0.0091\n",
      "Epoch [134/600], Train Loss: 0.0044, Gradient Norm: 0.06\n",
      "Epoch [134/600], Validation Loss: 0.0083\n",
      "Epoch [135/600], Train Loss: 0.0048, Gradient Norm: 0.08\n",
      "Epoch [135/600], Validation Loss: 0.0081\n",
      "Epoch 00135: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Epoch [136/600], Train Loss: 0.0048, Gradient Norm: 0.09\n",
      "Epoch [136/600], Validation Loss: 0.0086\n",
      "Epoch [137/600], Train Loss: 0.0045, Gradient Norm: 0.17\n",
      "Epoch [137/600], Validation Loss: 0.0088\n",
      "Epoch [138/600], Train Loss: 0.0047, Gradient Norm: 0.06\n",
      "Epoch [138/600], Validation Loss: 0.0089\n",
      "Epoch [139/600], Train Loss: 0.0045, Gradient Norm: 0.59\n",
      "Epoch [139/600], Validation Loss: 0.0082\n",
      "Epoch [140/600], Train Loss: 0.0046, Gradient Norm: 0.14\n",
      "Epoch [140/600], Validation Loss: 0.0084\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_140.png\n",
      "Model saved at epoch 140\n",
      "Epoch [141/600], Train Loss: 0.0049, Gradient Norm: 0.09\n",
      "Epoch [141/600], Validation Loss: 0.0081\n",
      "Epoch [142/600], Train Loss: 0.0046, Gradient Norm: 0.07\n",
      "Epoch [142/600], Validation Loss: 0.0096\n",
      "Epoch [143/600], Train Loss: 0.0047, Gradient Norm: 0.12\n",
      "Epoch [143/600], Validation Loss: 0.0092\n",
      "Epoch [144/600], Train Loss: 0.0046, Gradient Norm: 0.08\n",
      "Epoch [144/600], Validation Loss: 0.0068\n",
      "Epoch [145/600], Train Loss: 0.0045, Gradient Norm: 0.25\n",
      "Epoch [145/600], Validation Loss: 0.0072\n",
      "Epoch [146/600], Train Loss: 0.0047, Gradient Norm: 0.06\n",
      "Epoch [146/600], Validation Loss: 0.0091\n",
      "Epoch [147/600], Train Loss: 0.0046, Gradient Norm: 0.62\n",
      "Epoch [147/600], Validation Loss: 0.0078\n",
      "Epoch [148/600], Train Loss: 0.0047, Gradient Norm: 0.21\n",
      "Epoch [148/600], Validation Loss: 0.0077\n",
      "Epoch [149/600], Train Loss: 0.0046, Gradient Norm: 0.08\n",
      "Epoch [149/600], Validation Loss: 0.0085\n",
      "Epoch [150/600], Train Loss: 0.0048, Gradient Norm: 0.10\n",
      "Epoch [150/600], Validation Loss: 0.0085\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_150.png\n",
      "Model saved at epoch 150\n",
      "Epoch [151/600], Train Loss: 0.0046, Gradient Norm: 0.07\n",
      "Epoch [151/600], Validation Loss: 0.0074\n",
      "Epoch 00151: reducing learning rate of group 0 to 1.5625e-06.\n",
      "Epoch [152/600], Train Loss: 0.0042, Gradient Norm: 0.06\n",
      "Epoch [152/600], Validation Loss: 0.0087\n",
      "Epoch [153/600], Train Loss: 0.0046, Gradient Norm: 0.06\n",
      "Epoch [153/600], Validation Loss: 0.0082\n",
      "Epoch [154/600], Train Loss: 0.0049, Gradient Norm: 0.08\n",
      "Epoch [154/600], Validation Loss: 0.0090\n",
      "Epoch [155/600], Train Loss: 0.0046, Gradient Norm: 0.09\n",
      "Epoch [155/600], Validation Loss: 0.0091\n",
      "Epoch [156/600], Train Loss: 0.0047, Gradient Norm: 0.11\n",
      "Epoch [156/600], Validation Loss: 0.0085\n",
      "Epoch [157/600], Train Loss: 0.0050, Gradient Norm: 0.08\n",
      "Epoch [157/600], Validation Loss: 0.0082\n",
      "Epoch [158/600], Train Loss: 0.0045, Gradient Norm: 0.15\n",
      "Epoch [158/600], Validation Loss: 0.0089\n",
      "Epoch [159/600], Train Loss: 0.0046, Gradient Norm: 0.12\n",
      "Epoch [159/600], Validation Loss: 0.0101\n",
      "Epoch [160/600], Train Loss: 0.0050, Gradient Norm: 0.05\n",
      "Epoch [160/600], Validation Loss: 0.0089\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_160.png\n",
      "Model saved at epoch 160\n",
      "Epoch [161/600], Train Loss: 0.0046, Gradient Norm: 0.12\n",
      "Epoch [161/600], Validation Loss: 0.0082\n",
      "Epoch [162/600], Train Loss: 0.0045, Gradient Norm: 0.06\n",
      "Epoch [162/600], Validation Loss: 0.0075\n",
      "Epoch [163/600], Train Loss: 0.0046, Gradient Norm: 0.21\n",
      "Epoch [163/600], Validation Loss: 0.0089\n",
      "Epoch [164/600], Train Loss: 0.0050, Gradient Norm: 0.14\n",
      "Epoch [164/600], Validation Loss: 0.0086\n",
      "Epoch [165/600], Train Loss: 0.0049, Gradient Norm: 0.28\n",
      "Epoch [165/600], Validation Loss: 0.0088\n",
      "Epoch [166/600], Train Loss: 0.0046, Gradient Norm: 0.15\n",
      "Epoch [166/600], Validation Loss: 0.0084\n",
      "Epoch [167/600], Train Loss: 0.0048, Gradient Norm: 0.09\n",
      "Epoch [167/600], Validation Loss: 0.0088\n",
      "Epoch 00167: reducing learning rate of group 0 to 7.8125e-07.\n",
      "Epoch [168/600], Train Loss: 0.0044, Gradient Norm: 0.09\n",
      "Epoch [168/600], Validation Loss: 0.0081\n",
      "Epoch [169/600], Train Loss: 0.0049, Gradient Norm: 0.04\n",
      "Epoch [169/600], Validation Loss: 0.0074\n",
      "Epoch [170/600], Train Loss: 0.0047, Gradient Norm: 0.09\n",
      "Epoch [170/600], Validation Loss: 0.0088\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_170.png\n",
      "Model saved at epoch 170\n",
      "Epoch [171/600], Train Loss: 0.0045, Gradient Norm: 0.35\n",
      "Epoch [171/600], Validation Loss: 0.0083\n",
      "Epoch [172/600], Train Loss: 0.0046, Gradient Norm: 0.07\n",
      "Epoch [172/600], Validation Loss: 0.0078\n",
      "Epoch [173/600], Train Loss: 0.0049, Gradient Norm: 0.10\n",
      "Epoch [173/600], Validation Loss: 0.0092\n",
      "Epoch [174/600], Train Loss: 0.0047, Gradient Norm: 0.08\n",
      "Epoch [174/600], Validation Loss: 0.0077\n",
      "Epoch [175/600], Train Loss: 0.0045, Gradient Norm: 0.09\n",
      "Epoch [175/600], Validation Loss: 0.0081\n",
      "Epoch [176/600], Train Loss: 0.0042, Gradient Norm: 0.10\n",
      "Epoch [176/600], Validation Loss: 0.0092\n",
      "Epoch [177/600], Train Loss: 0.0049, Gradient Norm: 0.05\n",
      "Epoch [177/600], Validation Loss: 0.0095\n",
      "Epoch [178/600], Train Loss: 0.0047, Gradient Norm: 0.06\n",
      "Epoch [178/600], Validation Loss: 0.0078\n",
      "Epoch [179/600], Train Loss: 0.0046, Gradient Norm: 0.26\n",
      "Epoch [179/600], Validation Loss: 0.0074\n",
      "Epoch [180/600], Train Loss: 0.0044, Gradient Norm: 0.02\n",
      "Epoch [180/600], Validation Loss: 0.0076\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_180.png\n",
      "Model saved at epoch 180\n",
      "Epoch [181/600], Train Loss: 0.0047, Gradient Norm: 0.08\n",
      "Epoch [181/600], Validation Loss: 0.0088\n",
      "Epoch [182/600], Train Loss: 0.0049, Gradient Norm: 0.10\n",
      "Epoch [182/600], Validation Loss: 0.0082\n",
      "Epoch [183/600], Train Loss: 0.0051, Gradient Norm: 0.06\n",
      "Epoch [183/600], Validation Loss: 0.0075\n",
      "Epoch 00183: reducing learning rate of group 0 to 3.9063e-07.\n",
      "Epoch [184/600], Train Loss: 0.0046, Gradient Norm: 0.06\n",
      "Epoch [184/600], Validation Loss: 0.0069\n",
      "Epoch [185/600], Train Loss: 0.0045, Gradient Norm: 0.14\n",
      "Epoch [185/600], Validation Loss: 0.0088\n",
      "Epoch [186/600], Train Loss: 0.0047, Gradient Norm: 0.11\n",
      "Epoch [186/600], Validation Loss: 0.0085\n",
      "Epoch [187/600], Train Loss: 0.0045, Gradient Norm: 0.02\n",
      "Epoch [187/600], Validation Loss: 0.0104\n",
      "Epoch [188/600], Train Loss: 0.0046, Gradient Norm: 0.05\n",
      "Epoch [188/600], Validation Loss: 0.0075\n",
      "Epoch [189/600], Train Loss: 0.0048, Gradient Norm: 0.10\n",
      "Epoch [189/600], Validation Loss: 0.0081\n",
      "Epoch [190/600], Train Loss: 0.0046, Gradient Norm: 0.24\n",
      "Epoch [190/600], Validation Loss: 0.0103\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_190.png\n",
      "Model saved at epoch 190\n",
      "Epoch [191/600], Train Loss: 0.0048, Gradient Norm: 0.10\n",
      "Epoch [191/600], Validation Loss: 0.0072\n",
      "Epoch [192/600], Train Loss: 0.0045, Gradient Norm: 0.08\n",
      "Epoch [192/600], Validation Loss: 0.0086\n",
      "Epoch [193/600], Train Loss: 0.0047, Gradient Norm: 0.14\n",
      "Epoch [193/600], Validation Loss: 0.0091\n",
      "Epoch [194/600], Train Loss: 0.0046, Gradient Norm: 0.10\n",
      "Epoch [194/600], Validation Loss: 0.0077\n",
      "Epoch [195/600], Train Loss: 0.0045, Gradient Norm: 0.06\n",
      "Epoch [195/600], Validation Loss: 0.0098\n",
      "Epoch [196/600], Train Loss: 0.0045, Gradient Norm: 0.12\n",
      "Epoch [196/600], Validation Loss: 0.0079\n",
      "Epoch [197/600], Train Loss: 0.0048, Gradient Norm: 0.11\n",
      "Epoch [197/600], Validation Loss: 0.0094\n",
      "Epoch [198/600], Train Loss: 0.0048, Gradient Norm: 0.21\n",
      "Epoch [198/600], Validation Loss: 0.0092\n",
      "Epoch [199/600], Train Loss: 0.0048, Gradient Norm: 0.21\n",
      "Epoch [199/600], Validation Loss: 0.0070\n",
      "Epoch 00199: reducing learning rate of group 0 to 1.9531e-07.\n",
      "Epoch [200/600], Train Loss: 0.0046, Gradient Norm: 0.20\n",
      "Epoch [200/600], Validation Loss: 0.0086\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_200.png\n",
      "Model saved at epoch 200\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint_200.pth\n",
      "Special checkpoint saved at epoch 200\n",
      "Epoch [201/600], Train Loss: 0.0047, Gradient Norm: 0.06\n",
      "Epoch [201/600], Validation Loss: 0.0097\n",
      "Epoch [202/600], Train Loss: 0.0045, Gradient Norm: 0.04\n",
      "Epoch [202/600], Validation Loss: 0.0087\n",
      "Epoch [203/600], Train Loss: 0.0048, Gradient Norm: 0.06\n",
      "Epoch [203/600], Validation Loss: 0.0078\n",
      "Epoch [204/600], Train Loss: 0.0048, Gradient Norm: 0.11\n",
      "Epoch [204/600], Validation Loss: 0.0088\n",
      "Epoch [205/600], Train Loss: 0.0046, Gradient Norm: 0.07\n",
      "Epoch [205/600], Validation Loss: 0.0078\n",
      "Epoch [206/600], Train Loss: 0.0046, Gradient Norm: 0.09\n",
      "Epoch [206/600], Validation Loss: 0.0086\n",
      "Epoch [207/600], Train Loss: 0.0044, Gradient Norm: 0.04\n",
      "Epoch [207/600], Validation Loss: 0.0106\n",
      "Epoch [208/600], Train Loss: 0.0049, Gradient Norm: 0.45\n",
      "Epoch [208/600], Validation Loss: 0.0079\n",
      "Epoch [209/600], Train Loss: 0.0047, Gradient Norm: 0.07\n",
      "Epoch [209/600], Validation Loss: 0.0088\n",
      "Epoch [210/600], Train Loss: 0.0049, Gradient Norm: 0.09\n",
      "Epoch [210/600], Validation Loss: 0.0082\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_210.png\n",
      "Model saved at epoch 210\n",
      "Epoch [211/600], Train Loss: 0.0046, Gradient Norm: 0.12\n",
      "Epoch [211/600], Validation Loss: 0.0089\n",
      "Epoch [212/600], Train Loss: 0.0047, Gradient Norm: 0.07\n",
      "Epoch [212/600], Validation Loss: 0.0085\n",
      "Epoch [213/600], Train Loss: 0.0044, Gradient Norm: 0.14\n",
      "Epoch [213/600], Validation Loss: 0.0084\n",
      "Epoch [214/600], Train Loss: 0.0046, Gradient Norm: 0.17\n",
      "Epoch [214/600], Validation Loss: 0.0086\n",
      "Epoch [215/600], Train Loss: 0.0044, Gradient Norm: 0.04\n",
      "Epoch [215/600], Validation Loss: 0.0082\n",
      "Epoch 00215: reducing learning rate of group 0 to 9.7656e-08.\n",
      "Epoch [216/600], Train Loss: 0.0047, Gradient Norm: 0.05\n",
      "Epoch [216/600], Validation Loss: 0.0092\n",
      "Epoch [217/600], Train Loss: 0.0046, Gradient Norm: 0.04\n",
      "Epoch [217/600], Validation Loss: 0.0096\n",
      "Epoch [218/600], Train Loss: 0.0047, Gradient Norm: 0.08\n",
      "Epoch [218/600], Validation Loss: 0.0084\n",
      "Epoch [219/600], Train Loss: 0.0047, Gradient Norm: 0.06\n",
      "Epoch [219/600], Validation Loss: 0.0083\n",
      "Epoch [220/600], Train Loss: 0.0048, Gradient Norm: 0.11\n",
      "Epoch [220/600], Validation Loss: 0.0086\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_220.png\n",
      "Model saved at epoch 220\n",
      "Epoch [221/600], Train Loss: 0.0044, Gradient Norm: 0.03\n",
      "Epoch [221/600], Validation Loss: 0.0080\n",
      "Epoch [222/600], Train Loss: 0.0047, Gradient Norm: 0.02\n",
      "Epoch [222/600], Validation Loss: 0.0090\n",
      "Epoch [223/600], Train Loss: 0.0047, Gradient Norm: 0.01\n",
      "Epoch [223/600], Validation Loss: 0.0100\n",
      "Epoch [224/600], Train Loss: 0.0044, Gradient Norm: 0.20\n",
      "Epoch [224/600], Validation Loss: 0.0081\n",
      "Epoch [225/600], Train Loss: 0.0047, Gradient Norm: 0.20\n",
      "Epoch [225/600], Validation Loss: 0.0081\n",
      "Epoch [226/600], Train Loss: 0.0049, Gradient Norm: 0.08\n",
      "Epoch [226/600], Validation Loss: 0.0072\n",
      "Epoch [227/600], Train Loss: 0.0045, Gradient Norm: 0.13\n",
      "Epoch [227/600], Validation Loss: 0.0077\n",
      "Epoch [228/600], Train Loss: 0.0044, Gradient Norm: 0.07\n",
      "Epoch [228/600], Validation Loss: 0.0088\n",
      "Epoch [229/600], Train Loss: 0.0045, Gradient Norm: 0.04\n",
      "Epoch [229/600], Validation Loss: 0.0093\n",
      "Epoch [230/600], Train Loss: 0.0046, Gradient Norm: 0.11\n",
      "Epoch [230/600], Validation Loss: 0.0086\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_230.png\n",
      "Model saved at epoch 230\n",
      "Epoch [231/600], Train Loss: 0.0045, Gradient Norm: 0.41\n",
      "Epoch [231/600], Validation Loss: 0.0084\n",
      "Epoch 00231: reducing learning rate of group 0 to 4.8828e-08.\n",
      "Epoch [232/600], Train Loss: 0.0046, Gradient Norm: 0.07\n",
      "Epoch [232/600], Validation Loss: 0.0080\n",
      "Epoch [233/600], Train Loss: 0.0047, Gradient Norm: 0.09\n",
      "Epoch [233/600], Validation Loss: 0.0084\n",
      "Epoch [234/600], Train Loss: 0.0048, Gradient Norm: 0.16\n",
      "Epoch [234/600], Validation Loss: 0.0082\n",
      "Epoch [235/600], Train Loss: 0.0044, Gradient Norm: 0.18\n",
      "Epoch [235/600], Validation Loss: 0.0103\n",
      "Epoch [236/600], Train Loss: 0.0046, Gradient Norm: 0.16\n",
      "Epoch [236/600], Validation Loss: 0.0071\n",
      "Epoch [237/600], Train Loss: 0.0045, Gradient Norm: 0.03\n",
      "Epoch [237/600], Validation Loss: 0.0082\n",
      "Epoch [238/600], Train Loss: 0.0044, Gradient Norm: 0.08\n",
      "Epoch [238/600], Validation Loss: 0.0082\n",
      "Epoch [239/600], Train Loss: 0.0046, Gradient Norm: 0.19\n",
      "Epoch [239/600], Validation Loss: 0.0090\n",
      "Epoch [240/600], Train Loss: 0.0047, Gradient Norm: 0.09\n",
      "Epoch [240/600], Validation Loss: 0.0087\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_240.png\n",
      "Model saved at epoch 240\n",
      "Epoch [241/600], Train Loss: 0.0045, Gradient Norm: 0.29\n",
      "Epoch [241/600], Validation Loss: 0.0088\n",
      "Epoch [242/600], Train Loss: 0.0045, Gradient Norm: 0.15\n",
      "Epoch [242/600], Validation Loss: 0.0084\n",
      "Epoch [243/600], Train Loss: 0.0044, Gradient Norm: 0.08\n",
      "Epoch [243/600], Validation Loss: 0.0090\n",
      "Epoch [244/600], Train Loss: 0.0045, Gradient Norm: 0.05\n",
      "Epoch [244/600], Validation Loss: 0.0080\n",
      "Epoch [245/600], Train Loss: 0.0045, Gradient Norm: 0.12\n",
      "Epoch [245/600], Validation Loss: 0.0098\n",
      "Epoch [246/600], Train Loss: 0.0046, Gradient Norm: 0.02\n",
      "Epoch [246/600], Validation Loss: 0.0090\n",
      "Epoch [247/600], Train Loss: 0.0047, Gradient Norm: 0.13\n",
      "Epoch [247/600], Validation Loss: 0.0071\n",
      "Epoch 00247: reducing learning rate of group 0 to 2.4414e-08.\n",
      "Epoch [248/600], Train Loss: 0.0046, Gradient Norm: 0.05\n",
      "Epoch [248/600], Validation Loss: 0.0074\n",
      "Epoch [249/600], Train Loss: 0.0045, Gradient Norm: 0.09\n",
      "Epoch [249/600], Validation Loss: 0.0110\n",
      "Epoch [250/600], Train Loss: 0.0044, Gradient Norm: 0.06\n",
      "Epoch [250/600], Validation Loss: 0.0082\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_250.png\n",
      "Model saved at epoch 250\n",
      "Epoch [251/600], Train Loss: 0.0047, Gradient Norm: 0.05\n",
      "Epoch [251/600], Validation Loss: 0.0077\n",
      "Epoch [252/600], Train Loss: 0.0050, Gradient Norm: 0.36\n",
      "Epoch [252/600], Validation Loss: 0.0086\n",
      "Epoch [253/600], Train Loss: 0.0047, Gradient Norm: 0.20\n",
      "Epoch [253/600], Validation Loss: 0.0089\n",
      "Epoch [254/600], Train Loss: 0.0046, Gradient Norm: 0.15\n",
      "Epoch [254/600], Validation Loss: 0.0076\n",
      "Epoch [255/600], Train Loss: 0.0047, Gradient Norm: 0.05\n",
      "Epoch [255/600], Validation Loss: 0.0099\n",
      "Epoch [256/600], Train Loss: 0.0045, Gradient Norm: 0.06\n",
      "Epoch [256/600], Validation Loss: 0.0087\n",
      "Epoch [257/600], Train Loss: 0.0043, Gradient Norm: 0.10\n",
      "Epoch [257/600], Validation Loss: 0.0093\n",
      "Epoch [258/600], Train Loss: 0.0044, Gradient Norm: 0.15\n",
      "Epoch [258/600], Validation Loss: 0.0092\n",
      "Epoch [259/600], Train Loss: 0.0045, Gradient Norm: 0.10\n",
      "Epoch [259/600], Validation Loss: 0.0080\n",
      "Epoch [260/600], Train Loss: 0.0047, Gradient Norm: 0.11\n",
      "Epoch [260/600], Validation Loss: 0.0087\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_260.png\n",
      "Model saved at epoch 260\n",
      "Epoch [261/600], Train Loss: 0.0047, Gradient Norm: 0.06\n",
      "Epoch [261/600], Validation Loss: 0.0078\n",
      "Epoch [262/600], Train Loss: 0.0043, Gradient Norm: 0.10\n",
      "Epoch [262/600], Validation Loss: 0.0097\n",
      "Epoch [263/600], Train Loss: 0.0047, Gradient Norm: 0.07\n",
      "Epoch [263/600], Validation Loss: 0.0091\n",
      "Epoch 00263: reducing learning rate of group 0 to 1.2207e-08.\n",
      "Epoch [264/600], Train Loss: 0.0047, Gradient Norm: 0.09\n",
      "Epoch [264/600], Validation Loss: 0.0079\n",
      "Epoch [265/600], Train Loss: 0.0045, Gradient Norm: 0.10\n",
      "Epoch [265/600], Validation Loss: 0.0088\n",
      "Epoch [266/600], Train Loss: 0.0048, Gradient Norm: 0.07\n",
      "Epoch [266/600], Validation Loss: 0.0106\n",
      "Epoch [267/600], Train Loss: 0.0044, Gradient Norm: 0.24\n",
      "Epoch [267/600], Validation Loss: 0.0086\n",
      "Epoch [268/600], Train Loss: 0.0047, Gradient Norm: 0.10\n",
      "Epoch [268/600], Validation Loss: 0.0086\n",
      "Epoch [269/600], Train Loss: 0.0045, Gradient Norm: 0.07\n",
      "Epoch [269/600], Validation Loss: 0.0073\n",
      "Epoch [270/600], Train Loss: 0.0046, Gradient Norm: 0.12\n",
      "Epoch [270/600], Validation Loss: 0.0076\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_270.png\n",
      "Model saved at epoch 270\n",
      "Epoch [271/600], Train Loss: 0.0043, Gradient Norm: 0.16\n",
      "Epoch [271/600], Validation Loss: 0.0082\n",
      "Epoch [272/600], Train Loss: 0.0044, Gradient Norm: 0.12\n",
      "Epoch [272/600], Validation Loss: 0.0077\n",
      "Epoch [273/600], Train Loss: 0.0047, Gradient Norm: 0.12\n",
      "Epoch [273/600], Validation Loss: 0.0096\n",
      "Epoch [274/600], Train Loss: 0.0047, Gradient Norm: 0.11\n",
      "Epoch [274/600], Validation Loss: 0.0096\n",
      "Epoch [275/600], Train Loss: 0.0050, Gradient Norm: 0.14\n",
      "Epoch [275/600], Validation Loss: 0.0078\n",
      "Epoch [276/600], Train Loss: 0.0048, Gradient Norm: 0.08\n",
      "Epoch [276/600], Validation Loss: 0.0083\n",
      "Epoch [277/600], Train Loss: 0.0044, Gradient Norm: 0.09\n",
      "Epoch [277/600], Validation Loss: 0.0082\n",
      "Epoch [278/600], Train Loss: 0.0046, Gradient Norm: 0.07\n",
      "Epoch [278/600], Validation Loss: 0.0083\n",
      "Epoch [279/600], Train Loss: 0.0047, Gradient Norm: 0.08\n",
      "Epoch [279/600], Validation Loss: 0.0091\n",
      "Epoch [280/600], Train Loss: 0.0046, Gradient Norm: 0.21\n",
      "Epoch [280/600], Validation Loss: 0.0088\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_280.png\n",
      "Model saved at epoch 280\n",
      "Epoch [281/600], Train Loss: 0.0046, Gradient Norm: 0.17\n",
      "Epoch [281/600], Validation Loss: 0.0099\n",
      "Epoch [282/600], Train Loss: 0.0048, Gradient Norm: 0.08\n",
      "Epoch [282/600], Validation Loss: 0.0078\n",
      "Epoch [283/600], Train Loss: 0.0045, Gradient Norm: 0.12\n",
      "Epoch [283/600], Validation Loss: 0.0084\n",
      "Epoch [284/600], Train Loss: 0.0049, Gradient Norm: 0.21\n",
      "Epoch [284/600], Validation Loss: 0.0087\n",
      "Epoch [285/600], Train Loss: 0.0044, Gradient Norm: 0.08\n",
      "Epoch [285/600], Validation Loss: 0.0087\n",
      "Epoch [286/600], Train Loss: 0.0045, Gradient Norm: 0.05\n",
      "Epoch [286/600], Validation Loss: 0.0083\n",
      "Epoch [287/600], Train Loss: 0.0044, Gradient Norm: 0.34\n",
      "Epoch [287/600], Validation Loss: 0.0093\n",
      "Epoch [288/600], Train Loss: 0.0045, Gradient Norm: 0.05\n",
      "Epoch [288/600], Validation Loss: 0.0086\n",
      "Epoch [289/600], Train Loss: 0.0047, Gradient Norm: 0.12\n",
      "Epoch [289/600], Validation Loss: 0.0090\n",
      "Epoch [290/600], Train Loss: 0.0046, Gradient Norm: 0.13\n",
      "Epoch [290/600], Validation Loss: 0.0084\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_290.png\n",
      "Model saved at epoch 290\n",
      "Epoch [291/600], Train Loss: 0.0045, Gradient Norm: 0.14\n",
      "Epoch [291/600], Validation Loss: 0.0092\n",
      "Epoch [292/600], Train Loss: 0.0044, Gradient Norm: 0.09\n",
      "Epoch [292/600], Validation Loss: 0.0084\n",
      "Epoch [293/600], Train Loss: 0.0045, Gradient Norm: 0.13\n",
      "Epoch [293/600], Validation Loss: 0.0087\n",
      "Epoch [294/600], Train Loss: 0.0044, Gradient Norm: 0.02\n",
      "Epoch [294/600], Validation Loss: 0.0086\n",
      "Epoch [295/600], Train Loss: 0.0045, Gradient Norm: 0.10\n",
      "Epoch [295/600], Validation Loss: 0.0098\n",
      "Epoch [296/600], Train Loss: 0.0045, Gradient Norm: 0.04\n",
      "Epoch [296/600], Validation Loss: 0.0098\n",
      "Epoch [297/600], Train Loss: 0.0045, Gradient Norm: 0.07\n",
      "Epoch [297/600], Validation Loss: 0.0095\n",
      "Epoch [298/600], Train Loss: 0.0045, Gradient Norm: 0.11\n",
      "Epoch [298/600], Validation Loss: 0.0094\n",
      "Epoch [299/600], Train Loss: 0.0047, Gradient Norm: 0.38\n",
      "Epoch [299/600], Validation Loss: 0.0075\n",
      "Epoch [300/600], Train Loss: 0.0046, Gradient Norm: 0.24\n",
      "Epoch [300/600], Validation Loss: 0.0086\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_300.png\n",
      "Model saved at epoch 300\n",
      "Epoch [301/600], Train Loss: 0.0046, Gradient Norm: 0.08\n",
      "Epoch [301/600], Validation Loss: 0.0092\n",
      "Epoch [302/600], Train Loss: 0.0046, Gradient Norm: 0.07\n",
      "Epoch [302/600], Validation Loss: 0.0083\n",
      "Epoch [303/600], Train Loss: 0.0047, Gradient Norm: 0.15\n",
      "Epoch [303/600], Validation Loss: 0.0076\n",
      "Epoch [304/600], Train Loss: 0.0047, Gradient Norm: 0.11\n",
      "Epoch [304/600], Validation Loss: 0.0089\n",
      "Epoch [305/600], Train Loss: 0.0048, Gradient Norm: 0.18\n",
      "Epoch [305/600], Validation Loss: 0.0085\n",
      "Epoch [306/600], Train Loss: 0.0049, Gradient Norm: 0.18\n",
      "Epoch [306/600], Validation Loss: 0.0079\n",
      "Epoch [307/600], Train Loss: 0.0045, Gradient Norm: 0.11\n",
      "Epoch [307/600], Validation Loss: 0.0076\n",
      "Epoch [308/600], Train Loss: 0.0047, Gradient Norm: 0.16\n",
      "Epoch [308/600], Validation Loss: 0.0089\n",
      "Epoch [309/600], Train Loss: 0.0045, Gradient Norm: 0.14\n",
      "Epoch [309/600], Validation Loss: 0.0087\n",
      "Epoch [310/600], Train Loss: 0.0046, Gradient Norm: 0.14\n",
      "Epoch [310/600], Validation Loss: 0.0093\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_310.png\n",
      "Model saved at epoch 310\n",
      "Epoch [311/600], Train Loss: 0.0047, Gradient Norm: 0.05\n",
      "Epoch [311/600], Validation Loss: 0.0083\n",
      "Epoch [312/600], Train Loss: 0.0044, Gradient Norm: 0.11\n",
      "Epoch [312/600], Validation Loss: 0.0086\n",
      "Epoch [313/600], Train Loss: 0.0047, Gradient Norm: 0.09\n",
      "Epoch [313/600], Validation Loss: 0.0085\n",
      "Epoch [314/600], Train Loss: 0.0045, Gradient Norm: 0.08\n",
      "Epoch [314/600], Validation Loss: 0.0091\n",
      "Epoch [315/600], Train Loss: 0.0045, Gradient Norm: 0.02\n",
      "Epoch [315/600], Validation Loss: 0.0076\n",
      "Epoch [316/600], Train Loss: 0.0044, Gradient Norm: 0.09\n",
      "Epoch [316/600], Validation Loss: 0.0083\n",
      "Epoch [317/600], Train Loss: 0.0049, Gradient Norm: 0.11\n",
      "Epoch [317/600], Validation Loss: 0.0087\n",
      "Epoch [318/600], Train Loss: 0.0047, Gradient Norm: 0.10\n",
      "Epoch [318/600], Validation Loss: 0.0069\n",
      "Epoch [319/600], Train Loss: 0.0045, Gradient Norm: 0.03\n",
      "Epoch [319/600], Validation Loss: 0.0090\n",
      "Epoch [320/600], Train Loss: 0.0049, Gradient Norm: 0.08\n",
      "Epoch [320/600], Validation Loss: 0.0081\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_320.png\n",
      "Model saved at epoch 320\n",
      "Epoch [321/600], Train Loss: 0.0046, Gradient Norm: 0.05\n",
      "Epoch [321/600], Validation Loss: 0.0085\n",
      "Epoch [322/600], Train Loss: 0.0045, Gradient Norm: 0.10\n",
      "Epoch [322/600], Validation Loss: 0.0084\n",
      "Epoch [323/600], Train Loss: 0.0045, Gradient Norm: 0.06\n",
      "Epoch [323/600], Validation Loss: 0.0079\n",
      "Epoch [324/600], Train Loss: 0.0050, Gradient Norm: 0.08\n",
      "Epoch [324/600], Validation Loss: 0.0088\n",
      "Epoch [325/600], Train Loss: 0.0043, Gradient Norm: 0.09\n",
      "Epoch [325/600], Validation Loss: 0.0072\n",
      "Epoch [326/600], Train Loss: 0.0045, Gradient Norm: 0.11\n",
      "Epoch [326/600], Validation Loss: 0.0087\n",
      "Epoch [327/600], Train Loss: 0.0045, Gradient Norm: 0.09\n",
      "Epoch [327/600], Validation Loss: 0.0074\n",
      "Epoch [328/600], Train Loss: 0.0048, Gradient Norm: 0.36\n",
      "Epoch [328/600], Validation Loss: 0.0085\n",
      "Epoch [329/600], Train Loss: 0.0045, Gradient Norm: 0.07\n",
      "Epoch [329/600], Validation Loss: 0.0091\n",
      "Epoch [330/600], Train Loss: 0.0046, Gradient Norm: 0.04\n",
      "Epoch [330/600], Validation Loss: 0.0093\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_330.png\n",
      "Model saved at epoch 330\n",
      "Epoch [331/600], Train Loss: 0.0045, Gradient Norm: 0.08\n",
      "Epoch [331/600], Validation Loss: 0.0079\n",
      "Epoch [332/600], Train Loss: 0.0047, Gradient Norm: 0.05\n",
      "Epoch [332/600], Validation Loss: 0.0090\n",
      "Epoch [333/600], Train Loss: 0.0044, Gradient Norm: 0.05\n",
      "Epoch [333/600], Validation Loss: 0.0079\n",
      "Epoch [334/600], Train Loss: 0.0046, Gradient Norm: 0.12\n",
      "Epoch [334/600], Validation Loss: 0.0076\n",
      "Epoch [335/600], Train Loss: 0.0045, Gradient Norm: 0.11\n",
      "Epoch [335/600], Validation Loss: 0.0095\n",
      "Epoch [336/600], Train Loss: 0.0046, Gradient Norm: 0.10\n",
      "Epoch [336/600], Validation Loss: 0.0077\n",
      "Epoch [337/600], Train Loss: 0.0046, Gradient Norm: 0.04\n",
      "Epoch [337/600], Validation Loss: 0.0075\n",
      "Epoch [338/600], Train Loss: 0.0046, Gradient Norm: 0.03\n",
      "Epoch [338/600], Validation Loss: 0.0088\n",
      "Epoch [339/600], Train Loss: 0.0047, Gradient Norm: 0.05\n",
      "Epoch [339/600], Validation Loss: 0.0091\n",
      "Epoch [340/600], Train Loss: 0.0045, Gradient Norm: 0.06\n",
      "Epoch [340/600], Validation Loss: 0.0089\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_340.png\n",
      "Model saved at epoch 340\n",
      "Epoch [341/600], Train Loss: 0.0046, Gradient Norm: 0.05\n",
      "Epoch [341/600], Validation Loss: 0.0079\n",
      "Epoch [342/600], Train Loss: 0.0046, Gradient Norm: 0.08\n",
      "Epoch [342/600], Validation Loss: 0.0080\n",
      "Epoch [343/600], Train Loss: 0.0046, Gradient Norm: 0.46\n",
      "Epoch [343/600], Validation Loss: 0.0085\n",
      "Epoch [344/600], Train Loss: 0.0045, Gradient Norm: 0.07\n",
      "Epoch [344/600], Validation Loss: 0.0086\n",
      "Epoch [345/600], Train Loss: 0.0044, Gradient Norm: 0.23\n",
      "Epoch [345/600], Validation Loss: 0.0089\n",
      "Epoch [346/600], Train Loss: 0.0046, Gradient Norm: 0.20\n",
      "Epoch [346/600], Validation Loss: 0.0093\n",
      "Epoch [347/600], Train Loss: 0.0047, Gradient Norm: 0.20\n",
      "Epoch [347/600], Validation Loss: 0.0093\n",
      "Epoch [348/600], Train Loss: 0.0046, Gradient Norm: 0.08\n",
      "Epoch [348/600], Validation Loss: 0.0086\n",
      "Epoch [349/600], Train Loss: 0.0045, Gradient Norm: 0.02\n",
      "Epoch [349/600], Validation Loss: 0.0092\n",
      "Epoch [350/600], Train Loss: 0.0044, Gradient Norm: 0.13\n",
      "Epoch [350/600], Validation Loss: 0.0091\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_350.png\n",
      "Model saved at epoch 350\n",
      "Epoch [351/600], Train Loss: 0.0046, Gradient Norm: 0.17\n",
      "Epoch [351/600], Validation Loss: 0.0089\n",
      "Epoch [352/600], Train Loss: 0.0044, Gradient Norm: 0.10\n",
      "Epoch [352/600], Validation Loss: 0.0080\n",
      "Epoch [353/600], Train Loss: 0.0049, Gradient Norm: 0.06\n",
      "Epoch [353/600], Validation Loss: 0.0095\n",
      "Epoch [354/600], Train Loss: 0.0045, Gradient Norm: 0.04\n",
      "Epoch [354/600], Validation Loss: 0.0089\n",
      "Epoch [355/600], Train Loss: 0.0046, Gradient Norm: 0.05\n",
      "Epoch [355/600], Validation Loss: 0.0079\n",
      "Epoch [356/600], Train Loss: 0.0046, Gradient Norm: 0.18\n",
      "Epoch [356/600], Validation Loss: 0.0087\n",
      "Epoch [357/600], Train Loss: 0.0049, Gradient Norm: 0.07\n",
      "Epoch [357/600], Validation Loss: 0.0091\n",
      "Epoch [358/600], Train Loss: 0.0049, Gradient Norm: 0.18\n",
      "Epoch [358/600], Validation Loss: 0.0082\n",
      "Epoch [359/600], Train Loss: 0.0045, Gradient Norm: 0.14\n",
      "Epoch [359/600], Validation Loss: 0.0076\n",
      "Epoch [360/600], Train Loss: 0.0045, Gradient Norm: 0.09\n",
      "Epoch [360/600], Validation Loss: 0.0100\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_360.png\n",
      "Model saved at epoch 360\n",
      "Epoch [361/600], Train Loss: 0.0043, Gradient Norm: 0.08\n",
      "Epoch [361/600], Validation Loss: 0.0083\n",
      "Epoch [362/600], Train Loss: 0.0046, Gradient Norm: 0.04\n",
      "Epoch [362/600], Validation Loss: 0.0090\n",
      "Epoch [363/600], Train Loss: 0.0044, Gradient Norm: 0.10\n",
      "Epoch [363/600], Validation Loss: 0.0078\n",
      "Epoch [364/600], Train Loss: 0.0047, Gradient Norm: 0.15\n",
      "Epoch [364/600], Validation Loss: 0.0078\n",
      "Epoch [365/600], Train Loss: 0.0048, Gradient Norm: 0.09\n",
      "Epoch [365/600], Validation Loss: 0.0086\n",
      "Epoch [366/600], Train Loss: 0.0044, Gradient Norm: 0.07\n",
      "Epoch [366/600], Validation Loss: 0.0089\n",
      "Epoch [367/600], Train Loss: 0.0043, Gradient Norm: 0.07\n",
      "Epoch [367/600], Validation Loss: 0.0083\n",
      "Epoch [368/600], Train Loss: 0.0045, Gradient Norm: 0.04\n",
      "Epoch [368/600], Validation Loss: 0.0079\n",
      "Epoch [369/600], Train Loss: 0.0045, Gradient Norm: 0.18\n",
      "Epoch [369/600], Validation Loss: 0.0086\n",
      "Epoch [370/600], Train Loss: 0.0045, Gradient Norm: 0.10\n",
      "Epoch [370/600], Validation Loss: 0.0083\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_370.png\n",
      "Model saved at epoch 370\n",
      "Epoch [371/600], Train Loss: 0.0045, Gradient Norm: 0.05\n",
      "Epoch [371/600], Validation Loss: 0.0082\n",
      "Epoch [372/600], Train Loss: 0.0046, Gradient Norm: 0.35\n",
      "Epoch [372/600], Validation Loss: 0.0087\n",
      "Epoch [373/600], Train Loss: 0.0043, Gradient Norm: 0.06\n",
      "Epoch [373/600], Validation Loss: 0.0083\n",
      "Epoch [374/600], Train Loss: 0.0041, Gradient Norm: 0.05\n",
      "Epoch [374/600], Validation Loss: 0.0084\n",
      "Epoch [375/600], Train Loss: 0.0048, Gradient Norm: 0.23\n",
      "Epoch [375/600], Validation Loss: 0.0084\n",
      "Epoch [376/600], Train Loss: 0.0049, Gradient Norm: 0.06\n",
      "Epoch [376/600], Validation Loss: 0.0090\n",
      "Epoch [377/600], Train Loss: 0.0047, Gradient Norm: 0.17\n",
      "Epoch [377/600], Validation Loss: 0.0097\n",
      "Epoch [378/600], Train Loss: 0.0047, Gradient Norm: 0.05\n",
      "Epoch [378/600], Validation Loss: 0.0086\n",
      "Epoch [379/600], Train Loss: 0.0046, Gradient Norm: 0.09\n",
      "Epoch [379/600], Validation Loss: 0.0078\n",
      "Epoch [380/600], Train Loss: 0.0044, Gradient Norm: 0.04\n",
      "Epoch [380/600], Validation Loss: 0.0097\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_380.png\n",
      "Model saved at epoch 380\n",
      "Epoch [381/600], Train Loss: 0.0044, Gradient Norm: 0.14\n",
      "Epoch [381/600], Validation Loss: 0.0092\n",
      "Epoch [382/600], Train Loss: 0.0046, Gradient Norm: 0.11\n",
      "Epoch [382/600], Validation Loss: 0.0072\n",
      "Epoch [383/600], Train Loss: 0.0047, Gradient Norm: 0.06\n",
      "Epoch [383/600], Validation Loss: 0.0084\n",
      "Epoch [384/600], Train Loss: 0.0045, Gradient Norm: 0.10\n",
      "Epoch [384/600], Validation Loss: 0.0094\n",
      "Epoch [385/600], Train Loss: 0.0045, Gradient Norm: 0.12\n",
      "Epoch [385/600], Validation Loss: 0.0087\n",
      "Epoch [386/600], Train Loss: 0.0046, Gradient Norm: 0.11\n",
      "Epoch [386/600], Validation Loss: 0.0094\n",
      "Epoch [387/600], Train Loss: 0.0047, Gradient Norm: 0.10\n",
      "Epoch [387/600], Validation Loss: 0.0081\n",
      "Epoch [388/600], Train Loss: 0.0044, Gradient Norm: 0.05\n",
      "Epoch [388/600], Validation Loss: 0.0094\n",
      "Epoch [389/600], Train Loss: 0.0047, Gradient Norm: 0.16\n",
      "Epoch [389/600], Validation Loss: 0.0086\n",
      "Epoch [390/600], Train Loss: 0.0045, Gradient Norm: 0.05\n",
      "Epoch [390/600], Validation Loss: 0.0073\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_390.png\n",
      "Model saved at epoch 390\n",
      "Epoch [391/600], Train Loss: 0.0050, Gradient Norm: 0.08\n",
      "Epoch [391/600], Validation Loss: 0.0081\n",
      "Epoch [392/600], Train Loss: 0.0044, Gradient Norm: 0.11\n",
      "Epoch [392/600], Validation Loss: 0.0075\n",
      "Epoch [393/600], Train Loss: 0.0047, Gradient Norm: 0.09\n",
      "Epoch [393/600], Validation Loss: 0.0081\n",
      "Epoch [394/600], Train Loss: 0.0047, Gradient Norm: 0.06\n",
      "Epoch [394/600], Validation Loss: 0.0090\n",
      "Epoch [395/600], Train Loss: 0.0047, Gradient Norm: 0.22\n",
      "Epoch [395/600], Validation Loss: 0.0086\n",
      "Epoch [396/600], Train Loss: 0.0046, Gradient Norm: 0.19\n",
      "Epoch [396/600], Validation Loss: 0.0084\n",
      "Epoch [397/600], Train Loss: 0.0044, Gradient Norm: 0.23\n",
      "Epoch [397/600], Validation Loss: 0.0096\n",
      "Epoch [398/600], Train Loss: 0.0044, Gradient Norm: 0.14\n",
      "Epoch [398/600], Validation Loss: 0.0083\n",
      "Epoch [399/600], Train Loss: 0.0046, Gradient Norm: 0.05\n",
      "Epoch [399/600], Validation Loss: 0.0096\n",
      "Epoch [400/600], Train Loss: 0.0044, Gradient Norm: 0.08\n",
      "Epoch [400/600], Validation Loss: 0.0081\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_400.png\n",
      "Model saved at epoch 400\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint_400.pth\n",
      "Special checkpoint saved at epoch 400\n",
      "Epoch [401/600], Train Loss: 0.0046, Gradient Norm: 0.03\n",
      "Epoch [401/600], Validation Loss: 0.0083\n",
      "Epoch [402/600], Train Loss: 0.0047, Gradient Norm: 0.05\n",
      "Epoch [402/600], Validation Loss: 0.0076\n",
      "Epoch [403/600], Train Loss: 0.0044, Gradient Norm: 0.12\n",
      "Epoch [403/600], Validation Loss: 0.0096\n",
      "Epoch [404/600], Train Loss: 0.0048, Gradient Norm: 0.09\n",
      "Epoch [404/600], Validation Loss: 0.0081\n",
      "Epoch [405/600], Train Loss: 0.0048, Gradient Norm: 0.05\n",
      "Epoch [405/600], Validation Loss: 0.0091\n",
      "Epoch [406/600], Train Loss: 0.0042, Gradient Norm: 0.08\n",
      "Epoch [406/600], Validation Loss: 0.0090\n",
      "Epoch [407/600], Train Loss: 0.0047, Gradient Norm: 0.16\n",
      "Epoch [407/600], Validation Loss: 0.0089\n",
      "Epoch [408/600], Train Loss: 0.0044, Gradient Norm: 0.06\n",
      "Epoch [408/600], Validation Loss: 0.0083\n",
      "Epoch [409/600], Train Loss: 0.0048, Gradient Norm: 0.06\n",
      "Epoch [409/600], Validation Loss: 0.0078\n",
      "Epoch [410/600], Train Loss: 0.0044, Gradient Norm: 0.32\n",
      "Epoch [410/600], Validation Loss: 0.0092\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_410.png\n",
      "Model saved at epoch 410\n",
      "Epoch [411/600], Train Loss: 0.0046, Gradient Norm: 0.10\n",
      "Epoch [411/600], Validation Loss: 0.0089\n",
      "Epoch [412/600], Train Loss: 0.0049, Gradient Norm: 0.38\n",
      "Epoch [412/600], Validation Loss: 0.0090\n",
      "Epoch [413/600], Train Loss: 0.0048, Gradient Norm: 0.05\n",
      "Epoch [413/600], Validation Loss: 0.0090\n",
      "Epoch [414/600], Train Loss: 0.0044, Gradient Norm: 0.20\n",
      "Epoch [414/600], Validation Loss: 0.0085\n",
      "Epoch [415/600], Train Loss: 0.0045, Gradient Norm: 0.23\n",
      "Epoch [415/600], Validation Loss: 0.0080\n",
      "Epoch [416/600], Train Loss: 0.0046, Gradient Norm: 0.08\n",
      "Epoch [416/600], Validation Loss: 0.0071\n",
      "Epoch [417/600], Train Loss: 0.0044, Gradient Norm: 0.04\n",
      "Epoch [417/600], Validation Loss: 0.0079\n",
      "Epoch [418/600], Train Loss: 0.0047, Gradient Norm: 0.16\n",
      "Epoch [418/600], Validation Loss: 0.0086\n",
      "Epoch [419/600], Train Loss: 0.0048, Gradient Norm: 0.10\n",
      "Epoch [419/600], Validation Loss: 0.0081\n",
      "Epoch [420/600], Train Loss: 0.0048, Gradient Norm: 0.13\n",
      "Epoch [420/600], Validation Loss: 0.0094\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_420.png\n",
      "Model saved at epoch 420\n",
      "Epoch [421/600], Train Loss: 0.0046, Gradient Norm: 0.09\n",
      "Epoch [421/600], Validation Loss: 0.0097\n",
      "Epoch [422/600], Train Loss: 0.0047, Gradient Norm: 0.10\n",
      "Epoch [422/600], Validation Loss: 0.0092\n",
      "Epoch [423/600], Train Loss: 0.0047, Gradient Norm: 0.11\n",
      "Epoch [423/600], Validation Loss: 0.0086\n",
      "Epoch [424/600], Train Loss: 0.0045, Gradient Norm: 0.11\n",
      "Epoch [424/600], Validation Loss: 0.0081\n",
      "Epoch [425/600], Train Loss: 0.0045, Gradient Norm: 0.09\n",
      "Epoch [425/600], Validation Loss: 0.0093\n",
      "Epoch [426/600], Train Loss: 0.0047, Gradient Norm: 0.10\n",
      "Epoch [426/600], Validation Loss: 0.0081\n",
      "Epoch [427/600], Train Loss: 0.0044, Gradient Norm: 0.09\n",
      "Epoch [427/600], Validation Loss: 0.0083\n",
      "Epoch [428/600], Train Loss: 0.0045, Gradient Norm: 0.12\n",
      "Epoch [428/600], Validation Loss: 0.0075\n",
      "Epoch [429/600], Train Loss: 0.0050, Gradient Norm: 0.10\n",
      "Epoch [429/600], Validation Loss: 0.0084\n",
      "Epoch [430/600], Train Loss: 0.0046, Gradient Norm: 0.11\n",
      "Epoch [430/600], Validation Loss: 0.0089\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_430.png\n",
      "Model saved at epoch 430\n",
      "Epoch [431/600], Train Loss: 0.0047, Gradient Norm: 0.06\n",
      "Epoch [431/600], Validation Loss: 0.0084\n",
      "Epoch [432/600], Train Loss: 0.0044, Gradient Norm: 0.18\n",
      "Epoch [432/600], Validation Loss: 0.0080\n",
      "Epoch [433/600], Train Loss: 0.0047, Gradient Norm: 0.12\n",
      "Epoch [433/600], Validation Loss: 0.0093\n",
      "Epoch [434/600], Train Loss: 0.0048, Gradient Norm: 0.06\n",
      "Epoch [434/600], Validation Loss: 0.0088\n",
      "Epoch [435/600], Train Loss: 0.0046, Gradient Norm: 0.17\n",
      "Epoch [435/600], Validation Loss: 0.0087\n",
      "Epoch [436/600], Train Loss: 0.0048, Gradient Norm: 0.10\n",
      "Epoch [436/600], Validation Loss: 0.0084\n",
      "Epoch [437/600], Train Loss: 0.0047, Gradient Norm: 0.17\n",
      "Epoch [437/600], Validation Loss: 0.0095\n",
      "Epoch [438/600], Train Loss: 0.0047, Gradient Norm: 0.09\n",
      "Epoch [438/600], Validation Loss: 0.0084\n",
      "Epoch [439/600], Train Loss: 0.0047, Gradient Norm: 0.03\n",
      "Epoch [439/600], Validation Loss: 0.0080\n",
      "Epoch [440/600], Train Loss: 0.0046, Gradient Norm: 0.26\n",
      "Epoch [440/600], Validation Loss: 0.0087\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_440.png\n",
      "Model saved at epoch 440\n",
      "Epoch [441/600], Train Loss: 0.0049, Gradient Norm: 0.05\n",
      "Epoch [441/600], Validation Loss: 0.0071\n",
      "Epoch [442/600], Train Loss: 0.0046, Gradient Norm: 0.22\n",
      "Epoch [442/600], Validation Loss: 0.0082\n",
      "Epoch [443/600], Train Loss: 0.0047, Gradient Norm: 0.08\n",
      "Epoch [443/600], Validation Loss: 0.0090\n",
      "Epoch [444/600], Train Loss: 0.0046, Gradient Norm: 0.13\n",
      "Epoch [444/600], Validation Loss: 0.0079\n",
      "Epoch [445/600], Train Loss: 0.0046, Gradient Norm: 0.09\n",
      "Epoch [445/600], Validation Loss: 0.0096\n",
      "Epoch [446/600], Train Loss: 0.0048, Gradient Norm: 0.09\n",
      "Epoch [446/600], Validation Loss: 0.0084\n",
      "Epoch [447/600], Train Loss: 0.0045, Gradient Norm: 0.12\n",
      "Epoch [447/600], Validation Loss: 0.0087\n",
      "Epoch [448/600], Train Loss: 0.0048, Gradient Norm: 0.08\n",
      "Epoch [448/600], Validation Loss: 0.0073\n",
      "Epoch [449/600], Train Loss: 0.0048, Gradient Norm: 0.23\n",
      "Epoch [449/600], Validation Loss: 0.0093\n",
      "Epoch [450/600], Train Loss: 0.0048, Gradient Norm: 0.09\n",
      "Epoch [450/600], Validation Loss: 0.0088\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_450.png\n",
      "Model saved at epoch 450\n",
      "Epoch [451/600], Train Loss: 0.0047, Gradient Norm: 0.04\n",
      "Epoch [451/600], Validation Loss: 0.0088\n",
      "Epoch [452/600], Train Loss: 0.0045, Gradient Norm: 0.07\n",
      "Epoch [452/600], Validation Loss: 0.0091\n",
      "Epoch [453/600], Train Loss: 0.0046, Gradient Norm: 0.07\n",
      "Epoch [453/600], Validation Loss: 0.0084\n",
      "Epoch [454/600], Train Loss: 0.0045, Gradient Norm: 0.08\n",
      "Epoch [454/600], Validation Loss: 0.0072\n",
      "Epoch [455/600], Train Loss: 0.0047, Gradient Norm: 0.05\n",
      "Epoch [455/600], Validation Loss: 0.0092\n",
      "Epoch [456/600], Train Loss: 0.0043, Gradient Norm: 0.10\n",
      "Epoch [456/600], Validation Loss: 0.0093\n",
      "Epoch [457/600], Train Loss: 0.0047, Gradient Norm: 0.06\n",
      "Epoch [457/600], Validation Loss: 0.0095\n",
      "Epoch [458/600], Train Loss: 0.0045, Gradient Norm: 0.07\n",
      "Epoch [458/600], Validation Loss: 0.0081\n",
      "Epoch [459/600], Train Loss: 0.0044, Gradient Norm: 0.06\n",
      "Epoch [459/600], Validation Loss: 0.0088\n",
      "Epoch [460/600], Train Loss: 0.0046, Gradient Norm: 0.13\n",
      "Epoch [460/600], Validation Loss: 0.0075\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_460.png\n",
      "Model saved at epoch 460\n",
      "Epoch [461/600], Train Loss: 0.0046, Gradient Norm: 0.05\n",
      "Epoch [461/600], Validation Loss: 0.0072\n",
      "Epoch [462/600], Train Loss: 0.0044, Gradient Norm: 0.13\n",
      "Epoch [462/600], Validation Loss: 0.0092\n",
      "Epoch [463/600], Train Loss: 0.0045, Gradient Norm: 0.05\n",
      "Epoch [463/600], Validation Loss: 0.0094\n",
      "Epoch [464/600], Train Loss: 0.0045, Gradient Norm: 0.14\n",
      "Epoch [464/600], Validation Loss: 0.0085\n",
      "Epoch [465/600], Train Loss: 0.0044, Gradient Norm: 0.06\n",
      "Epoch [465/600], Validation Loss: 0.0077\n",
      "Epoch [466/600], Train Loss: 0.0047, Gradient Norm: 0.07\n",
      "Epoch [466/600], Validation Loss: 0.0094\n",
      "Epoch [467/600], Train Loss: 0.0047, Gradient Norm: 0.23\n",
      "Epoch [467/600], Validation Loss: 0.0086\n",
      "Epoch [468/600], Train Loss: 0.0043, Gradient Norm: 0.10\n",
      "Epoch [468/600], Validation Loss: 0.0088\n",
      "Epoch [469/600], Train Loss: 0.0047, Gradient Norm: 0.08\n",
      "Epoch [469/600], Validation Loss: 0.0079\n",
      "Epoch [470/600], Train Loss: 0.0045, Gradient Norm: 0.09\n",
      "Epoch [470/600], Validation Loss: 0.0083\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_470.png\n",
      "Model saved at epoch 470\n",
      "Epoch [471/600], Train Loss: 0.0047, Gradient Norm: 0.05\n",
      "Epoch [471/600], Validation Loss: 0.0091\n",
      "Epoch [472/600], Train Loss: 0.0046, Gradient Norm: 0.11\n",
      "Epoch [472/600], Validation Loss: 0.0074\n",
      "Epoch [473/600], Train Loss: 0.0048, Gradient Norm: 0.12\n",
      "Epoch [473/600], Validation Loss: 0.0074\n",
      "Epoch [474/600], Train Loss: 0.0045, Gradient Norm: 0.07\n",
      "Epoch [474/600], Validation Loss: 0.0089\n",
      "Epoch [475/600], Train Loss: 0.0045, Gradient Norm: 0.11\n",
      "Epoch [475/600], Validation Loss: 0.0082\n",
      "Epoch [476/600], Train Loss: 0.0047, Gradient Norm: 0.06\n",
      "Epoch [476/600], Validation Loss: 0.0079\n",
      "Epoch [477/600], Train Loss: 0.0046, Gradient Norm: 0.13\n",
      "Epoch [477/600], Validation Loss: 0.0076\n",
      "Epoch [478/600], Train Loss: 0.0048, Gradient Norm: 0.07\n",
      "Epoch [478/600], Validation Loss: 0.0095\n",
      "Epoch [479/600], Train Loss: 0.0044, Gradient Norm: 0.06\n",
      "Epoch [479/600], Validation Loss: 0.0090\n",
      "Epoch [480/600], Train Loss: 0.0048, Gradient Norm: 0.09\n",
      "Epoch [480/600], Validation Loss: 0.0082\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_480.png\n",
      "Model saved at epoch 480\n",
      "Epoch [481/600], Train Loss: 0.0047, Gradient Norm: 0.18\n",
      "Epoch [481/600], Validation Loss: 0.0083\n",
      "Epoch [482/600], Train Loss: 0.0045, Gradient Norm: 0.11\n",
      "Epoch [482/600], Validation Loss: 0.0082\n",
      "Epoch [483/600], Train Loss: 0.0045, Gradient Norm: 0.05\n",
      "Epoch [483/600], Validation Loss: 0.0089\n",
      "Epoch [484/600], Train Loss: 0.0048, Gradient Norm: 0.11\n",
      "Epoch [484/600], Validation Loss: 0.0090\n",
      "Epoch [485/600], Train Loss: 0.0047, Gradient Norm: 0.10\n",
      "Epoch [485/600], Validation Loss: 0.0073\n",
      "Epoch [486/600], Train Loss: 0.0046, Gradient Norm: 0.07\n",
      "Epoch [486/600], Validation Loss: 0.0090\n",
      "Epoch [487/600], Train Loss: 0.0045, Gradient Norm: 0.04\n",
      "Epoch [487/600], Validation Loss: 0.0084\n",
      "Epoch [488/600], Train Loss: 0.0045, Gradient Norm: 0.12\n",
      "Epoch [488/600], Validation Loss: 0.0084\n",
      "Epoch [489/600], Train Loss: 0.0047, Gradient Norm: 0.37\n",
      "Epoch [489/600], Validation Loss: 0.0080\n",
      "Epoch [490/600], Train Loss: 0.0046, Gradient Norm: 0.03\n",
      "Epoch [490/600], Validation Loss: 0.0084\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_490.png\n",
      "Model saved at epoch 490\n",
      "Epoch [491/600], Train Loss: 0.0046, Gradient Norm: 0.08\n",
      "Epoch [491/600], Validation Loss: 0.0090\n",
      "Epoch [492/600], Train Loss: 0.0045, Gradient Norm: 0.08\n",
      "Epoch [492/600], Validation Loss: 0.0086\n",
      "Epoch [493/600], Train Loss: 0.0046, Gradient Norm: 0.09\n",
      "Epoch [493/600], Validation Loss: 0.0072\n",
      "Epoch [494/600], Train Loss: 0.0046, Gradient Norm: 0.09\n",
      "Epoch [494/600], Validation Loss: 0.0067\n",
      "Epoch [495/600], Train Loss: 0.0049, Gradient Norm: 0.33\n",
      "Epoch [495/600], Validation Loss: 0.0095\n",
      "Epoch [496/600], Train Loss: 0.0045, Gradient Norm: 0.09\n",
      "Epoch [496/600], Validation Loss: 0.0082\n",
      "Epoch [497/600], Train Loss: 0.0046, Gradient Norm: 0.14\n",
      "Epoch [497/600], Validation Loss: 0.0075\n",
      "Epoch [498/600], Train Loss: 0.0045, Gradient Norm: 0.14\n",
      "Epoch [498/600], Validation Loss: 0.0076\n",
      "Epoch [499/600], Train Loss: 0.0047, Gradient Norm: 0.06\n",
      "Epoch [499/600], Validation Loss: 0.0084\n",
      "Epoch [500/600], Train Loss: 0.0045, Gradient Norm: 0.08\n",
      "Epoch [500/600], Validation Loss: 0.0101\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_500.png\n",
      "Model saved at epoch 500\n",
      "Epoch [501/600], Train Loss: 0.0044, Gradient Norm: 0.07\n",
      "Epoch [501/600], Validation Loss: 0.0085\n",
      "Epoch [502/600], Train Loss: 0.0047, Gradient Norm: 0.05\n",
      "Epoch [502/600], Validation Loss: 0.0087\n",
      "Epoch [503/600], Train Loss: 0.0046, Gradient Norm: 0.11\n",
      "Epoch [503/600], Validation Loss: 0.0091\n",
      "Epoch [504/600], Train Loss: 0.0046, Gradient Norm: 0.05\n",
      "Epoch [504/600], Validation Loss: 0.0079\n",
      "Epoch [505/600], Train Loss: 0.0046, Gradient Norm: 0.12\n",
      "Epoch [505/600], Validation Loss: 0.0078\n",
      "Epoch [506/600], Train Loss: 0.0046, Gradient Norm: 0.12\n",
      "Epoch [506/600], Validation Loss: 0.0077\n",
      "Epoch [507/600], Train Loss: 0.0046, Gradient Norm: 0.10\n",
      "Epoch [507/600], Validation Loss: 0.0084\n",
      "Epoch [508/600], Train Loss: 0.0046, Gradient Norm: 0.10\n",
      "Epoch [508/600], Validation Loss: 0.0081\n",
      "Epoch [509/600], Train Loss: 0.0043, Gradient Norm: 0.11\n",
      "Epoch [509/600], Validation Loss: 0.0081\n",
      "Epoch [510/600], Train Loss: 0.0045, Gradient Norm: 0.04\n",
      "Epoch [510/600], Validation Loss: 0.0087\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_510.png\n",
      "Model saved at epoch 510\n",
      "Epoch [511/600], Train Loss: 0.0046, Gradient Norm: 0.23\n",
      "Epoch [511/600], Validation Loss: 0.0098\n",
      "Epoch [512/600], Train Loss: 0.0045, Gradient Norm: 0.15\n",
      "Epoch [512/600], Validation Loss: 0.0083\n",
      "Epoch [513/600], Train Loss: 0.0045, Gradient Norm: 0.04\n",
      "Epoch [513/600], Validation Loss: 0.0078\n",
      "Epoch [514/600], Train Loss: 0.0047, Gradient Norm: 0.12\n",
      "Epoch [514/600], Validation Loss: 0.0078\n",
      "Epoch [515/600], Train Loss: 0.0045, Gradient Norm: 0.10\n",
      "Epoch [515/600], Validation Loss: 0.0088\n",
      "Epoch [516/600], Train Loss: 0.0046, Gradient Norm: 0.09\n",
      "Epoch [516/600], Validation Loss: 0.0072\n",
      "Epoch [517/600], Train Loss: 0.0045, Gradient Norm: 0.04\n",
      "Epoch [517/600], Validation Loss: 0.0080\n",
      "Epoch [518/600], Train Loss: 0.0047, Gradient Norm: 0.07\n",
      "Epoch [518/600], Validation Loss: 0.0078\n",
      "Epoch [519/600], Train Loss: 0.0044, Gradient Norm: 0.03\n",
      "Epoch [519/600], Validation Loss: 0.0084\n",
      "Epoch [520/600], Train Loss: 0.0044, Gradient Norm: 0.07\n",
      "Epoch [520/600], Validation Loss: 0.0089\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_520.png\n",
      "Model saved at epoch 520\n",
      "Epoch [521/600], Train Loss: 0.0045, Gradient Norm: 0.02\n",
      "Epoch [521/600], Validation Loss: 0.0106\n",
      "Epoch [522/600], Train Loss: 0.0044, Gradient Norm: 0.11\n",
      "Epoch [522/600], Validation Loss: 0.0090\n",
      "Epoch [523/600], Train Loss: 0.0048, Gradient Norm: 0.07\n",
      "Epoch [523/600], Validation Loss: 0.0084\n",
      "Epoch [524/600], Train Loss: 0.0047, Gradient Norm: 0.04\n",
      "Epoch [524/600], Validation Loss: 0.0080\n",
      "Epoch [525/600], Train Loss: 0.0049, Gradient Norm: 0.09\n",
      "Epoch [525/600], Validation Loss: 0.0085\n",
      "Epoch [526/600], Train Loss: 0.0048, Gradient Norm: 0.07\n",
      "Epoch [526/600], Validation Loss: 0.0089\n",
      "Epoch [527/600], Train Loss: 0.0049, Gradient Norm: 0.08\n",
      "Epoch [527/600], Validation Loss: 0.0088\n",
      "Epoch [528/600], Train Loss: 0.0049, Gradient Norm: 0.15\n",
      "Epoch [528/600], Validation Loss: 0.0087\n",
      "Epoch [529/600], Train Loss: 0.0047, Gradient Norm: 0.20\n",
      "Epoch [529/600], Validation Loss: 0.0078\n",
      "Epoch [530/600], Train Loss: 0.0046, Gradient Norm: 0.20\n",
      "Epoch [530/600], Validation Loss: 0.0090\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_530.png\n",
      "Model saved at epoch 530\n",
      "Epoch [531/600], Train Loss: 0.0048, Gradient Norm: 0.17\n",
      "Epoch [531/600], Validation Loss: 0.0086\n",
      "Epoch [532/600], Train Loss: 0.0049, Gradient Norm: 0.06\n",
      "Epoch [532/600], Validation Loss: 0.0076\n",
      "Epoch [533/600], Train Loss: 0.0044, Gradient Norm: 0.11\n",
      "Epoch [533/600], Validation Loss: 0.0094\n",
      "Epoch [534/600], Train Loss: 0.0044, Gradient Norm: 0.07\n",
      "Epoch [534/600], Validation Loss: 0.0087\n",
      "Epoch [535/600], Train Loss: 0.0045, Gradient Norm: 0.10\n",
      "Epoch [535/600], Validation Loss: 0.0090\n",
      "Epoch [536/600], Train Loss: 0.0047, Gradient Norm: 0.12\n",
      "Epoch [536/600], Validation Loss: 0.0090\n",
      "Epoch [537/600], Train Loss: 0.0048, Gradient Norm: 0.18\n",
      "Epoch [537/600], Validation Loss: 0.0085\n",
      "Epoch [538/600], Train Loss: 0.0046, Gradient Norm: 0.15\n",
      "Epoch [538/600], Validation Loss: 0.0089\n",
      "Epoch [539/600], Train Loss: 0.0047, Gradient Norm: 0.03\n",
      "Epoch [539/600], Validation Loss: 0.0082\n",
      "Epoch [540/600], Train Loss: 0.0045, Gradient Norm: 0.04\n",
      "Epoch [540/600], Validation Loss: 0.0070\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_540.png\n",
      "Model saved at epoch 540\n",
      "Epoch [541/600], Train Loss: 0.0047, Gradient Norm: 0.06\n",
      "Epoch [541/600], Validation Loss: 0.0078\n",
      "Epoch [542/600], Train Loss: 0.0046, Gradient Norm: 0.06\n",
      "Epoch [542/600], Validation Loss: 0.0085\n",
      "Epoch [543/600], Train Loss: 0.0045, Gradient Norm: 0.07\n",
      "Epoch [543/600], Validation Loss: 0.0085\n",
      "Epoch [544/600], Train Loss: 0.0044, Gradient Norm: 0.09\n",
      "Epoch [544/600], Validation Loss: 0.0090\n",
      "Epoch [545/600], Train Loss: 0.0044, Gradient Norm: 0.24\n",
      "Epoch [545/600], Validation Loss: 0.0088\n",
      "Epoch [546/600], Train Loss: 0.0047, Gradient Norm: 0.18\n",
      "Epoch [546/600], Validation Loss: 0.0084\n",
      "Epoch [547/600], Train Loss: 0.0046, Gradient Norm: 0.18\n",
      "Epoch [547/600], Validation Loss: 0.0088\n",
      "Epoch [548/600], Train Loss: 0.0045, Gradient Norm: 0.03\n",
      "Epoch [548/600], Validation Loss: 0.0090\n",
      "Epoch [549/600], Train Loss: 0.0045, Gradient Norm: 0.24\n",
      "Epoch [549/600], Validation Loss: 0.0081\n",
      "Epoch [550/600], Train Loss: 0.0048, Gradient Norm: 0.32\n",
      "Epoch [550/600], Validation Loss: 0.0088\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_550.png\n",
      "Model saved at epoch 550\n",
      "Epoch [551/600], Train Loss: 0.0046, Gradient Norm: 0.13\n",
      "Epoch [551/600], Validation Loss: 0.0088\n",
      "Epoch [552/600], Train Loss: 0.0046, Gradient Norm: 0.20\n",
      "Epoch [552/600], Validation Loss: 0.0090\n",
      "Epoch [553/600], Train Loss: 0.0047, Gradient Norm: 0.05\n",
      "Epoch [553/600], Validation Loss: 0.0076\n",
      "Epoch [554/600], Train Loss: 0.0044, Gradient Norm: 0.11\n",
      "Epoch [554/600], Validation Loss: 0.0085\n",
      "Epoch [555/600], Train Loss: 0.0048, Gradient Norm: 0.11\n",
      "Epoch [555/600], Validation Loss: 0.0083\n",
      "Epoch [556/600], Train Loss: 0.0045, Gradient Norm: 0.02\n",
      "Epoch [556/600], Validation Loss: 0.0087\n",
      "Epoch [557/600], Train Loss: 0.0045, Gradient Norm: 0.06\n",
      "Epoch [557/600], Validation Loss: 0.0078\n",
      "Epoch [558/600], Train Loss: 0.0046, Gradient Norm: 0.04\n",
      "Epoch [558/600], Validation Loss: 0.0086\n",
      "Epoch [559/600], Train Loss: 0.0046, Gradient Norm: 0.16\n",
      "Epoch [559/600], Validation Loss: 0.0092\n",
      "Epoch [560/600], Train Loss: 0.0047, Gradient Norm: 0.13\n",
      "Epoch [560/600], Validation Loss: 0.0100\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_560.png\n",
      "Model saved at epoch 560\n",
      "Epoch [561/600], Train Loss: 0.0047, Gradient Norm: 0.17\n",
      "Epoch [561/600], Validation Loss: 0.0091\n",
      "Epoch [562/600], Train Loss: 0.0046, Gradient Norm: 0.04\n",
      "Epoch [562/600], Validation Loss: 0.0086\n",
      "Epoch [563/600], Train Loss: 0.0045, Gradient Norm: 0.04\n",
      "Epoch [563/600], Validation Loss: 0.0091\n",
      "Epoch [564/600], Train Loss: 0.0046, Gradient Norm: 0.04\n",
      "Epoch [564/600], Validation Loss: 0.0084\n",
      "Epoch [565/600], Train Loss: 0.0045, Gradient Norm: 0.13\n",
      "Epoch [565/600], Validation Loss: 0.0080\n",
      "Epoch [566/600], Train Loss: 0.0046, Gradient Norm: 0.03\n",
      "Epoch [566/600], Validation Loss: 0.0083\n",
      "Epoch [567/600], Train Loss: 0.0046, Gradient Norm: 0.12\n",
      "Epoch [567/600], Validation Loss: 0.0075\n",
      "Epoch [568/600], Train Loss: 0.0046, Gradient Norm: 0.07\n",
      "Epoch [568/600], Validation Loss: 0.0082\n",
      "Epoch [569/600], Train Loss: 0.0045, Gradient Norm: 0.06\n",
      "Epoch [569/600], Validation Loss: 0.0087\n",
      "Epoch [570/600], Train Loss: 0.0045, Gradient Norm: 0.15\n",
      "Epoch [570/600], Validation Loss: 0.0091\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_570.png\n",
      "Model saved at epoch 570\n",
      "Epoch [571/600], Train Loss: 0.0045, Gradient Norm: 0.18\n",
      "Epoch [571/600], Validation Loss: 0.0086\n",
      "Epoch [572/600], Train Loss: 0.0045, Gradient Norm: 0.09\n",
      "Epoch [572/600], Validation Loss: 0.0084\n",
      "Epoch [573/600], Train Loss: 0.0046, Gradient Norm: 0.06\n",
      "Epoch [573/600], Validation Loss: 0.0071\n",
      "Epoch [574/600], Train Loss: 0.0045, Gradient Norm: 0.04\n",
      "Epoch [574/600], Validation Loss: 0.0083\n",
      "Epoch [575/600], Train Loss: 0.0046, Gradient Norm: 0.09\n",
      "Epoch [575/600], Validation Loss: 0.0085\n",
      "Epoch [576/600], Train Loss: 0.0047, Gradient Norm: 0.11\n",
      "Epoch [576/600], Validation Loss: 0.0077\n",
      "Epoch [577/600], Train Loss: 0.0044, Gradient Norm: 0.12\n",
      "Epoch [577/600], Validation Loss: 0.0078\n",
      "Epoch [578/600], Train Loss: 0.0047, Gradient Norm: 0.06\n",
      "Epoch [578/600], Validation Loss: 0.0087\n",
      "Epoch [579/600], Train Loss: 0.0048, Gradient Norm: 0.02\n",
      "Epoch [579/600], Validation Loss: 0.0084\n",
      "Epoch [580/600], Train Loss: 0.0043, Gradient Norm: 0.10\n",
      "Epoch [580/600], Validation Loss: 0.0085\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_580.png\n",
      "Model saved at epoch 580\n",
      "Epoch [581/600], Train Loss: 0.0046, Gradient Norm: 0.06\n",
      "Epoch [581/600], Validation Loss: 0.0085\n",
      "Epoch [582/600], Train Loss: 0.0044, Gradient Norm: 0.13\n",
      "Epoch [582/600], Validation Loss: 0.0078\n",
      "Epoch [583/600], Train Loss: 0.0046, Gradient Norm: 0.03\n",
      "Epoch [583/600], Validation Loss: 0.0090\n",
      "Epoch [584/600], Train Loss: 0.0045, Gradient Norm: 0.11\n",
      "Epoch [584/600], Validation Loss: 0.0089\n",
      "Epoch [585/600], Train Loss: 0.0045, Gradient Norm: 0.07\n",
      "Epoch [585/600], Validation Loss: 0.0090\n",
      "Epoch [586/600], Train Loss: 0.0048, Gradient Norm: 0.04\n",
      "Epoch [586/600], Validation Loss: 0.0092\n",
      "Epoch [587/600], Train Loss: 0.0046, Gradient Norm: 0.11\n",
      "Epoch [587/600], Validation Loss: 0.0087\n",
      "Epoch [588/600], Train Loss: 0.0046, Gradient Norm: 0.03\n",
      "Epoch [588/600], Validation Loss: 0.0085\n",
      "Epoch [589/600], Train Loss: 0.0045, Gradient Norm: 0.18\n",
      "Epoch [589/600], Validation Loss: 0.0079\n",
      "Epoch [590/600], Train Loss: 0.0047, Gradient Norm: 0.04\n",
      "Epoch [590/600], Validation Loss: 0.0100\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_590.png\n",
      "Model saved at epoch 590\n",
      "Epoch [591/600], Train Loss: 0.0044, Gradient Norm: 0.15\n",
      "Epoch [591/600], Validation Loss: 0.0087\n",
      "Epoch [592/600], Train Loss: 0.0046, Gradient Norm: 0.23\n",
      "Epoch [592/600], Validation Loss: 0.0101\n",
      "Epoch [593/600], Train Loss: 0.0047, Gradient Norm: 0.05\n",
      "Epoch [593/600], Validation Loss: 0.0087\n",
      "Epoch [594/600], Train Loss: 0.0045, Gradient Norm: 0.15\n",
      "Epoch [594/600], Validation Loss: 0.0086\n",
      "Epoch [595/600], Train Loss: 0.0049, Gradient Norm: 0.14\n",
      "Epoch [595/600], Validation Loss: 0.0091\n",
      "Epoch [596/600], Train Loss: 0.0045, Gradient Norm: 0.30\n",
      "Epoch [596/600], Validation Loss: 0.0080\n",
      "Epoch [597/600], Train Loss: 0.0046, Gradient Norm: 0.30\n",
      "Epoch [597/600], Validation Loss: 0.0088\n",
      "Epoch [598/600], Train Loss: 0.0045, Gradient Norm: 0.18\n",
      "Epoch [598/600], Validation Loss: 0.0079\n",
      "Epoch [599/600], Train Loss: 0.0048, Gradient Norm: 0.16\n",
      "Epoch [599/600], Validation Loss: 0.0088\n",
      "Epoch [600/600], Train Loss: 0.0045, Gradient Norm: 0.09\n",
      "Epoch [600/600], Validation Loss: 0.0081\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint.pth\n",
      "Image saved at generated_images/training/lol/generated_image_epoch_600.png\n",
      "Model saved at epoch 600\n",
      "Model checkpoint saved at swinddpm64MRnet_checkpoint_600.pth\n",
      "Special checkpoint saved at epoch 600\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DDPM model\n",
    "in_channels = 1  # For grayscale images\n",
    "out_channels = 1  # For grayscale images\n",
    "emb_dim = 128\n",
    "num_timesteps = 2000\n",
    "latent_dim = 128\n",
    "\n",
    "unet = AttentionUNet(in_channels, out_channels, emb_dim).to(device)\n",
    "ddpm = DDPM(unet, num_timesteps, latent_dim).to(device)\n",
    "\n",
    "# Train the model\n",
    "train_diffusion_model(ddpm, train_loader, valid_loader, epochs=600, save_interval=10, checkpoint_path=\"Model_Savepoints/ddpm_checkpoint.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc20e226",
   "metadata": {},
   "source": [
    "##### 9. Generating Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdbc68be-e879-42f7-9a63-b283231e58f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 images saved in a table format at generated_images/epoch600/generated_images_table.png\n"
     ]
    }
   ],
   "source": [
    "# Function to generate and save images after training\n",
    "def generate_and_save_images_post_training(ddpm, num_images=10, save_dir='generated_images/DDPM_images'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    sample_shape = (1, 1, 64, 64)  # Generate 1 image at a time, 1 channel, 64x64 images\n",
    "\n",
    "    # Create a figure to plot images\n",
    "    fig, axes = plt.subplots(10, 5, figsize=(15, 30))  # Adjust the layout for 10 images in a 2x5 grid\n",
    "\n",
    "    ddpm.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_images):\n",
    "            samples = ddpm.sample(sample_shape)\n",
    "            # Convert to numpy\n",
    "            samples = samples.cpu().numpy().squeeze()\n",
    "            \n",
    "            # Plot the image in the grid\n",
    "            ax = axes[i // 5, i % 5]\n",
    "            ax.imshow(samples, cmap='gray')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    # Save the figure\n",
    "    save_path = os.path.join(save_dir, 'generated_images_table.png')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f'{num_images} images saved in a table format at {save_path}')\n",
    "\n",
    "# Load the trained model\n",
    "checkpoint_path = \"Model_Savepoints/ddpm_checkpoint_600.pth\" \n",
    "ddpm, start_epoch = load_model(ddpm, checkpoint_path)\n",
    "\n",
    "# Generate and save images after training\n",
    "generate_and_save_images_post_training(ddpm, num_images=50, save_dir='generated_images/epoch600')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
