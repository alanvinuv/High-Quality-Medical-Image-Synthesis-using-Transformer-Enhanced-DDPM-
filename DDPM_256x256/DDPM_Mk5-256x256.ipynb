{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b922c6",
   "metadata": {},
   "source": [
    "# Training a Denoising Diffusion Probabilistic Model (DDPM) on 256x256 MRNet Images with Swin Transformer Integration\n",
    "\n",
    "This notebook demonstrates the implementation and training of a Denoising Diffusion Probabilistic Model (DDPM) on MRI images from the MRNet dataset at 256x256 image resolution. The core of this implementation integrates Swin Transformer blocks within a UNet architecture to enhance the model's ability to capture complex image details at different scales, leveraging the self-attention mechanism within local windows of the image.\n",
    "\n",
    "The workflow includes:\n",
    "1. **Data Preprocessing**: Loading and transforming the MRI slices into a format suitable for training.\n",
    "2. **Model Architecture**: Building a UNet-based architecture augmented with Swin Transformer blocks and sinusoidal positional embeddings.\n",
    "3. **Training Process**: Implementing the DDPM training loop, including loss calculation, gradient checks, and saving intermediate model checkpoints.\n",
    "4. **Image Generation**: After training, the model is used to generate synthetic MRI slices, which are saved for further analysis and comparison.\n",
    "\n",
    "This approach aims to evaluate the direct application of DDPM on high-resolution medical images, with a focus on comparing its performance against a cascaded super-resolution DDPM model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51232ac7",
   "metadata": {},
   "source": [
    "##### 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1675cbed-a2b9-4977-acb2-47bc55388704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517c370b",
   "metadata": {},
   "source": [
    "##### 2. Applying Transforms and Initializing Dataset Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf403831-137a-4d53-b841-25ece3fab3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "# Define transformations to preprocess the MRI images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Convert images to grayscale\n",
    "    transforms.Resize((64, 64)),  # Resize images to 64x64\n",
    "    transforms.ToTensor()  # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "class MRNetSliceDataset(Dataset):\n",
    "    def __init__(self, slice_dir, label_files, transform=None):\n",
    "        super().__init__()\n",
    "        self.slice_dir = slice_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Dictionary to store labels for each image ID\n",
    "        self.labels_dict = {}\n",
    "        for label_file in label_files:\n",
    "            records = pd.read_csv(label_file, header=None, names=['id', 'label'])\n",
    "            records['id'] = records['id'].map(lambda i: '0' * (4 - len(str(i))) + str(i))\n",
    "            self.labels_dict.update(dict(zip(records['id'], records['label'])))\n",
    "\n",
    "        # List all slice files in the directory\n",
    "        self.slice_files = [os.path.join(slice_dir, fname) for fname in os.listdir(slice_dir) if fname.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        slice_path = self.slice_files[index]\n",
    "        image = Image.open(slice_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Extract ID from the filename to find the corresponding label\n",
    "        slice_id = os.path.basename(slice_path).split('_')[1]\n",
    "\n",
    "        if slice_id in self.labels_dict:\n",
    "            label = self.labels_dict[slice_id]\n",
    "            label = torch.FloatTensor([label])\n",
    "        else:\n",
    "            print(f\"Label for ID {slice_id} not found in the CSV file.\")\n",
    "            label = torch.FloatTensor([0])  # Default label or handle missing labels as needed\n",
    "\n",
    "        return {'data': image, 'label': label, 'id': slice_id}\n",
    "\n",
    "# Initialize training and validation datasets and data loaders\n",
    "root_dir = \"Raw_Images\"\n",
    "train_slice_dir = os.path.join(root_dir, \"train_slices_raw\")\n",
    "valid_slice_dir = os.path.join(root_dir, \"valid_slices_raw\")\n",
    "\n",
    "train_label_files = [\n",
    "    os.path.join(root_dir, \"train-acl.csv\"),\n",
    "    os.path.join(root_dir, \"train-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"train-meniscus.csv\")\n",
    "]\n",
    "\n",
    "valid_label_files = [\n",
    "    os.path.join(root_dir, \"valid-acl.csv\"),\n",
    "    os.path.join(root_dir, \"valid-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"valid-meniscus.csv\")\n",
    "]\n",
    "\n",
    "train_dataset = MRNetSliceDataset(slice_dir=train_slice_dir, label_files=train_label_files, transform=transform)\n",
    "valid_dataset = MRNetSliceDataset(slice_dir=valid_slice_dir, label_files=valid_label_files, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978d9cfb",
   "metadata": {},
   "source": [
    "##### 3. Implementing Swin Transformer and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26b0a914-c38c-47c8-8e8b-704433cb5b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "<https://arxiv.org/abs/2103.14030>\n",
    "https://github.com/microsoft/Swin-Transformer\n",
    "\"\"\"\n",
    "\n",
    "# DropPath (Stochastic Depth) module to implement drop path regularization\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "        return output\n",
    "\n",
    "# Helper functions to handle tuple and truncation\n",
    "def to_2tuple(x):\n",
    "    if isinstance(x, (tuple, list)):\n",
    "        return x\n",
    "    return (x, x)\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1.):\n",
    "    with torch.no_grad():\n",
    "        size = tensor.shape\n",
    "        tmp = tensor.new_empty(size + (4,)).normal_()\n",
    "        valid = (tmp < 2) & (tmp > -2)\n",
    "        ind = valid.max(-1, keepdim=True)[1]\n",
    "        tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
    "        tensor.data.mul_(std).add_(mean)\n",
    "        return tensor\n",
    "\n",
    "# MLP module used within Swin Transformer\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "# Functions to partition and reverse windows in the Swin Transformer\n",
    "def window_partition(x, window_size):\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.view(B, C, H // window_size, window_size, W // window_size, window_size)\n",
    "    windows = x.permute(0, 2, 4, 3, 5, 1).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 5, 1, 3, 2, 4).contiguous().view(B, -1, H, W)\n",
    "    return x\n",
    "\n",
    "# Window-based multi-head self-attention (W-MSA) module\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # Relative position bias table for all windows\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n",
    "\n",
    "        # Get relative position index for each window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1)\n",
    "        attn = attn + relative_position_bias.unsqueeze(0).to(attn.dtype)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "# Swin Transformer block implementing the shifted window-based attention mechanism\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * self.mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, out_features=dim, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            attn_mask = self.calculate_mask(self.input_resolution)\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def calculate_mask(self, x_size):\n",
    "        H, W = x_size\n",
    "        img_mask = torch.zeros((1, H, W, 1))\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size).view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        return attn_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, C = x.shape\n",
    "        H, W = self.input_resolution\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        x_windows = window_partition(shifted_x, self.window_size)\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
    "\n",
    "        if self.attn_mask is not None:\n",
    "            attn_windows = self.attn(x_windows, mask=self.attn_mask.to(x.dtype))\n",
    "        else:\n",
    "            attn_windows = self.attn(x_windows)\n",
    "\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dc26c1",
   "metadata": {},
   "source": [
    "##### 4. Implementing the Attention UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a10fab5-00ea-42bd-9e38-468a4bd32d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sinusoidal positional embedding for timestep encoding in DDPM\n",
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=10000):\n",
    "        super(SinusoidalPositionalEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, timesteps):\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        emb = math.log(self.max_len) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n",
    "        emb = timesteps[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "        if self.embedding_dim % 2 == 1:\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=1)\n",
    "        return emb\n",
    "\n",
    "# UNet with SWIN Transformer in bottleneck\n",
    "class AttentionUNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=128):\n",
    "        super(AttentionUNet, self).__init__()\n",
    "\n",
    "        self.encoder1 = self.conv_block(in_channels + emb_dim, 64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "        self.swin_block = SwinTransformerBlock(dim=512, input_resolution=(32, 32), num_heads=8, window_size=4) \n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self.conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.conv_block(256, 128)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self.conv_block(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "        self.timestep_embedding_layer = SinusoidalPositionalEmbedding(emb_dim)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.timestep_embedding_layer(t)\n",
    "        t_embed = t_embed.view(t.size(0), -1, 1, 1)\n",
    "        t_embed = t_embed.repeat(1, 1, x.size(2), x.size(3))\n",
    "        x = torch.cat((x, t_embed), dim=1)\n",
    "\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc1_pooled = self.pool1(enc1)\n",
    "\n",
    "        enc2 = self.encoder2(enc1_pooled)\n",
    "        enc2_pooled = self.pool2(enc2)\n",
    "\n",
    "        enc3 = self.encoder3(enc2_pooled)\n",
    "        enc3_pooled = self.pool3(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc3_pooled)\n",
    "        B, C, H, W = bottleneck.shape\n",
    "        bottleneck = bottleneck.view(B, H * W, C)\n",
    "        bottleneck = self.swin_block(bottleneck)\n",
    "        bottleneck = bottleneck.view(B, C, H, W)\n",
    "\n",
    "        # Decoder\n",
    "        upconv3 = self.upconv3(bottleneck)\n",
    "        dec3 = torch.cat((upconv3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        upconv2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((upconv2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        upconv1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((upconv1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        final_output = self.final_conv(dec1)\n",
    "        return final_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d473c8c1",
   "metadata": {},
   "source": [
    "##### 5. Implementing the DDPM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1778e06d-1c6b-43dc-9c7f-2f314c25a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DDPM implementation adapted from:\n",
    "https://github.com/hojonathanho/diffusion/tree/master\n",
    "\"\"\"\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, model, num_timesteps, latent_dim, beta_start=0.00085, beta_end=0.0120):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.model = model\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', 1 - betas)\n",
    "        self.register_buffer('alphas_cumprod', torch.cumprod(1 - betas, dim=0))\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(self.alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - self.alphas_cumprod))\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def forward(self, z_t, t):\n",
    "        return self.model(z_t, t)\n",
    "\n",
    "    def sample_timesteps(self, batch_size):\n",
    "        return torch.randint(0, self.num_timesteps, (batch_size,)).to(device)\n",
    "\n",
    "    def forward_diffusion(self, z_0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(z_0)\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        return sqrt_alphas_cumprod_t * z_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_losses(self, z_0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(z_0)\n",
    "        z_t = self.forward_diffusion(z_0, t, noise)\n",
    "        predicted_noise = self.forward(z_t, t)\n",
    "        return nn.MSELoss()(noise, predicted_noise)\n",
    "\n",
    "    def sample(self, shape):\n",
    "        z_t = torch.randn(shape).to(device)\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "            t_tensor = torch.tensor([t], device=z_t.device).long()\n",
    "            alpha_t = self.alphas[t]\n",
    "            sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "            sqrt_one_minus_alpha_t = torch.sqrt(1 - self.alphas_cumprod[t])\n",
    "            beta_t = self.betas[t]\n",
    "\n",
    "            # Predict the noise\n",
    "            predicted_noise = self.forward(z_t, t_tensor)\n",
    "\n",
    "            # Remove the predicted noise\n",
    "            z_t = (z_t - beta_t / sqrt_one_minus_alpha_t * predicted_noise) / sqrt_alpha_t\n",
    "\n",
    "            # Add noise for non-final steps\n",
    "            if t > 0:\n",
    "                z_t += torch.randn_like(z_t) * torch.sqrt(beta_t)\n",
    "\n",
    "        return z_t\n",
    "\n",
    "    def p_sample(self, z, t):\n",
    "        predicted_noise = self.forward(z, t)\n",
    "        alpha_t = self.alphas[t]\n",
    "        sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "        sqrt_one_minus_alpha_t = torch.sqrt(1 - alpha_t)\n",
    "        z = (z - predicted_noise * (1 - alpha_t) / sqrt_one_minus_alpha_t) / sqrt_alpha_t\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c67104",
   "metadata": {},
   "source": [
    "##### 6. Functions for Training and Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f81216a-5b1a-4610-a564-99ce3e130220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "def save_model(ddpm, epoch, checkpoint_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': ddpm.state_dict()\n",
    "    }, checkpoint_path)\n",
    "    print(f'Model checkpoint saved at {checkpoint_path}')\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(ddpm, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    ddpm.load_state_dict(checkpoint['model_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    return ddpm, start_epoch\n",
    "\n",
    "# Function to generate and save images\n",
    "def generate_and_save_images(ddpm, epoch, save_dir='generated_images/training'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    latent_dim = 128\n",
    "    shape = (1, 1, 256, 256)  # Change to 1 channel if using grayscale images\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    ddpm.eval()\n",
    "    with torch.no_grad():\n",
    "        # Sample from the DDPM in smaller batches to avoid memory issues\n",
    "        samples = ddpm.sample(shape)\n",
    "\n",
    "        # Convert to numpy and save images\n",
    "        samples = samples.squeeze().cpu().detach().numpy()\n",
    "        samples = (samples * 255).astype(np.uint8)\n",
    "        save_path = os.path.join(save_dir, f'generated_image_epoch_{epoch+1}.png')\n",
    "        Image.fromarray(samples, mode='L').save(save_path)\n",
    "        print(f'Image saved at {save_path}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a95201c",
   "metadata": {},
   "source": [
    "##### 7. Training routine for the Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ec93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the diffusion model\n",
    "def train_diffusion_model(ddpm, train_loader, valid_loader, epochs=10, save_interval=10, checkpoint_path='swinddpm__checkpoint.pth'):\n",
    "    optimizer = optim.AdamW(ddpm.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, verbose=True)\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Check if a checkpoint exists\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        ddpm, start_epoch = load_model(ddpm, checkpoint_path)\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "        print(f\"Resuming training from epoch {start_epoch+1}\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}, starting from scratch.\")\n",
    "\n",
    "    ddpm.to(device)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        ddpm.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs = batch['data'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            noise = torch.randn_like(inputs).to(device)\n",
    "            t = ddpm.sample_timesteps(inputs.size(0))\n",
    "            loss = ddpm.p_losses(inputs, t, noise)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient checking\n",
    "            total_norm = 0\n",
    "            for p in ddpm.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "\n",
    "            if total_norm > 1e3:  # Threshold for exploding gradients\n",
    "                print(f\"Warning: Exploding gradients detected at epoch {epoch+1}, total norm: {total_norm:.2f}\")\n",
    "                continue\n",
    "            if total_norm < 1e-3:  # Threshold for vanishing gradients\n",
    "                print(f\"Warning: Vanishing gradients detected at epoch {epoch+1}, total norm: {total_norm:.2f}\")\n",
    "                continue\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Gradient Norm: {total_norm:.2f}')\n",
    "\n",
    "        ddpm.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                inputs = batch['data'].to(device)\n",
    "                noise = torch.randn_like(inputs).to(device)\n",
    "                t = ddpm.sample_timesteps(inputs.size(0))\n",
    "                loss = ddpm.p_losses(inputs, t, noise)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        avg_valid_loss = valid_loss / len(valid_loader)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_valid_loss:.4f}')\n",
    "\n",
    "         # Learning Rate Scheduler\n",
    "        scheduler.step(avg_valid_loss)\n",
    "        \n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            save_model(ddpm, epoch, checkpoint_path)\n",
    "            generate_and_save_images(ddpm, epoch)\n",
    "            print(f'Model saved at epoch {epoch+1}')\n",
    "\n",
    "        # Special checkpoint save\n",
    "        # if (epoch + 1) % 200 == 0:\n",
    "        #     special_checkpoint_path = checkpoint_path.replace(\".pth\", f\"_{epoch+1}.pth\")\n",
    "        #     save_model(ddpm, epoch, special_checkpoint_path)\n",
    "        #     print(f'Special checkpoint saved at epoch {epoch+1}')\n",
    "            \n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f68f9da",
   "metadata": {},
   "source": [
    "##### 8. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a9ddc2f-4a6f-4872-8207-108ad49a3649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found at swinddpm256_MRNet_checkpoint.pth, starting from scratch.\n",
      "Epoch [1/200], Train Loss: 0.2033, Gradient Norm: 0.62\n",
      "Epoch [1/200], Validation Loss: 0.0344\n",
      "Epoch [2/200], Train Loss: 0.0179, Gradient Norm: 0.60\n",
      "Epoch [2/200], Validation Loss: 0.0151\n",
      "Epoch [3/200], Train Loss: 0.0111, Gradient Norm: 2.05\n",
      "Epoch [3/200], Validation Loss: 0.0167\n",
      "Epoch [4/200], Train Loss: 0.0104, Gradient Norm: 0.41\n",
      "Epoch [4/200], Validation Loss: 0.0111\n",
      "Epoch [5/200], Train Loss: 0.0087, Gradient Norm: 0.46\n",
      "Epoch [5/200], Validation Loss: 0.0096\n",
      "Epoch [6/200], Train Loss: 0.0085, Gradient Norm: 0.05\n",
      "Epoch [6/200], Validation Loss: 0.0078\n",
      "Epoch [7/200], Train Loss: 0.0074, Gradient Norm: 0.86\n",
      "Epoch [7/200], Validation Loss: 0.0102\n",
      "Epoch [8/200], Train Loss: 0.0072, Gradient Norm: 0.46\n",
      "Epoch [8/200], Validation Loss: 0.0081\n",
      "Epoch [9/200], Train Loss: 0.0072, Gradient Norm: 0.42\n",
      "Epoch [9/200], Validation Loss: 0.0071\n",
      "Epoch [10/200], Train Loss: 0.0064, Gradient Norm: 0.19\n",
      "Epoch [10/200], Validation Loss: 0.0062\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_10.png\n",
      "Model saved at epoch 10\n",
      "Epoch [11/200], Train Loss: 0.0075, Gradient Norm: 0.57\n",
      "Epoch [11/200], Validation Loss: 0.0088\n",
      "Epoch [12/200], Train Loss: 0.0072, Gradient Norm: 0.06\n",
      "Epoch [12/200], Validation Loss: 0.0073\n",
      "Epoch [13/200], Train Loss: 0.0066, Gradient Norm: 0.29\n",
      "Epoch [13/200], Validation Loss: 0.0067\n",
      "Epoch [14/200], Train Loss: 0.0070, Gradient Norm: 2.28\n",
      "Epoch [14/200], Validation Loss: 0.0101\n",
      "Epoch [15/200], Train Loss: 0.0075, Gradient Norm: 0.12\n",
      "Epoch [15/200], Validation Loss: 0.0055\n",
      "Epoch [16/200], Train Loss: 0.0056, Gradient Norm: 0.40\n",
      "Epoch [16/200], Validation Loss: 0.0064\n",
      "Epoch [17/200], Train Loss: 0.0066, Gradient Norm: 0.15\n",
      "Epoch [17/200], Validation Loss: 0.0064\n",
      "Epoch [18/200], Train Loss: 0.0058, Gradient Norm: 0.35\n",
      "Epoch [18/200], Validation Loss: 0.0067\n",
      "Epoch [19/200], Train Loss: 0.0066, Gradient Norm: 0.04\n",
      "Epoch [19/200], Validation Loss: 0.0053\n",
      "Epoch [20/200], Train Loss: 0.0062, Gradient Norm: 0.30\n",
      "Epoch [20/200], Validation Loss: 0.0061\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_20.png\n",
      "Model saved at epoch 20\n",
      "Epoch [21/200], Train Loss: 0.0058, Gradient Norm: 0.21\n",
      "Epoch [21/200], Validation Loss: 0.0067\n",
      "Epoch [22/200], Train Loss: 0.0061, Gradient Norm: 0.27\n",
      "Epoch [22/200], Validation Loss: 0.0052\n",
      "Epoch [23/200], Train Loss: 0.0062, Gradient Norm: 0.26\n",
      "Epoch [23/200], Validation Loss: 0.0051\n",
      "Epoch [24/200], Train Loss: 0.0053, Gradient Norm: 0.28\n",
      "Epoch [24/200], Validation Loss: 0.0057\n",
      "Epoch [25/200], Train Loss: 0.0053, Gradient Norm: 0.22\n",
      "Epoch [25/200], Validation Loss: 0.0059\n",
      "Epoch [26/200], Train Loss: 0.0061, Gradient Norm: 0.48\n",
      "Epoch [26/200], Validation Loss: 0.0062\n",
      "Epoch [27/200], Train Loss: 0.0057, Gradient Norm: 0.22\n",
      "Epoch [27/200], Validation Loss: 0.0052\n",
      "Epoch [28/200], Train Loss: 0.0063, Gradient Norm: 0.27\n",
      "Epoch [28/200], Validation Loss: 0.0060\n",
      "Epoch [29/200], Train Loss: 0.0061, Gradient Norm: 0.27\n",
      "Epoch [29/200], Validation Loss: 0.0056\n",
      "Epoch [30/200], Train Loss: 0.0058, Gradient Norm: 0.57\n",
      "Epoch [30/200], Validation Loss: 0.0065\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_30.png\n",
      "Model saved at epoch 30\n",
      "Epoch [31/200], Train Loss: 0.0054, Gradient Norm: 0.10\n",
      "Epoch [31/200], Validation Loss: 0.0055\n",
      "Epoch [32/200], Train Loss: 0.0053, Gradient Norm: 0.02\n",
      "Epoch [32/200], Validation Loss: 0.0055\n",
      "Epoch [33/200], Train Loss: 0.0056, Gradient Norm: 0.09\n",
      "Epoch [33/200], Validation Loss: 0.0053\n",
      "Epoch [34/200], Train Loss: 0.0052, Gradient Norm: 0.12\n",
      "Epoch [34/200], Validation Loss: 0.0064\n",
      "Epoch [35/200], Train Loss: 0.0061, Gradient Norm: 0.37\n",
      "Epoch [35/200], Validation Loss: 0.0062\n",
      "Epoch [36/200], Train Loss: 0.0053, Gradient Norm: 0.04\n",
      "Epoch [36/200], Validation Loss: 0.0050\n",
      "Epoch [37/200], Train Loss: 0.0056, Gradient Norm: 0.57\n",
      "Epoch [37/200], Validation Loss: 0.0050\n",
      "Epoch [38/200], Train Loss: 0.0060, Gradient Norm: 0.08\n",
      "Epoch [38/200], Validation Loss: 0.0055\n",
      "Epoch [39/200], Train Loss: 0.0057, Gradient Norm: 0.02\n",
      "Epoch [39/200], Validation Loss: 0.0055\n",
      "Epoch [40/200], Train Loss: 0.0051, Gradient Norm: 0.02\n",
      "Epoch [40/200], Validation Loss: 0.0063\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_40.png\n",
      "Model saved at epoch 40\n",
      "Epoch [41/200], Train Loss: 0.0050, Gradient Norm: 0.11\n",
      "Epoch [41/200], Validation Loss: 0.0056\n",
      "Epoch [42/200], Train Loss: 0.0059, Gradient Norm: 0.36\n",
      "Epoch [42/200], Validation Loss: 0.0059\n",
      "Epoch [43/200], Train Loss: 0.0058, Gradient Norm: 0.04\n",
      "Epoch [43/200], Validation Loss: 0.0056\n",
      "Epoch [44/200], Train Loss: 0.0052, Gradient Norm: 0.25\n",
      "Epoch [44/200], Validation Loss: 0.0062\n",
      "Epoch [45/200], Train Loss: 0.0050, Gradient Norm: 0.03\n",
      "Epoch [45/200], Validation Loss: 0.0053\n",
      "Epoch [46/200], Train Loss: 0.0054, Gradient Norm: 0.22\n",
      "Epoch [46/200], Validation Loss: 0.0055\n",
      "Epoch [47/200], Train Loss: 0.0052, Gradient Norm: 0.29\n",
      "Epoch [47/200], Validation Loss: 0.0056\n",
      "Epoch [48/200], Train Loss: 0.0053, Gradient Norm: 0.27\n",
      "Epoch [48/200], Validation Loss: 0.0057\n",
      "Epoch [49/200], Train Loss: 0.0055, Gradient Norm: 0.43\n",
      "Epoch [49/200], Validation Loss: 0.0056\n",
      "Epoch [50/200], Train Loss: 0.0051, Gradient Norm: 0.05\n",
      "Epoch [50/200], Validation Loss: 0.0061\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_50.png\n",
      "Model saved at epoch 50\n",
      "Epoch [51/200], Train Loss: 0.0047, Gradient Norm: 0.24\n",
      "Epoch [51/200], Validation Loss: 0.0041\n",
      "Epoch [52/200], Train Loss: 0.0056, Gradient Norm: 0.06\n",
      "Epoch [52/200], Validation Loss: 0.0058\n",
      "Epoch [53/200], Train Loss: 0.0055, Gradient Norm: 0.17\n",
      "Epoch [53/200], Validation Loss: 0.0059\n",
      "Epoch [54/200], Train Loss: 0.0055, Gradient Norm: 0.19\n",
      "Epoch [54/200], Validation Loss: 0.0052\n",
      "Epoch [55/200], Train Loss: 0.0053, Gradient Norm: 0.26\n",
      "Epoch [55/200], Validation Loss: 0.0048\n",
      "Epoch [56/200], Train Loss: 0.0054, Gradient Norm: 0.24\n",
      "Epoch [56/200], Validation Loss: 0.0056\n",
      "Epoch [57/200], Train Loss: 0.0048, Gradient Norm: 0.25\n",
      "Epoch [57/200], Validation Loss: 0.0048\n",
      "Epoch [58/200], Train Loss: 0.0055, Gradient Norm: 0.07\n",
      "Epoch [58/200], Validation Loss: 0.0046\n",
      "Epoch [59/200], Train Loss: 0.0052, Gradient Norm: 0.25\n",
      "Epoch [59/200], Validation Loss: 0.0054\n",
      "Epoch [60/200], Train Loss: 0.0049, Gradient Norm: 0.30\n",
      "Epoch [60/200], Validation Loss: 0.0046\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_60.png\n",
      "Model saved at epoch 60\n",
      "Epoch [61/200], Train Loss: 0.0053, Gradient Norm: 0.69\n",
      "Epoch [61/200], Validation Loss: 0.0109\n",
      "Epoch [62/200], Train Loss: 0.0052, Gradient Norm: 0.09\n",
      "Epoch [62/200], Validation Loss: 0.0049\n",
      "Epoch [63/200], Train Loss: 0.0049, Gradient Norm: 0.13\n",
      "Epoch [63/200], Validation Loss: 0.0054\n",
      "Epoch [64/200], Train Loss: 0.0052, Gradient Norm: 0.04\n",
      "Epoch [64/200], Validation Loss: 0.0053\n",
      "Epoch [65/200], Train Loss: 0.0052, Gradient Norm: 0.14\n",
      "Epoch [65/200], Validation Loss: 0.0041\n",
      "Epoch [66/200], Train Loss: 0.0051, Gradient Norm: 0.57\n",
      "Epoch [66/200], Validation Loss: 0.0060\n",
      "Epoch [67/200], Train Loss: 0.0053, Gradient Norm: 0.18\n",
      "Epoch [67/200], Validation Loss: 0.0055\n",
      "Epoch [68/200], Train Loss: 0.0052, Gradient Norm: 0.24\n",
      "Epoch [68/200], Validation Loss: 0.0060\n",
      "Epoch [69/200], Train Loss: 0.0058, Gradient Norm: 0.02\n",
      "Epoch [69/200], Validation Loss: 0.0048\n",
      "Epoch [70/200], Train Loss: 0.0050, Gradient Norm: 0.25\n",
      "Epoch [70/200], Validation Loss: 0.0058\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_70.png\n",
      "Model saved at epoch 70\n",
      "Epoch [71/200], Train Loss: 0.0055, Gradient Norm: 0.27\n",
      "Epoch [71/200], Validation Loss: 0.0046\n",
      "Epoch [72/200], Train Loss: 0.0047, Gradient Norm: 0.07\n",
      "Epoch [72/200], Validation Loss: 0.0053\n",
      "Epoch [73/200], Train Loss: 0.0048, Gradient Norm: 0.16\n",
      "Epoch [73/200], Validation Loss: 0.0049\n",
      "Epoch [74/200], Train Loss: 0.0049, Gradient Norm: 0.56\n",
      "Epoch [74/200], Validation Loss: 0.0046\n",
      "Epoch [75/200], Train Loss: 0.0052, Gradient Norm: 0.18\n",
      "Epoch [75/200], Validation Loss: 0.0063\n",
      "Epoch [76/200], Train Loss: 0.0042, Gradient Norm: 0.33\n",
      "Epoch [76/200], Validation Loss: 0.0053\n",
      "Epoch [77/200], Train Loss: 0.0051, Gradient Norm: 0.24\n",
      "Epoch [77/200], Validation Loss: 0.0053\n",
      "Epoch [78/200], Train Loss: 0.0048, Gradient Norm: 0.20\n",
      "Epoch [78/200], Validation Loss: 0.0047\n",
      "Epoch [79/200], Train Loss: 0.0049, Gradient Norm: 0.13\n",
      "Epoch [79/200], Validation Loss: 0.0058\n",
      "Epoch [80/200], Train Loss: 0.0048, Gradient Norm: 0.02\n",
      "Epoch [80/200], Validation Loss: 0.0055\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_80.png\n",
      "Model saved at epoch 80\n",
      "Epoch [81/200], Train Loss: 0.0050, Gradient Norm: 0.55\n",
      "Epoch [81/200], Validation Loss: 0.0047\n",
      "Epoch 00081: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch [82/200], Train Loss: 0.0050, Gradient Norm: 0.25\n",
      "Epoch [82/200], Validation Loss: 0.0047\n",
      "Epoch [83/200], Train Loss: 0.0049, Gradient Norm: 0.04\n",
      "Epoch [83/200], Validation Loss: 0.0046\n",
      "Epoch [84/200], Train Loss: 0.0045, Gradient Norm: 0.26\n",
      "Epoch [84/200], Validation Loss: 0.0051\n",
      "Epoch [85/200], Train Loss: 0.0045, Gradient Norm: 0.02\n",
      "Epoch [85/200], Validation Loss: 0.0046\n",
      "Epoch [86/200], Train Loss: 0.0046, Gradient Norm: 0.10\n",
      "Epoch [86/200], Validation Loss: 0.0052\n",
      "Epoch [87/200], Train Loss: 0.0046, Gradient Norm: 0.27\n",
      "Epoch [87/200], Validation Loss: 0.0044\n",
      "Epoch [88/200], Train Loss: 0.0049, Gradient Norm: 0.14\n",
      "Epoch [88/200], Validation Loss: 0.0044\n",
      "Epoch [89/200], Train Loss: 0.0049, Gradient Norm: 0.16\n",
      "Epoch [89/200], Validation Loss: 0.0038\n",
      "Epoch [90/200], Train Loss: 0.0047, Gradient Norm: 0.36\n",
      "Epoch [90/200], Validation Loss: 0.0058\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_90.png\n",
      "Model saved at epoch 90\n",
      "Epoch [91/200], Train Loss: 0.0047, Gradient Norm: 0.14\n",
      "Epoch [91/200], Validation Loss: 0.0049\n",
      "Epoch [92/200], Train Loss: 0.0050, Gradient Norm: 0.19\n",
      "Epoch [92/200], Validation Loss: 0.0048\n",
      "Epoch [93/200], Train Loss: 0.0048, Gradient Norm: 0.33\n",
      "Epoch [93/200], Validation Loss: 0.0043\n",
      "Epoch [94/200], Train Loss: 0.0045, Gradient Norm: 0.48\n",
      "Epoch [94/200], Validation Loss: 0.0045\n",
      "Epoch [95/200], Train Loss: 0.0044, Gradient Norm: 0.20\n",
      "Epoch [95/200], Validation Loss: 0.0044\n",
      "Epoch [96/200], Train Loss: 0.0045, Gradient Norm: 0.16\n",
      "Epoch [96/200], Validation Loss: 0.0051\n",
      "Epoch [97/200], Train Loss: 0.0047, Gradient Norm: 0.15\n",
      "Epoch [97/200], Validation Loss: 0.0054\n",
      "Epoch [98/200], Train Loss: 0.0044, Gradient Norm: 0.03\n",
      "Epoch [98/200], Validation Loss: 0.0046\n",
      "Epoch [99/200], Train Loss: 0.0044, Gradient Norm: 0.05\n",
      "Epoch [99/200], Validation Loss: 0.0051\n",
      "Epoch [100/200], Train Loss: 0.0044, Gradient Norm: 0.27\n",
      "Epoch [100/200], Validation Loss: 0.0049\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_100.png\n",
      "Model saved at epoch 100\n",
      "Epoch [101/200], Train Loss: 0.0041, Gradient Norm: 0.08\n",
      "Epoch [101/200], Validation Loss: 0.0043\n",
      "Epoch [102/200], Train Loss: 0.0048, Gradient Norm: 0.26\n",
      "Epoch [102/200], Validation Loss: 0.0052\n",
      "Epoch [103/200], Train Loss: 0.0043, Gradient Norm: 0.12\n",
      "Epoch [103/200], Validation Loss: 0.0053\n",
      "Epoch [104/200], Train Loss: 0.0047, Gradient Norm: 0.10\n",
      "Epoch [104/200], Validation Loss: 0.0050\n",
      "Epoch [105/200], Train Loss: 0.0046, Gradient Norm: 0.09\n",
      "Epoch [105/200], Validation Loss: 0.0046\n",
      "Epoch 00105: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch [106/200], Train Loss: 0.0043, Gradient Norm: 0.11\n",
      "Epoch [106/200], Validation Loss: 0.0045\n",
      "Epoch [107/200], Train Loss: 0.0045, Gradient Norm: 0.03\n",
      "Epoch [107/200], Validation Loss: 0.0051\n",
      "Epoch [108/200], Train Loss: 0.0044, Gradient Norm: 0.06\n",
      "Epoch [108/200], Validation Loss: 0.0044\n",
      "Epoch [109/200], Train Loss: 0.0043, Gradient Norm: 0.04\n",
      "Epoch [109/200], Validation Loss: 0.0046\n",
      "Epoch [110/200], Train Loss: 0.0043, Gradient Norm: 0.27\n",
      "Epoch [110/200], Validation Loss: 0.0051\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_110.png\n",
      "Model saved at epoch 110\n",
      "Epoch [111/200], Train Loss: 0.0045, Gradient Norm: 0.02\n",
      "Epoch [111/200], Validation Loss: 0.0046\n",
      "Epoch [112/200], Train Loss: 0.0044, Gradient Norm: 0.02\n",
      "Epoch [112/200], Validation Loss: 0.0051\n",
      "Epoch [113/200], Train Loss: 0.0045, Gradient Norm: 0.09\n",
      "Epoch [113/200], Validation Loss: 0.0049\n",
      "Epoch [114/200], Train Loss: 0.0045, Gradient Norm: 0.08\n",
      "Epoch [114/200], Validation Loss: 0.0051\n",
      "Epoch [115/200], Train Loss: 0.0046, Gradient Norm: 0.09\n",
      "Epoch [115/200], Validation Loss: 0.0054\n",
      "Epoch [116/200], Train Loss: 0.0045, Gradient Norm: 0.06\n",
      "Epoch [116/200], Validation Loss: 0.0048\n",
      "Epoch [117/200], Train Loss: 0.0044, Gradient Norm: 0.04\n",
      "Epoch [117/200], Validation Loss: 0.0062\n",
      "Epoch [118/200], Train Loss: 0.0046, Gradient Norm: 0.07\n",
      "Epoch [118/200], Validation Loss: 0.0044\n",
      "Epoch [119/200], Train Loss: 0.0044, Gradient Norm: 0.09\n",
      "Epoch [119/200], Validation Loss: 0.0053\n",
      "Epoch [120/200], Train Loss: 0.0047, Gradient Norm: 0.13\n",
      "Epoch [120/200], Validation Loss: 0.0050\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_120.png\n",
      "Model saved at epoch 120\n",
      "Epoch [121/200], Train Loss: 0.0044, Gradient Norm: 0.12\n",
      "Epoch [121/200], Validation Loss: 0.0045\n",
      "Epoch 00121: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch [122/200], Train Loss: 0.0043, Gradient Norm: 0.01\n",
      "Epoch [122/200], Validation Loss: 0.0052\n",
      "Epoch [123/200], Train Loss: 0.0044, Gradient Norm: 0.17\n",
      "Epoch [123/200], Validation Loss: 0.0052\n",
      "Epoch [124/200], Train Loss: 0.0042, Gradient Norm: 0.10\n",
      "Epoch [124/200], Validation Loss: 0.0046\n",
      "Epoch [125/200], Train Loss: 0.0043, Gradient Norm: 0.05\n",
      "Epoch [125/200], Validation Loss: 0.0053\n",
      "Epoch [126/200], Train Loss: 0.0046, Gradient Norm: 0.01\n",
      "Epoch [126/200], Validation Loss: 0.0044\n",
      "Epoch [127/200], Train Loss: 0.0043, Gradient Norm: 0.11\n",
      "Epoch [127/200], Validation Loss: 0.0049\n",
      "Epoch [128/200], Train Loss: 0.0040, Gradient Norm: 0.09\n",
      "Epoch [128/200], Validation Loss: 0.0050\n",
      "Epoch [129/200], Train Loss: 0.0045, Gradient Norm: 0.17\n",
      "Epoch [129/200], Validation Loss: 0.0045\n",
      "Epoch [130/200], Train Loss: 0.0042, Gradient Norm: 0.09\n",
      "Epoch [130/200], Validation Loss: 0.0055\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_130.png\n",
      "Model saved at epoch 130\n",
      "Epoch [131/200], Train Loss: 0.0043, Gradient Norm: 0.13\n",
      "Epoch [131/200], Validation Loss: 0.0037\n",
      "Epoch [132/200], Train Loss: 0.0045, Gradient Norm: 0.06\n",
      "Epoch [132/200], Validation Loss: 0.0054\n",
      "Epoch [133/200], Train Loss: 0.0043, Gradient Norm: 0.07\n",
      "Epoch [133/200], Validation Loss: 0.0046\n",
      "Epoch [134/200], Train Loss: 0.0047, Gradient Norm: 0.10\n",
      "Epoch [134/200], Validation Loss: 0.0048\n",
      "Epoch [135/200], Train Loss: 0.0044, Gradient Norm: 0.10\n",
      "Epoch [135/200], Validation Loss: 0.0048\n",
      "Epoch [136/200], Train Loss: 0.0042, Gradient Norm: 0.08\n",
      "Epoch [136/200], Validation Loss: 0.0046\n",
      "Epoch [137/200], Train Loss: 0.0043, Gradient Norm: 0.02\n",
      "Epoch [137/200], Validation Loss: 0.0048\n",
      "Epoch [138/200], Train Loss: 0.0044, Gradient Norm: 0.11\n",
      "Epoch [138/200], Validation Loss: 0.0041\n",
      "Epoch [139/200], Train Loss: 0.0041, Gradient Norm: 0.02\n",
      "Epoch [139/200], Validation Loss: 0.0057\n",
      "Epoch [140/200], Train Loss: 0.0044, Gradient Norm: 0.02\n",
      "Epoch [140/200], Validation Loss: 0.0043\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_140.png\n",
      "Model saved at epoch 140\n",
      "Epoch [141/200], Train Loss: 0.0044, Gradient Norm: 0.03\n",
      "Epoch [141/200], Validation Loss: 0.0045\n",
      "Epoch [142/200], Train Loss: 0.0048, Gradient Norm: 0.10\n",
      "Epoch [142/200], Validation Loss: 0.0058\n",
      "Epoch [143/200], Train Loss: 0.0043, Gradient Norm: 0.16\n",
      "Epoch [143/200], Validation Loss: 0.0044\n",
      "Epoch [144/200], Train Loss: 0.0046, Gradient Norm: 0.06\n",
      "Epoch [144/200], Validation Loss: 0.0045\n",
      "Epoch [145/200], Train Loss: 0.0045, Gradient Norm: 0.02\n",
      "Epoch [145/200], Validation Loss: 0.0047\n",
      "Epoch [146/200], Train Loss: 0.0045, Gradient Norm: 0.12\n",
      "Epoch [146/200], Validation Loss: 0.0060\n",
      "Epoch [147/200], Train Loss: 0.0048, Gradient Norm: 0.10\n",
      "Epoch [147/200], Validation Loss: 0.0051\n",
      "Epoch 00147: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch [148/200], Train Loss: 0.0044, Gradient Norm: 0.02\n",
      "Epoch [148/200], Validation Loss: 0.0045\n",
      "Epoch [149/200], Train Loss: 0.0040, Gradient Norm: 0.05\n",
      "Epoch [149/200], Validation Loss: 0.0055\n",
      "Epoch [150/200], Train Loss: 0.0044, Gradient Norm: 0.03\n",
      "Epoch [150/200], Validation Loss: 0.0057\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_150.png\n",
      "Model saved at epoch 150\n",
      "Epoch [151/200], Train Loss: 0.0042, Gradient Norm: 0.05\n",
      "Epoch [151/200], Validation Loss: 0.0038\n",
      "Epoch [152/200], Train Loss: 0.0046, Gradient Norm: 0.11\n",
      "Epoch [152/200], Validation Loss: 0.0050\n",
      "Epoch [153/200], Train Loss: 0.0044, Gradient Norm: 0.11\n",
      "Epoch [153/200], Validation Loss: 0.0049\n",
      "Epoch [154/200], Train Loss: 0.0043, Gradient Norm: 0.08\n",
      "Epoch [154/200], Validation Loss: 0.0052\n",
      "Epoch [155/200], Train Loss: 0.0040, Gradient Norm: 0.06\n",
      "Epoch [155/200], Validation Loss: 0.0049\n",
      "Epoch [156/200], Train Loss: 0.0043, Gradient Norm: 0.02\n",
      "Epoch [156/200], Validation Loss: 0.0046\n",
      "Epoch [157/200], Train Loss: 0.0041, Gradient Norm: 0.11\n",
      "Epoch [157/200], Validation Loss: 0.0049\n",
      "Epoch [158/200], Train Loss: 0.0045, Gradient Norm: 0.01\n",
      "Epoch [158/200], Validation Loss: 0.0050\n",
      "Epoch [159/200], Train Loss: 0.0043, Gradient Norm: 0.02\n",
      "Epoch [159/200], Validation Loss: 0.0048\n",
      "Epoch [160/200], Train Loss: 0.0042, Gradient Norm: 0.07\n",
      "Epoch [160/200], Validation Loss: 0.0050\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_160.png\n",
      "Model saved at epoch 160\n",
      "Epoch [161/200], Train Loss: 0.0042, Gradient Norm: 0.01\n",
      "Epoch [161/200], Validation Loss: 0.0046\n",
      "Epoch [162/200], Train Loss: 0.0041, Gradient Norm: 0.06\n",
      "Epoch [162/200], Validation Loss: 0.0043\n",
      "Epoch [163/200], Train Loss: 0.0044, Gradient Norm: 0.02\n",
      "Epoch [163/200], Validation Loss: 0.0048\n",
      "Epoch 00163: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Epoch [164/200], Train Loss: 0.0040, Gradient Norm: 0.02\n",
      "Epoch [164/200], Validation Loss: 0.0047\n",
      "Epoch [165/200], Train Loss: 0.0042, Gradient Norm: 0.05\n",
      "Epoch [165/200], Validation Loss: 0.0044\n",
      "Epoch [166/200], Train Loss: 0.0043, Gradient Norm: 0.02\n",
      "Epoch [166/200], Validation Loss: 0.0052\n",
      "Epoch [167/200], Train Loss: 0.0044, Gradient Norm: 0.05\n",
      "Epoch [167/200], Validation Loss: 0.0049\n",
      "Epoch [168/200], Train Loss: 0.0043, Gradient Norm: 0.02\n",
      "Epoch [168/200], Validation Loss: 0.0041\n",
      "Epoch [169/200], Train Loss: 0.0043, Gradient Norm: 0.01\n",
      "Epoch [169/200], Validation Loss: 0.0052\n",
      "Epoch [170/200], Train Loss: 0.0044, Gradient Norm: 0.01\n",
      "Epoch [170/200], Validation Loss: 0.0048\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_170.png\n",
      "Model saved at epoch 170\n",
      "Epoch [171/200], Train Loss: 0.0042, Gradient Norm: 0.01\n",
      "Epoch [171/200], Validation Loss: 0.0048\n",
      "Epoch [172/200], Train Loss: 0.0041, Gradient Norm: 0.03\n",
      "Epoch [172/200], Validation Loss: 0.0058\n",
      "Epoch [173/200], Train Loss: 0.0047, Gradient Norm: 0.05\n",
      "Epoch [173/200], Validation Loss: 0.0052\n",
      "Epoch [174/200], Train Loss: 0.0041, Gradient Norm: 0.03\n",
      "Epoch [174/200], Validation Loss: 0.0045\n",
      "Epoch [175/200], Train Loss: 0.0045, Gradient Norm: 0.03\n",
      "Epoch [175/200], Validation Loss: 0.0053\n",
      "Epoch [176/200], Train Loss: 0.0042, Gradient Norm: 0.14\n",
      "Epoch [176/200], Validation Loss: 0.0042\n",
      "Epoch [177/200], Train Loss: 0.0044, Gradient Norm: 0.03\n",
      "Epoch [177/200], Validation Loss: 0.0046\n",
      "Epoch [178/200], Train Loss: 0.0046, Gradient Norm: 0.02\n",
      "Epoch [178/200], Validation Loss: 0.0057\n",
      "Epoch [179/200], Train Loss: 0.0045, Gradient Norm: 0.04\n",
      "Epoch [179/200], Validation Loss: 0.0049\n",
      "Epoch 00179: reducing learning rate of group 0 to 1.5625e-06.\n",
      "Epoch [180/200], Train Loss: 0.0041, Gradient Norm: 0.03\n",
      "Epoch [180/200], Validation Loss: 0.0041\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_180.png\n",
      "Model saved at epoch 180\n",
      "Epoch [181/200], Train Loss: 0.0044, Gradient Norm: 0.03\n",
      "Epoch [181/200], Validation Loss: 0.0049\n",
      "Epoch [182/200], Train Loss: 0.0042, Gradient Norm: 0.03\n",
      "Epoch [182/200], Validation Loss: 0.0045\n",
      "Epoch [183/200], Train Loss: 0.0042, Gradient Norm: 0.02\n",
      "Epoch [183/200], Validation Loss: 0.0054\n",
      "Epoch [184/200], Train Loss: 0.0045, Gradient Norm: 0.03\n",
      "Epoch [184/200], Validation Loss: 0.0050\n",
      "Epoch [185/200], Train Loss: 0.0041, Gradient Norm: 0.06\n",
      "Epoch [185/200], Validation Loss: 0.0046\n",
      "Epoch [186/200], Train Loss: 0.0044, Gradient Norm: 0.03\n",
      "Epoch [186/200], Validation Loss: 0.0050\n",
      "Epoch [187/200], Train Loss: 0.0042, Gradient Norm: 0.06\n",
      "Epoch [187/200], Validation Loss: 0.0045\n",
      "Epoch [188/200], Train Loss: 0.0048, Gradient Norm: 0.04\n",
      "Epoch [188/200], Validation Loss: 0.0043\n",
      "Epoch [189/200], Train Loss: 0.0042, Gradient Norm: 0.02\n",
      "Epoch [189/200], Validation Loss: 0.0055\n",
      "Epoch [190/200], Train Loss: 0.0041, Gradient Norm: 0.09\n",
      "Epoch [190/200], Validation Loss: 0.0057\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_190.png\n",
      "Model saved at epoch 190\n",
      "Epoch [191/200], Train Loss: 0.0045, Gradient Norm: 0.05\n",
      "Epoch [191/200], Validation Loss: 0.0043\n",
      "Epoch [192/200], Train Loss: 0.0045, Gradient Norm: 0.02\n",
      "Epoch [192/200], Validation Loss: 0.0047\n",
      "Epoch [193/200], Train Loss: 0.0041, Gradient Norm: 0.02\n",
      "Epoch [193/200], Validation Loss: 0.0054\n",
      "Epoch [194/200], Train Loss: 0.0041, Gradient Norm: 0.03\n",
      "Epoch [194/200], Validation Loss: 0.0048\n",
      "Epoch [195/200], Train Loss: 0.0044, Gradient Norm: 0.01\n",
      "Epoch [195/200], Validation Loss: 0.0052\n",
      "Epoch 00195: reducing learning rate of group 0 to 7.8125e-07.\n",
      "Epoch [196/200], Train Loss: 0.0042, Gradient Norm: 0.01\n",
      "Epoch [196/200], Validation Loss: 0.0055\n",
      "Epoch [197/200], Train Loss: 0.0041, Gradient Norm: 0.01\n",
      "Epoch [197/200], Validation Loss: 0.0049\n",
      "Epoch [198/200], Train Loss: 0.0046, Gradient Norm: 0.02\n",
      "Epoch [198/200], Validation Loss: 0.0041\n",
      "Epoch [199/200], Train Loss: 0.0044, Gradient Norm: 0.04\n",
      "Epoch [199/200], Validation Loss: 0.0040\n",
      "Epoch [200/200], Train Loss: 0.0042, Gradient Norm: 0.05\n",
      "Epoch [200/200], Validation Loss: 0.0052\n",
      "Model checkpoint saved at swinddpm256_MRNet_checkpoint.pth\n",
      "Image saved at generated_images/training/generated_image_epoch_200.png\n",
      "Model saved at epoch 200\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DDPM model\n",
    "in_channels = 1  # For grayscale images\n",
    "out_channels = 1  # For grayscale images\n",
    "emb_dim = 128\n",
    "num_timesteps = 2000\n",
    "latent_dim = 128\n",
    "\n",
    "unet = AttentionUNet(in_channels, out_channels, emb_dim).to(device)\n",
    "ddpm = DDPM(unet, num_timesteps, latent_dim).to(device)\n",
    "\n",
    "# Train the model\n",
    "train_diffusion_model(ddpm, train_loader, valid_loader, epochs=200, save_interval=10, checkpoint_path=\"swinddpm256_MRNet_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a37fd0",
   "metadata": {},
   "source": [
    "##### 9. Generating Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0b73092-c90b-46f6-b11b-253652f813d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 images saved separately in generated_images/DDPM_IMAGES256x256\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to save a single generated image\n",
    "def save_single_image(image, save_dir, prefix, index):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    file_path = os.path.join(save_dir, f'{prefix}_{index}.png')\n",
    "    plt.imsave(file_path, image, cmap='gray')\n",
    "\n",
    "# Function to generate and save images after training\n",
    "def generate_and_save_images_post_training(ddpm, num_images=10, save_dir='generated_images/DDPM_images'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    sample_shape = (1, 1, 256, 256)  \n",
    "\n",
    "    ddpm.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_images):\n",
    "            samples = ddpm.sample(sample_shape)\n",
    "            # Convert to numpy\n",
    "            samples = samples.cpu().numpy().squeeze()\n",
    "            \n",
    "            # Save each image individually\n",
    "            save_single_image(samples, save_dir, 'generated_image', i+1)\n",
    "\n",
    "    print(f'{num_images} images saved separately in {save_dir}')\n",
    "\n",
    "\n",
    "# Initialize the DDPM model\n",
    "in_channels = 1  # For grayscale images\n",
    "out_channels = 1  # For grayscale images\n",
    "emb_dim = 128\n",
    "num_timesteps = 2000\n",
    "latent_dim = 128\n",
    "\n",
    "unet = AttentionUNet(in_channels, out_channels, emb_dim).to(device)\n",
    "ddpm = DDPM(unet, num_timesteps, latent_dim).to(device)\n",
    "\n",
    "# Optimizer \n",
    "optimizer = optim.AdamW(ddpm.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "\n",
    "# Load the trained model\n",
    "checkpoint_path = \"swinddpm256_MRNet_checkpoint.pth\"  \n",
    "ddpm, start_epoch = load_model(ddpm, checkpoint_path)\n",
    "\n",
    "# Generate and save images after training\n",
    "generate_and_save_images_post_training(ddpm, num_images=200, save_dir='generated_images/DDPM_IMAGES256x256')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
