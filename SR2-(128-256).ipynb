{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2bc3ecd",
   "metadata": {},
   "source": [
    "# Super-Resolution Model Using Diffusion Model (With Swin Transformer in the UNet)\n",
    "\n",
    "This notebook presents the implementation of super-resolution model SR2 that upscales images from 128x128 to 256x256 using a Diffusion Probabilistic Model (DDPM) which has a SWIN transformer in its UNet. The model's forward process involves concatenating the low-resolution image with its bilinearly upscaled version, which is then passed through the network to upscale and produce a high-resolution output.\n",
    "\n",
    "The notebook also includes the training routine for the model, which uses a combination of AdamW optimizer and a learning rate scheduler to minimize the mean squared error (MSE) between the predicted and target high-resolution images. The code saves the model's checkpoints periodically and generates comparison images that showcase the input, interpolated, upscaled, and target images side by side.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e9ca9e",
   "metadata": {},
   "source": [
    "##### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79299322-b79b-48f0-8f80-bab75283a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d8077",
   "metadata": {},
   "source": [
    "##### 2. Applying Transforms and Initializing Dataset Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90843e2d-e196-4cc2-8f41-05b32eab7387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define transformations for 128x128 and 256x256 images\n",
    "transform_128 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((128, 128)),  # Resize to 128x128\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "transform_256 = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Ensure images are single-channel\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "class MRNetUpscaleDataset(Dataset):\n",
    "    def __init__(self, slice_dir, label_files, transform_128=None, transform_256=None):\n",
    "        super().__init__()\n",
    "        self.slice_dir = slice_dir\n",
    "        self.transform_128 = transform_128\n",
    "        self.transform_256 = transform_256\n",
    "\n",
    "        self.labels_dict = {}\n",
    "        for label_file in label_files:\n",
    "            records = pd.read_csv(label_file, header=None, names=['id', 'label'])\n",
    "            records['id'] = records['id'].map(lambda i: '0' * (4 - len(str(i))) + str(i))\n",
    "            self.labels_dict.update(dict(zip(records['id'], records['label'])))\n",
    "\n",
    "        # List all slice files\n",
    "        self.slice_files = [os.path.join(slice_dir, fname) for fname in os.listdir(slice_dir) if fname.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        slice_path = self.slice_files[index]\n",
    "        image = Image.open(slice_path)\n",
    "        \n",
    "        if self.transform_128:\n",
    "            image_128 = self.transform_128(image)\n",
    "        if self.transform_256:\n",
    "            image_256 = self.transform_256(image)\n",
    "        \n",
    "        # Extract ID from filename to match with label\n",
    "        # Filename format: abnormal_0000_slice_0 -> extract \"0000\" as ID\n",
    "        slice_id = os.path.basename(slice_path).split('_')[1]\n",
    "\n",
    "        if slice_id in self.labels_dict:\n",
    "            label = self.labels_dict[slice_id]\n",
    "            label = torch.FloatTensor([label])\n",
    "        else:\n",
    "            print(f\"Label for ID {slice_id} not found in the CSV file.\")\n",
    "            label = torch.FloatTensor([0])  # or raise an error if preferred\n",
    "\n",
    "        return {'data_128': image_128, 'data_256': image_256, 'label': label, 'id': slice_id}\n",
    "\n",
    "\n",
    "# Initialize datasets and data loaders\n",
    "root_dir = \"Raw_Images\"\n",
    "train_slice_dir = os.path.join(root_dir, \"train_slices_raw\")\n",
    "valid_slice_dir = os.path.join(root_dir, \"valid_slices_raw\")\n",
    "\n",
    "train_label_files = [\n",
    "    os.path.join(root_dir, \"train-acl.csv\"),\n",
    "    os.path.join(root_dir, \"train-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"train-meniscus.csv\")\n",
    "]\n",
    "\n",
    "valid_label_files = [\n",
    "    os.path.join(root_dir, \"valid-acl.csv\"),\n",
    "    os.path.join(root_dir, \"valid-abnormal.csv\"),\n",
    "    os.path.join(root_dir, \"valid-meniscus.csv\")\n",
    "]\n",
    "\n",
    "train_dataset = MRNetUpscaleDataset(slice_dir=train_slice_dir, label_files=train_label_files, transform_128=transform_128, transform_256=transform_256)\n",
    "valid_dataset = MRNetUpscaleDataset(slice_dir=valid_slice_dir, label_files=valid_label_files, transform_128=transform_128, transform_256=transform_256)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53961e1e",
   "metadata": {},
   "source": [
    "##### 3. Implementing Swin Transformer and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28e57d8-fb6d-4a97-a262-74a1d3da62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "<https://arxiv.org/abs/2103.14030>\n",
    "https://github.com/microsoft/Swin-Transformer\n",
    "\"\"\"\n",
    "\n",
    "# DropPath (Stochastic Depth) module to implement drop path regularization\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "        return output\n",
    "\n",
    "# Helper functions to handle tuple and truncation\n",
    "def to_2tuple(x):\n",
    "    if isinstance(x, (tuple, list)):\n",
    "        return x\n",
    "    return (x, x)\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1.):\n",
    "    with torch.no_grad():\n",
    "        size = tensor.shape\n",
    "        tmp = tensor.new_empty(size + (4,)).normal_()\n",
    "        valid = (tmp < 2) & (tmp > -2)\n",
    "        ind = valid.max(-1, keepdim=True)[1]\n",
    "        tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
    "        tensor.data.mul_(std).add_(mean)\n",
    "        return tensor\n",
    "\n",
    "# MLP module used within Swin Transformer\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "# Functions to partition and reverse windows in the Swin Transformer\n",
    "def window_partition(x, window_size):\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.view(B, C, H // window_size, window_size, W // window_size, window_size)\n",
    "    windows = x.permute(0, 2, 4, 3, 5, 1).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 5, 1, 3, 2, 4).contiguous().view(B, -1, H, W)\n",
    "    return x\n",
    "\n",
    "# Window-based multi-head self-attention (W-MSA) module\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # Relative position bias table for all windows\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n",
    "\n",
    "        # Get relative position index for each window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1)\n",
    "        attn = attn + relative_position_bias.unsqueeze(0).to(attn.dtype)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "# Swin Transformer block implementing the shifted window-based attention mechanism\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * self.mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, out_features=dim, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            attn_mask = self.calculate_mask(self.input_resolution)\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def calculate_mask(self, x_size):\n",
    "        H, W = x_size\n",
    "        img_mask = torch.zeros((1, H, W, 1))\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size).view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        return attn_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, C = x.shape\n",
    "        H, W = self.input_resolution\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        x_windows = window_partition(shifted_x, self.window_size)\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
    "\n",
    "        if self.attn_mask is not None:\n",
    "            attn_windows = self.attn(x_windows, mask=self.attn_mask.to(x.dtype))\n",
    "        else:\n",
    "            attn_windows = self.attn(x_windows)\n",
    "\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62465185",
   "metadata": {},
   "source": [
    "##### 4. Implementing the UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c816a39a-8339-4bfb-9888-ac1aced5df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sinusoidal positional embedding\n",
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=10000):\n",
    "        super(SinusoidalPositionalEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, timesteps):\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        emb = math.log(self.max_len) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n",
    "        emb = timesteps[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "        if self.embedding_dim % 2 == 1:\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=1)\n",
    "        return emb\n",
    "\n",
    "# Define the UNet model with attention\n",
    "class SuperResUNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=128):\n",
    "        super(SuperResUNet, self).__init__()\n",
    "\n",
    "        self.encoder1 = self.conv_block(in_channels + emb_dim, 64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "        self.swin_block = SwinTransformerBlock(dim=512, input_resolution=(32, 32), num_heads=8, window_size=4)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self.conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.conv_block(256, 128)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self.conv_block(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "        self.timestep_embedding_layer = SinusoidalPositionalEmbedding(emb_dim)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.timestep_embedding_layer(t)\n",
    "        t_embed = t_embed.view(t.size(0), -1, 1, 1)\n",
    "        t_embed = t_embed.repeat(1, 1, x.size(2), x.size(3))\n",
    "\n",
    "        x = torch.cat((x, t_embed), dim=1)\n",
    "\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc1_pooled = self.pool1(enc1)\n",
    "\n",
    "        enc2 = self.encoder2(enc1_pooled)\n",
    "        enc2_pooled = self.pool2(enc2)\n",
    "\n",
    "        enc3 = self.encoder3(enc2_pooled)\n",
    "        enc3_pooled = self.pool3(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc3_pooled)\n",
    "        B, C, H, W = bottleneck.shape\n",
    "        bottleneck = bottleneck.view(B, H * W, C)\n",
    "        bottleneck = self.swin_block(bottleneck)\n",
    "        bottleneck = bottleneck.view(B, C, H, W)\n",
    "\n",
    "        # Decoder\n",
    "        upconv3 = self.upconv3(bottleneck)\n",
    "        dec3 = torch.cat((upconv3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        upconv2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((upconv2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        upconv1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((upconv1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        final_output = self.final_conv(dec1)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee492808",
   "metadata": {},
   "source": [
    "##### 5. Implementing the DDPM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb80b048-5c00-4a59-bb9a-cb6e3ece9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DDPM implementation adapted from:\n",
    "https://github.com/hojonathanho/diffusion/tree/master\n",
    "\"\"\"\n",
    "\n",
    "class SuperResDDPM(nn.Module):\n",
    "    def __init__(self, model, num_timesteps, beta_start=0.00085, beta_end=0.0120):\n",
    "        super(SuperResDDPM, self).__init__()\n",
    "        self.model = model\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', 1 - betas)\n",
    "        self.register_buffer('alphas_cumprod', torch.cumprod(1 - betas, dim=0))\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(self.alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - self.alphas_cumprod))\n",
    "\n",
    "    def forward(self, z_t, t, low_res_image):\n",
    "        # Concatenate low_res_image with z_t to condition the model\n",
    "        low_res_upsampled = F.interpolate(low_res_image, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        return self.model(torch.cat([z_t, low_res_upsampled], dim=1), t)\n",
    "\n",
    "    def sample_timesteps(self, batch_size):\n",
    "        return torch.randint(0, self.num_timesteps, (batch_size,)).to(device)\n",
    "\n",
    "    def forward_diffusion(self, target_high_res_img, t, noise):\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        return sqrt_alphas_cumprod_t * target_high_res_img + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_losses(self, input_low_res_img, target_high_res_img, t):\n",
    "        sqrt_alpha_t = self.sqrt_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        sqrt_one_minus_alpha_t = self.sqrt_one_minus_alphas_cumprod[t].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "        input_high_res_img = F.interpolate(input_low_res_img, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        noise = torch.randn_like(target_high_res_img)\n",
    "        z_t = self.forward_diffusion(target_high_res_img, t, noise)\n",
    "\n",
    "        predicted_noise = self.forward(z_t, t, input_low_res_img)\n",
    "\n",
    "        return nn.MSELoss()(predicted_noise, noise)\n",
    "\n",
    "    def sample(self, low_res_image):\n",
    "        z_t = torch.randn_like(F.interpolate(low_res_image, scale_factor=2, mode='bilinear', align_corners=False))\n",
    "\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "            t_tensor = torch.tensor([t], device=z_t.device).long()\n",
    "            alpha_t = self.alphas[t]\n",
    "            sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "            sqrt_one_minus_alpha_t = torch.sqrt(1 - self.alphas_cumprod[t])\n",
    "            beta_t = self.betas[t]\n",
    "\n",
    "            predicted_noise = self.forward(z_t, t_tensor, low_res_image)\n",
    "\n",
    "            z_t = (z_t - (1 - self.alphas[t]) * predicted_noise / sqrt_one_minus_alpha_t) / sqrt_alpha_t\n",
    "\n",
    "            if t > 0:\n",
    "                z_t += torch.randn_like(z_t) * torch.sqrt(beta_t)\n",
    "\n",
    "        return z_t\n",
    "\n",
    "    def p_sample(self, z, t, low_res_image):\n",
    "        alpha_t = self.alphas[t]\n",
    "        sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "        sqrt_one_minus_alpha_t = torch.sqrt(1 - self.alphas_cumprod[t])\n",
    "        beta_t = self.betas[t]\n",
    "        predicted_noise = self.forward(z, t, low_res_image)\n",
    "\n",
    "        z = (z - beta_t / sqrt_one_minus_alpha_t * predicted_noise) / sqrt_alpha_t\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e82d637",
   "metadata": {},
   "source": [
    "##### 6. Functions for Training and Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a076c6b-7911-421b-af1f-f83e9ed35f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "def save_model(ddpm, epoch, checkpoint_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': ddpm.state_dict()\n",
    "    }, checkpoint_path)\n",
    "    print(f'Model checkpoint saved at {checkpoint_path}')\n",
    "\n",
    "# Load the model checkpoint\n",
    "def load_model(ddpm, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    ddpm.load_state_dict(checkpoint['model_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    return ddpm, start_epoch\n",
    "\n",
    "# Function to generate and save comparison images\n",
    "def compare_and_save_images(ddpm, valid_loader, epoch, save_dir='generated_images/training'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    ddpm.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(valid_loader):\n",
    "            inputs = batch['data_128'].to(device)\n",
    "            targets = batch['data_256'].to(device)\n",
    "            \n",
    "            # Prepare lists to store images for comparison\n",
    "            inputs_list = []\n",
    "            interpolated_list = []\n",
    "            upscaled_list = []\n",
    "            targets_list = []\n",
    "            \n",
    "            # Process each image individually\n",
    "            for j in range(inputs.size(0)):\n",
    "                input_image = inputs[j].unsqueeze(0)\n",
    "                target_image = targets[j].unsqueeze(0)\n",
    "                \n",
    "                # Interpolate the low-resolution image\n",
    "                interpolated_image = F.interpolate(input_image, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "                \n",
    "                # Upscale the image using the DDPM model\n",
    "                upscaled_image = ddpm.sample(input_image)\n",
    "                \n",
    "                inputs_list.append(input_image.cpu().numpy().squeeze())\n",
    "                interpolated_list.append(interpolated_image.cpu().numpy().squeeze())\n",
    "                upscaled_list.append(upscaled_image.cpu().numpy().squeeze())\n",
    "                targets_list.append(target_image.cpu().numpy().squeeze())\n",
    "                \n",
    "            \n",
    "            # Plot comparison for the first 5 images\n",
    "            num_images = min(5, inputs.size(0))\n",
    "            fig, axes = plt.subplots(num_images, 4, figsize=(20, 5 * num_images))\n",
    "            \n",
    "            for j in range(num_images):\n",
    "                ax = axes[j, 0]\n",
    "                ax.imshow(inputs_list[j], cmap='gray')\n",
    "                ax.set_title(f'Input 128x128 Image {j+1}')\n",
    "                ax.axis('off')\n",
    "\n",
    "                ax = axes[j, 1]\n",
    "                ax.imshow(interpolated_list[j], cmap='gray')\n",
    "                ax.set_title(f'Interpolated 256x256 Image {j+1}')\n",
    "                ax.axis('off')\n",
    "                \n",
    "                ax = axes[j, 2]\n",
    "                ax.imshow(upscaled_list[j], cmap='gray')\n",
    "                ax.set_title(f'Upscaled 256x256 Image {j+1}')\n",
    "                ax.axis('off')\n",
    "\n",
    "                ax = axes[j, 3]\n",
    "                ax.imshow(targets_list[j], cmap='gray')\n",
    "                ax.set_title(f'Target 256x256 Image {j+1}')\n",
    "                ax.axis('off')\n",
    "            \n",
    "            save_path = os.path.join(save_dir, f'comparison_epoch_{epoch+1}.png')\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "            print(f'Comparison images saved at {save_path}')\n",
    "            break  # Only process the first batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef60d92",
   "metadata": {},
   "source": [
    "##### 7. Training routine for the Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b8878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diffusion_model(ddpm, train_loader, valid_loader, epochs=10, save_interval=10, checkpoint_path='cascadedddpm_checkpoint.pth'):\n",
    "    optimizer = optim.AdamW(ddpm.parameters(), lr=5e-5, weight_decay=1e-2)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, verbose=True)\n",
    "    start_epoch = 0\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        ddpm, start_epoch = load_model(ddpm, checkpoint_path)\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "        print(f\"Resuming training from epoch {start_epoch+1}\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}, starting from scratch.\")\n",
    "\n",
    "    ddpm.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        ddpm.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs = batch['data_128'].to(device)\n",
    "            targets = batch['data_256'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            t = ddpm.sample_timesteps(inputs.size(0))\n",
    "            loss = ddpm.p_losses(inputs, targets, t)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient checking\n",
    "            total_norm = 0\n",
    "            for p in ddpm.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "\n",
    "            if total_norm > 1e3:  # Threshold for exploding gradients\n",
    "                print(f\"Warning: Exploding gradients detected at epoch {epoch+1}, total norm: {total_norm:.2f}\")\n",
    "                continue\n",
    "            if total_norm < 1e-3:  # Threshold for vanishing gradients\n",
    "                print(f\"Warning: Vanishing gradients detected at epoch {epoch+1}, total norm: {total_norm:.2f}\")\n",
    "                continue\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Gradient Norm: {total_norm:.2f}')\n",
    "\n",
    "        ddpm.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                inputs = batch['data_128'].to(device)\n",
    "                targets = batch['data_256'].to(device)\n",
    "                t = ddpm.sample_timesteps(inputs.size(0))\n",
    "                loss = ddpm.p_losses(inputs, targets, t)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        avg_valid_loss = valid_loss / len(valid_loader)\n",
    "        valid_losses.append(avg_valid_loss)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_valid_loss:.4f}')\n",
    "\n",
    "        # Step the scheduler with the validation loss\n",
    "        scheduler.step(avg_valid_loss)\n",
    "\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            save_model(ddpm, epoch, checkpoint_path)\n",
    "            compare_and_save_images(ddpm, valid_loader, epoch)\n",
    "            print(f'Model saved at epoch {epoch+1}')\n",
    "        \n",
    "        # Special checkpoint save\n",
    "        # if (epoch + 1) % 100 == 0:\n",
    "        #     special_checkpoint_path = checkpoint_path.replace(\".pth\", f\"_{epoch+1}.pth\")\n",
    "        #     save_model(ddpm, epoch, special_checkpoint_path)\n",
    "        #     print(f'Special checkpoint saved at epoch {epoch+1}')\n",
    "\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6830507",
   "metadata": {},
   "source": [
    "##### 8. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "791c44b3-e198-4fbc-8484-351c5eff9400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found at cascadedddpm128added4_checkpoint.pth, starting from scratch.\n",
      "Epoch [1/300], Train Loss: 0.3114, Gradient Norm: 1.28\n",
      "Epoch [1/300], Validation Loss: 0.0543\n",
      "Epoch [2/300], Train Loss: 0.0355, Gradient Norm: 1.64\n",
      "Epoch [2/300], Validation Loss: 0.0301\n",
      "Epoch [3/300], Train Loss: 0.0217, Gradient Norm: 1.64\n",
      "Epoch [3/300], Validation Loss: 0.0145\n",
      "Epoch [4/300], Train Loss: 0.0142, Gradient Norm: 1.39\n",
      "Epoch [4/300], Validation Loss: 0.0121\n",
      "Epoch [5/300], Train Loss: 0.0110, Gradient Norm: 0.34\n",
      "Epoch [5/300], Validation Loss: 0.0085\n",
      "Epoch [6/300], Train Loss: 0.0090, Gradient Norm: 0.65\n",
      "Epoch [6/300], Validation Loss: 0.0056\n",
      "Epoch [7/300], Train Loss: 0.0103, Gradient Norm: 0.56\n",
      "Epoch [7/300], Validation Loss: 0.0065\n",
      "Epoch [8/300], Train Loss: 0.0064, Gradient Norm: 0.47\n",
      "Epoch [8/300], Validation Loss: 0.0092\n",
      "Epoch [9/300], Train Loss: 0.0053, Gradient Norm: 0.12\n",
      "Epoch [9/300], Validation Loss: 0.0059\n",
      "Epoch [10/300], Train Loss: 0.0062, Gradient Norm: 0.11\n",
      "Epoch [10/300], Validation Loss: 0.0062\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_10.png\n",
      "Model saved at epoch 10\n",
      "Epoch [11/300], Train Loss: 0.0055, Gradient Norm: 0.98\n",
      "Epoch [11/300], Validation Loss: 0.0038\n",
      "Epoch [12/300], Train Loss: 0.0068, Gradient Norm: 0.23\n",
      "Epoch [12/300], Validation Loss: 0.0048\n",
      "Epoch [13/300], Train Loss: 0.0045, Gradient Norm: 0.07\n",
      "Epoch [13/300], Validation Loss: 0.0057\n",
      "Epoch [14/300], Train Loss: 0.0042, Gradient Norm: 0.23\n",
      "Epoch [14/300], Validation Loss: 0.0048\n",
      "Epoch [15/300], Train Loss: 0.0063, Gradient Norm: 0.19\n",
      "Epoch [15/300], Validation Loss: 0.0041\n",
      "Epoch [16/300], Train Loss: 0.0042, Gradient Norm: 0.25\n",
      "Epoch [16/300], Validation Loss: 0.0049\n",
      "Epoch [17/300], Train Loss: 0.0040, Gradient Norm: 0.02\n",
      "Epoch [17/300], Validation Loss: 0.0030\n",
      "Epoch [18/300], Train Loss: 0.0041, Gradient Norm: 1.10\n",
      "Epoch [18/300], Validation Loss: 0.0029\n",
      "Epoch [19/300], Train Loss: 0.0041, Gradient Norm: 0.07\n",
      "Epoch [19/300], Validation Loss: 0.0045\n",
      "Epoch [20/300], Train Loss: 0.0048, Gradient Norm: 0.05\n",
      "Epoch [20/300], Validation Loss: 0.0037\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_20.png\n",
      "Model saved at epoch 20\n",
      "Epoch [21/300], Train Loss: 0.0037, Gradient Norm: 1.06\n",
      "Epoch [21/300], Validation Loss: 0.0045\n",
      "Epoch [22/300], Train Loss: 0.0034, Gradient Norm: 0.43\n",
      "Epoch [22/300], Validation Loss: 0.0029\n",
      "Epoch [23/300], Train Loss: 0.0069, Gradient Norm: 0.51\n",
      "Epoch [23/300], Validation Loss: 0.0095\n",
      "Epoch [24/300], Train Loss: 0.0064, Gradient Norm: 0.44\n",
      "Epoch [24/300], Validation Loss: 0.0035\n",
      "Epoch [25/300], Train Loss: 0.0038, Gradient Norm: 0.17\n",
      "Epoch [25/300], Validation Loss: 0.0052\n",
      "Epoch [26/300], Train Loss: 0.0037, Gradient Norm: 0.22\n",
      "Epoch [26/300], Validation Loss: 0.0038\n",
      "Epoch [27/300], Train Loss: 0.0041, Gradient Norm: 0.34\n",
      "Epoch [27/300], Validation Loss: 0.0038\n",
      "Epoch [28/300], Train Loss: 0.0031, Gradient Norm: 0.13\n",
      "Epoch [28/300], Validation Loss: 0.0032\n",
      "Epoch [29/300], Train Loss: 0.0037, Gradient Norm: 0.43\n",
      "Epoch [29/300], Validation Loss: 0.0028\n",
      "Epoch [30/300], Train Loss: 0.0035, Gradient Norm: 0.24\n",
      "Epoch [30/300], Validation Loss: 0.0042\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_30.png\n",
      "Model saved at epoch 30\n",
      "Epoch [31/300], Train Loss: 0.0036, Gradient Norm: 0.28\n",
      "Epoch [31/300], Validation Loss: 0.0031\n",
      "Epoch [32/300], Train Loss: 0.0031, Gradient Norm: 0.26\n",
      "Epoch [32/300], Validation Loss: 0.0032\n",
      "Epoch [33/300], Train Loss: 0.0034, Gradient Norm: 0.26\n",
      "Epoch [33/300], Validation Loss: 0.0044\n",
      "Epoch [34/300], Train Loss: 0.0031, Gradient Norm: 0.31\n",
      "Epoch [34/300], Validation Loss: 0.0027\n",
      "Epoch [35/300], Train Loss: 0.0037, Gradient Norm: 0.06\n",
      "Epoch [35/300], Validation Loss: 0.0029\n",
      "Epoch [36/300], Train Loss: 0.0026, Gradient Norm: 0.28\n",
      "Epoch [36/300], Validation Loss: 0.0024\n",
      "Epoch [37/300], Train Loss: 0.0028, Gradient Norm: 0.84\n",
      "Epoch [37/300], Validation Loss: 0.0028\n",
      "Epoch [38/300], Train Loss: 0.0037, Gradient Norm: 0.21\n",
      "Epoch [38/300], Validation Loss: 0.0029\n",
      "Epoch [39/300], Train Loss: 0.0028, Gradient Norm: 0.12\n",
      "Epoch [39/300], Validation Loss: 0.0036\n",
      "Epoch [40/300], Train Loss: 0.0029, Gradient Norm: 0.11\n",
      "Epoch [40/300], Validation Loss: 0.0032\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_40.png\n",
      "Model saved at epoch 40\n",
      "Epoch [41/300], Train Loss: 0.0027, Gradient Norm: 0.32\n",
      "Epoch [41/300], Validation Loss: 0.0022\n",
      "Epoch [42/300], Train Loss: 0.0033, Gradient Norm: 0.26\n",
      "Epoch [42/300], Validation Loss: 0.0025\n",
      "Epoch [43/300], Train Loss: 0.0032, Gradient Norm: 0.08\n",
      "Epoch [43/300], Validation Loss: 0.0022\n",
      "Epoch [44/300], Train Loss: 0.0029, Gradient Norm: 0.58\n",
      "Epoch [44/300], Validation Loss: 0.0024\n",
      "Epoch [45/300], Train Loss: 0.0025, Gradient Norm: 0.05\n",
      "Epoch [45/300], Validation Loss: 0.0038\n",
      "Epoch [46/300], Train Loss: 0.0028, Gradient Norm: 0.35\n",
      "Epoch [46/300], Validation Loss: 0.0029\n",
      "Epoch [47/300], Train Loss: 0.0030, Gradient Norm: 0.21\n",
      "Epoch [47/300], Validation Loss: 0.0030\n",
      "Epoch [48/300], Train Loss: 0.0026, Gradient Norm: 0.49\n",
      "Epoch [48/300], Validation Loss: 0.0031\n",
      "Epoch [49/300], Train Loss: 0.0026, Gradient Norm: 0.34\n",
      "Epoch [49/300], Validation Loss: 0.0024\n",
      "Epoch [50/300], Train Loss: 0.0028, Gradient Norm: 0.31\n",
      "Epoch [50/300], Validation Loss: 0.0024\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_50.png\n",
      "Model saved at epoch 50\n",
      "Epoch [51/300], Train Loss: 0.0027, Gradient Norm: 0.23\n",
      "Epoch [51/300], Validation Loss: 0.0028\n",
      "Epoch [52/300], Train Loss: 0.0028, Gradient Norm: 0.54\n",
      "Epoch [52/300], Validation Loss: 0.0035\n",
      "Epoch [53/300], Train Loss: 0.0030, Gradient Norm: 0.37\n",
      "Epoch [53/300], Validation Loss: 0.0025\n",
      "Epoch [54/300], Train Loss: 0.0023, Gradient Norm: 0.13\n",
      "Epoch [54/300], Validation Loss: 0.0023\n",
      "Epoch [55/300], Train Loss: 0.0027, Gradient Norm: 0.14\n",
      "Epoch [55/300], Validation Loss: 0.0024\n",
      "Epoch [56/300], Train Loss: 0.0027, Gradient Norm: 0.44\n",
      "Epoch [56/300], Validation Loss: 0.0024\n",
      "Epoch [57/300], Train Loss: 0.0025, Gradient Norm: 0.05\n",
      "Epoch [57/300], Validation Loss: 0.0024\n",
      "Epoch 00057: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch [58/300], Train Loss: 0.0024, Gradient Norm: 0.20\n",
      "Epoch [58/300], Validation Loss: 0.0022\n",
      "Epoch [59/300], Train Loss: 0.0023, Gradient Norm: 0.13\n",
      "Epoch [59/300], Validation Loss: 0.0025\n",
      "Epoch [60/300], Train Loss: 0.0021, Gradient Norm: 0.22\n",
      "Epoch [60/300], Validation Loss: 0.0023\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_60.png\n",
      "Model saved at epoch 60\n",
      "Epoch [61/300], Train Loss: 0.0022, Gradient Norm: 0.41\n",
      "Epoch [61/300], Validation Loss: 0.0029\n",
      "Epoch [62/300], Train Loss: 0.0022, Gradient Norm: 0.02\n",
      "Epoch [62/300], Validation Loss: 0.0021\n",
      "Epoch [63/300], Train Loss: 0.0021, Gradient Norm: 0.04\n",
      "Epoch [63/300], Validation Loss: 0.0019\n",
      "Epoch [64/300], Train Loss: 0.0023, Gradient Norm: 0.21\n",
      "Epoch [64/300], Validation Loss: 0.0024\n",
      "Epoch [65/300], Train Loss: 0.0022, Gradient Norm: 0.08\n",
      "Epoch [65/300], Validation Loss: 0.0017\n",
      "Epoch [66/300], Train Loss: 0.0023, Gradient Norm: 0.17\n",
      "Epoch [66/300], Validation Loss: 0.0023\n",
      "Epoch [67/300], Train Loss: 0.0021, Gradient Norm: 0.08\n",
      "Epoch [67/300], Validation Loss: 0.0025\n",
      "Epoch [68/300], Train Loss: 0.0021, Gradient Norm: 0.04\n",
      "Epoch [68/300], Validation Loss: 0.0023\n",
      "Epoch [69/300], Train Loss: 0.0024, Gradient Norm: 0.24\n",
      "Epoch [69/300], Validation Loss: 0.0021\n",
      "Epoch [70/300], Train Loss: 0.0022, Gradient Norm: 0.44\n",
      "Epoch [70/300], Validation Loss: 0.0024\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_70.png\n",
      "Model saved at epoch 70\n",
      "Epoch [71/300], Train Loss: 0.0022, Gradient Norm: 0.12\n",
      "Epoch [71/300], Validation Loss: 0.0022\n",
      "Epoch [72/300], Train Loss: 0.0024, Gradient Norm: 0.17\n",
      "Epoch [72/300], Validation Loss: 0.0020\n",
      "Epoch [73/300], Train Loss: 0.0024, Gradient Norm: 0.34\n",
      "Epoch [73/300], Validation Loss: 0.0029\n",
      "Epoch [74/300], Train Loss: 0.0022, Gradient Norm: 0.08\n",
      "Epoch [74/300], Validation Loss: 0.0021\n",
      "Epoch [75/300], Train Loss: 0.0022, Gradient Norm: 0.21\n",
      "Epoch [75/300], Validation Loss: 0.0030\n",
      "Epoch [76/300], Train Loss: 0.0022, Gradient Norm: 0.22\n",
      "Epoch [76/300], Validation Loss: 0.0021\n",
      "Epoch [77/300], Train Loss: 0.0023, Gradient Norm: 0.04\n",
      "Epoch [77/300], Validation Loss: 0.0026\n",
      "Epoch [78/300], Train Loss: 0.0022, Gradient Norm: 0.13\n",
      "Epoch [78/300], Validation Loss: 0.0021\n",
      "Epoch [79/300], Train Loss: 0.0021, Gradient Norm: 0.19\n",
      "Epoch [79/300], Validation Loss: 0.0025\n",
      "Epoch [80/300], Train Loss: 0.0020, Gradient Norm: 0.24\n",
      "Epoch [80/300], Validation Loss: 0.0020\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_80.png\n",
      "Model saved at epoch 80\n",
      "Epoch [81/300], Train Loss: 0.0022, Gradient Norm: 0.25\n",
      "Epoch [81/300], Validation Loss: 0.0024\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch [82/300], Train Loss: 0.0022, Gradient Norm: 0.09\n",
      "Epoch [82/300], Validation Loss: 0.0019\n",
      "Epoch [83/300], Train Loss: 0.0021, Gradient Norm: 0.04\n",
      "Epoch [83/300], Validation Loss: 0.0024\n",
      "Epoch [84/300], Train Loss: 0.0020, Gradient Norm: 0.02\n",
      "Epoch [84/300], Validation Loss: 0.0024\n",
      "Epoch [85/300], Train Loss: 0.0021, Gradient Norm: 0.12\n",
      "Epoch [85/300], Validation Loss: 0.0018\n",
      "Epoch [86/300], Train Loss: 0.0020, Gradient Norm: 0.28\n",
      "Epoch [86/300], Validation Loss: 0.0018\n",
      "Epoch [87/300], Train Loss: 0.0020, Gradient Norm: 0.19\n",
      "Epoch [87/300], Validation Loss: 0.0019\n",
      "Epoch [88/300], Train Loss: 0.0022, Gradient Norm: 0.02\n",
      "Epoch [88/300], Validation Loss: 0.0015\n",
      "Epoch [89/300], Train Loss: 0.0020, Gradient Norm: 0.07\n",
      "Epoch [89/300], Validation Loss: 0.0021\n",
      "Epoch [90/300], Train Loss: 0.0020, Gradient Norm: 0.05\n",
      "Epoch [90/300], Validation Loss: 0.0017\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_90.png\n",
      "Model saved at epoch 90\n",
      "Epoch [91/300], Train Loss: 0.0020, Gradient Norm: 0.04\n",
      "Epoch [91/300], Validation Loss: 0.0029\n",
      "Epoch [92/300], Train Loss: 0.0021, Gradient Norm: 0.03\n",
      "Epoch [92/300], Validation Loss: 0.0021\n",
      "Epoch [93/300], Train Loss: 0.0022, Gradient Norm: 0.07\n",
      "Epoch [93/300], Validation Loss: 0.0015\n",
      "Epoch [94/300], Train Loss: 0.0022, Gradient Norm: 0.04\n",
      "Epoch [94/300], Validation Loss: 0.0022\n",
      "Epoch [95/300], Train Loss: 0.0020, Gradient Norm: 0.19\n",
      "Epoch [95/300], Validation Loss: 0.0019\n",
      "Epoch [96/300], Train Loss: 0.0021, Gradient Norm: 0.37\n",
      "Epoch [96/300], Validation Loss: 0.0025\n",
      "Epoch [97/300], Train Loss: 0.0021, Gradient Norm: 0.19\n",
      "Epoch [97/300], Validation Loss: 0.0024\n",
      "Epoch [98/300], Train Loss: 0.0020, Gradient Norm: 0.05\n",
      "Epoch [98/300], Validation Loss: 0.0019\n",
      "Epoch [99/300], Train Loss: 0.0020, Gradient Norm: 0.10\n",
      "Epoch [99/300], Validation Loss: 0.0023\n",
      "Epoch [100/300], Train Loss: 0.0018, Gradient Norm: 0.13\n",
      "Epoch [100/300], Validation Loss: 0.0026\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_100.png\n",
      "Model saved at epoch 100\n",
      "Epoch [101/300], Train Loss: 0.0019, Gradient Norm: 0.13\n",
      "Epoch [101/300], Validation Loss: 0.0024\n",
      "Epoch [102/300], Train Loss: 0.0020, Gradient Norm: 0.19\n",
      "Epoch [102/300], Validation Loss: 0.0019\n",
      "Epoch [103/300], Train Loss: 0.0021, Gradient Norm: 0.05\n",
      "Epoch [103/300], Validation Loss: 0.0017\n",
      "Epoch [104/300], Train Loss: 0.0020, Gradient Norm: 0.27\n",
      "Epoch [104/300], Validation Loss: 0.0028\n",
      "Epoch [105/300], Train Loss: 0.0021, Gradient Norm: 0.14\n",
      "Epoch [105/300], Validation Loss: 0.0021\n",
      "Epoch [106/300], Train Loss: 0.0019, Gradient Norm: 0.13\n",
      "Epoch [106/300], Validation Loss: 0.0017\n",
      "Epoch [107/300], Train Loss: 0.0021, Gradient Norm: 0.14\n",
      "Epoch [107/300], Validation Loss: 0.0014\n",
      "Epoch [108/300], Train Loss: 0.0019, Gradient Norm: 0.21\n",
      "Epoch [108/300], Validation Loss: 0.0024\n",
      "Epoch [109/300], Train Loss: 0.0021, Gradient Norm: 0.25\n",
      "Epoch [109/300], Validation Loss: 0.0017\n",
      "Epoch [110/300], Train Loss: 0.0020, Gradient Norm: 0.02\n",
      "Epoch [110/300], Validation Loss: 0.0018\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_110.png\n",
      "Model saved at epoch 110\n",
      "Epoch [111/300], Train Loss: 0.0020, Gradient Norm: 0.13\n",
      "Epoch [111/300], Validation Loss: 0.0022\n",
      "Epoch [112/300], Train Loss: 0.0020, Gradient Norm: 0.13\n",
      "Epoch [112/300], Validation Loss: 0.0019\n",
      "Epoch [113/300], Train Loss: 0.0017, Gradient Norm: 0.02\n",
      "Epoch [113/300], Validation Loss: 0.0021\n",
      "Epoch [114/300], Train Loss: 0.0019, Gradient Norm: 0.24\n",
      "Epoch [114/300], Validation Loss: 0.0022\n",
      "Epoch [115/300], Train Loss: 0.0021, Gradient Norm: 0.10\n",
      "Epoch [115/300], Validation Loss: 0.0020\n",
      "Epoch [116/300], Train Loss: 0.0018, Gradient Norm: 0.18\n",
      "Epoch [116/300], Validation Loss: 0.0017\n",
      "Epoch [117/300], Train Loss: 0.0020, Gradient Norm: 0.11\n",
      "Epoch [117/300], Validation Loss: 0.0017\n",
      "Epoch [118/300], Train Loss: 0.0018, Gradient Norm: 0.21\n",
      "Epoch [118/300], Validation Loss: 0.0017\n",
      "Epoch [119/300], Train Loss: 0.0020, Gradient Norm: 0.27\n",
      "Epoch [119/300], Validation Loss: 0.0022\n",
      "Epoch [120/300], Train Loss: 0.0021, Gradient Norm: 0.24\n",
      "Epoch [120/300], Validation Loss: 0.0019\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_120.png\n",
      "Model saved at epoch 120\n",
      "Epoch [121/300], Train Loss: 0.0020, Gradient Norm: 0.10\n",
      "Epoch [121/300], Validation Loss: 0.0018\n",
      "Epoch [122/300], Train Loss: 0.0021, Gradient Norm: 0.05\n",
      "Epoch [122/300], Validation Loss: 0.0015\n",
      "Epoch [123/300], Train Loss: 0.0020, Gradient Norm: 0.05\n",
      "Epoch [123/300], Validation Loss: 0.0026\n",
      "Epoch 00123: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch [124/300], Train Loss: 0.0019, Gradient Norm: 0.06\n",
      "Epoch [124/300], Validation Loss: 0.0022\n",
      "Epoch [125/300], Train Loss: 0.0016, Gradient Norm: 0.03\n",
      "Epoch [125/300], Validation Loss: 0.0017\n",
      "Epoch [126/300], Train Loss: 0.0018, Gradient Norm: 0.04\n",
      "Epoch [126/300], Validation Loss: 0.0020\n",
      "Epoch [127/300], Train Loss: 0.0017, Gradient Norm: 0.05\n",
      "Epoch [127/300], Validation Loss: 0.0016\n",
      "Epoch [128/300], Train Loss: 0.0019, Gradient Norm: 0.06\n",
      "Epoch [128/300], Validation Loss: 0.0018\n",
      "Epoch [129/300], Train Loss: 0.0019, Gradient Norm: 0.35\n",
      "Epoch [129/300], Validation Loss: 0.0021\n",
      "Epoch [130/300], Train Loss: 0.0020, Gradient Norm: 0.07\n",
      "Epoch [130/300], Validation Loss: 0.0016\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_130.png\n",
      "Model saved at epoch 130\n",
      "Epoch [131/300], Train Loss: 0.0018, Gradient Norm: 0.12\n",
      "Epoch [131/300], Validation Loss: 0.0019\n",
      "Epoch [132/300], Train Loss: 0.0021, Gradient Norm: 0.07\n",
      "Epoch [132/300], Validation Loss: 0.0016\n",
      "Epoch [133/300], Train Loss: 0.0021, Gradient Norm: 0.01\n",
      "Epoch [133/300], Validation Loss: 0.0029\n",
      "Epoch [134/300], Train Loss: 0.0020, Gradient Norm: 0.01\n",
      "Epoch [134/300], Validation Loss: 0.0013\n",
      "Epoch [135/300], Train Loss: 0.0018, Gradient Norm: 0.11\n",
      "Epoch [135/300], Validation Loss: 0.0023\n",
      "Epoch [136/300], Train Loss: 0.0018, Gradient Norm: 0.08\n",
      "Epoch [136/300], Validation Loss: 0.0017\n",
      "Epoch [137/300], Train Loss: 0.0018, Gradient Norm: 0.07\n",
      "Epoch [137/300], Validation Loss: 0.0017\n",
      "Epoch [138/300], Train Loss: 0.0018, Gradient Norm: 0.05\n",
      "Epoch [138/300], Validation Loss: 0.0016\n",
      "Epoch [139/300], Train Loss: 0.0019, Gradient Norm: 0.09\n",
      "Epoch [139/300], Validation Loss: 0.0017\n",
      "Epoch [140/300], Train Loss: 0.0017, Gradient Norm: 0.14\n",
      "Epoch [140/300], Validation Loss: 0.0020\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_140.png\n",
      "Model saved at epoch 140\n",
      "Epoch [141/300], Train Loss: 0.0016, Gradient Norm: 0.11\n",
      "Epoch [141/300], Validation Loss: 0.0025\n",
      "Epoch [142/300], Train Loss: 0.0019, Gradient Norm: 0.22\n",
      "Epoch [142/300], Validation Loss: 0.0018\n",
      "Epoch [143/300], Train Loss: 0.0018, Gradient Norm: 0.10\n",
      "Epoch [143/300], Validation Loss: 0.0016\n",
      "Epoch [144/300], Train Loss: 0.0019, Gradient Norm: 0.08\n",
      "Epoch [144/300], Validation Loss: 0.0016\n",
      "Epoch [145/300], Train Loss: 0.0019, Gradient Norm: 0.13\n",
      "Epoch [145/300], Validation Loss: 0.0016\n",
      "Epoch [146/300], Train Loss: 0.0019, Gradient Norm: 0.04\n",
      "Epoch [146/300], Validation Loss: 0.0017\n",
      "Epoch [147/300], Train Loss: 0.0019, Gradient Norm: 0.09\n",
      "Epoch [147/300], Validation Loss: 0.0021\n",
      "Epoch [148/300], Train Loss: 0.0019, Gradient Norm: 0.09\n",
      "Epoch [148/300], Validation Loss: 0.0013\n",
      "Epoch [149/300], Train Loss: 0.0018, Gradient Norm: 0.03\n",
      "Epoch [149/300], Validation Loss: 0.0015\n",
      "Epoch [150/300], Train Loss: 0.0018, Gradient Norm: 0.05\n",
      "Epoch [150/300], Validation Loss: 0.0022\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_150.png\n",
      "Model saved at epoch 150\n",
      "Epoch [151/300], Train Loss: 0.0019, Gradient Norm: 0.18\n",
      "Epoch [151/300], Validation Loss: 0.0017\n",
      "Epoch [152/300], Train Loss: 0.0019, Gradient Norm: 0.21\n",
      "Epoch [152/300], Validation Loss: 0.0016\n",
      "Epoch [153/300], Train Loss: 0.0018, Gradient Norm: 0.10\n",
      "Epoch [153/300], Validation Loss: 0.0016\n",
      "Epoch [154/300], Train Loss: 0.0019, Gradient Norm: 0.17\n",
      "Epoch [154/300], Validation Loss: 0.0016\n",
      "Epoch [155/300], Train Loss: 0.0017, Gradient Norm: 0.06\n",
      "Epoch [155/300], Validation Loss: 0.0019\n",
      "Epoch [156/300], Train Loss: 0.0018, Gradient Norm: 0.01\n",
      "Epoch [156/300], Validation Loss: 0.0016\n",
      "Epoch [157/300], Train Loss: 0.0019, Gradient Norm: 0.08\n",
      "Epoch [157/300], Validation Loss: 0.0016\n",
      "Epoch [158/300], Train Loss: 0.0016, Gradient Norm: 0.09\n",
      "Epoch [158/300], Validation Loss: 0.0017\n",
      "Epoch [159/300], Train Loss: 0.0018, Gradient Norm: 0.09\n",
      "Epoch [159/300], Validation Loss: 0.0018\n",
      "Epoch [160/300], Train Loss: 0.0019, Gradient Norm: 0.03\n",
      "Epoch [160/300], Validation Loss: 0.0023\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_160.png\n",
      "Model saved at epoch 160\n",
      "Epoch [161/300], Train Loss: 0.0019, Gradient Norm: 0.13\n",
      "Epoch [161/300], Validation Loss: 0.0016\n",
      "Epoch [162/300], Train Loss: 0.0017, Gradient Norm: 0.09\n",
      "Epoch [162/300], Validation Loss: 0.0017\n",
      "Epoch [163/300], Train Loss: 0.0017, Gradient Norm: 0.09\n",
      "Epoch [163/300], Validation Loss: 0.0028\n",
      "Epoch [164/300], Train Loss: 0.0018, Gradient Norm: 0.08\n",
      "Epoch [164/300], Validation Loss: 0.0023\n",
      "Epoch 00164: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Epoch [165/300], Train Loss: 0.0018, Gradient Norm: 0.10\n",
      "Epoch [165/300], Validation Loss: 0.0022\n",
      "Epoch [166/300], Train Loss: 0.0018, Gradient Norm: 0.03\n",
      "Epoch [166/300], Validation Loss: 0.0020\n",
      "Epoch [167/300], Train Loss: 0.0018, Gradient Norm: 0.06\n",
      "Epoch [167/300], Validation Loss: 0.0012\n",
      "Epoch [168/300], Train Loss: 0.0018, Gradient Norm: 0.03\n",
      "Epoch [168/300], Validation Loss: 0.0020\n",
      "Epoch [169/300], Train Loss: 0.0016, Gradient Norm: 0.16\n",
      "Epoch [169/300], Validation Loss: 0.0015\n",
      "Epoch [170/300], Train Loss: 0.0019, Gradient Norm: 0.05\n",
      "Epoch [170/300], Validation Loss: 0.0016\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_170.png\n",
      "Model saved at epoch 170\n",
      "Epoch [171/300], Train Loss: 0.0019, Gradient Norm: 0.04\n",
      "Epoch [171/300], Validation Loss: 0.0020\n",
      "Epoch [172/300], Train Loss: 0.0020, Gradient Norm: 0.09\n",
      "Epoch [172/300], Validation Loss: 0.0011\n",
      "Epoch [173/300], Train Loss: 0.0018, Gradient Norm: 0.05\n",
      "Epoch [173/300], Validation Loss: 0.0018\n",
      "Epoch [174/300], Train Loss: 0.0017, Gradient Norm: 0.01\n",
      "Epoch [174/300], Validation Loss: 0.0015\n",
      "Epoch [175/300], Train Loss: 0.0015, Gradient Norm: 0.11\n",
      "Epoch [175/300], Validation Loss: 0.0013\n",
      "Epoch [176/300], Train Loss: 0.0018, Gradient Norm: 0.02\n",
      "Epoch [176/300], Validation Loss: 0.0015\n",
      "Epoch [177/300], Train Loss: 0.0019, Gradient Norm: 0.03\n",
      "Epoch [177/300], Validation Loss: 0.0014\n",
      "Epoch [178/300], Train Loss: 0.0018, Gradient Norm: 0.02\n",
      "Epoch [178/300], Validation Loss: 0.0016\n",
      "Epoch [179/300], Train Loss: 0.0021, Gradient Norm: 0.15\n",
      "Epoch [179/300], Validation Loss: 0.0020\n",
      "Epoch [180/300], Train Loss: 0.0020, Gradient Norm: 0.08\n",
      "Epoch [180/300], Validation Loss: 0.0016\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_180.png\n",
      "Model saved at epoch 180\n",
      "Epoch [181/300], Train Loss: 0.0020, Gradient Norm: 0.07\n",
      "Epoch [181/300], Validation Loss: 0.0016\n",
      "Epoch [182/300], Train Loss: 0.0017, Gradient Norm: 0.07\n",
      "Epoch [182/300], Validation Loss: 0.0017\n",
      "Epoch [183/300], Train Loss: 0.0018, Gradient Norm: 0.06\n",
      "Epoch [183/300], Validation Loss: 0.0017\n",
      "Epoch [184/300], Train Loss: 0.0020, Gradient Norm: 0.07\n",
      "Epoch [184/300], Validation Loss: 0.0015\n",
      "Epoch [185/300], Train Loss: 0.0018, Gradient Norm: 0.06\n",
      "Epoch [185/300], Validation Loss: 0.0018\n",
      "Epoch [186/300], Train Loss: 0.0019, Gradient Norm: 0.08\n",
      "Epoch [186/300], Validation Loss: 0.0022\n",
      "Epoch [187/300], Train Loss: 0.0016, Gradient Norm: 0.03\n",
      "Epoch [187/300], Validation Loss: 0.0020\n",
      "Epoch [188/300], Train Loss: 0.0018, Gradient Norm: 0.01\n",
      "Epoch [188/300], Validation Loss: 0.0016\n",
      "Epoch 00188: reducing learning rate of group 0 to 1.5625e-06.\n",
      "Epoch [189/300], Train Loss: 0.0019, Gradient Norm: 0.03\n",
      "Epoch [189/300], Validation Loss: 0.0014\n",
      "Epoch [190/300], Train Loss: 0.0018, Gradient Norm: 0.05\n",
      "Epoch [190/300], Validation Loss: 0.0020\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_190.png\n",
      "Model saved at epoch 190\n",
      "Epoch [191/300], Train Loss: 0.0019, Gradient Norm: 0.05\n",
      "Epoch [191/300], Validation Loss: 0.0012\n",
      "Epoch [192/300], Train Loss: 0.0016, Gradient Norm: 0.05\n",
      "Epoch [192/300], Validation Loss: 0.0016\n",
      "Epoch [193/300], Train Loss: 0.0017, Gradient Norm: 0.03\n",
      "Epoch [193/300], Validation Loss: 0.0015\n",
      "Epoch [194/300], Train Loss: 0.0018, Gradient Norm: 0.03\n",
      "Epoch [194/300], Validation Loss: 0.0015\n",
      "Epoch [195/300], Train Loss: 0.0020, Gradient Norm: 0.13\n",
      "Epoch [195/300], Validation Loss: 0.0014\n",
      "Epoch [196/300], Train Loss: 0.0017, Gradient Norm: 0.06\n",
      "Epoch [196/300], Validation Loss: 0.0017\n",
      "Epoch [197/300], Train Loss: 0.0017, Gradient Norm: 0.01\n",
      "Epoch [197/300], Validation Loss: 0.0025\n",
      "Epoch [198/300], Train Loss: 0.0019, Gradient Norm: 0.03\n",
      "Epoch [198/300], Validation Loss: 0.0018\n",
      "Epoch [199/300], Train Loss: 0.0018, Gradient Norm: 0.10\n",
      "Epoch [199/300], Validation Loss: 0.0023\n",
      "Epoch [200/300], Train Loss: 0.0018, Gradient Norm: 0.09\n",
      "Epoch [200/300], Validation Loss: 0.0023\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_200.png\n",
      "Model saved at epoch 200\n",
      "Epoch [201/300], Train Loss: 0.0017, Gradient Norm: 0.07\n",
      "Epoch [201/300], Validation Loss: 0.0018\n",
      "Epoch [202/300], Train Loss: 0.0020, Gradient Norm: 0.09\n",
      "Epoch [202/300], Validation Loss: 0.0017\n",
      "Epoch [203/300], Train Loss: 0.0019, Gradient Norm: 0.01\n",
      "Epoch [203/300], Validation Loss: 0.0019\n",
      "Epoch [204/300], Train Loss: 0.0018, Gradient Norm: 0.09\n",
      "Epoch [204/300], Validation Loss: 0.0026\n",
      "Epoch 00204: reducing learning rate of group 0 to 7.8125e-07.\n",
      "Epoch [205/300], Train Loss: 0.0019, Gradient Norm: 0.03\n",
      "Epoch [205/300], Validation Loss: 0.0019\n",
      "Epoch [206/300], Train Loss: 0.0020, Gradient Norm: 0.07\n",
      "Epoch [206/300], Validation Loss: 0.0014\n",
      "Epoch [207/300], Train Loss: 0.0018, Gradient Norm: 0.01\n",
      "Epoch [207/300], Validation Loss: 0.0024\n",
      "Epoch [208/300], Train Loss: 0.0019, Gradient Norm: 0.02\n",
      "Epoch [208/300], Validation Loss: 0.0022\n",
      "Epoch [209/300], Train Loss: 0.0017, Gradient Norm: 0.05\n",
      "Epoch [209/300], Validation Loss: 0.0012\n",
      "Epoch [210/300], Train Loss: 0.0018, Gradient Norm: 0.02\n",
      "Epoch [210/300], Validation Loss: 0.0015\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_210.png\n",
      "Model saved at epoch 210\n",
      "Epoch [211/300], Train Loss: 0.0019, Gradient Norm: 0.07\n",
      "Epoch [211/300], Validation Loss: 0.0014\n",
      "Epoch [212/300], Train Loss: 0.0018, Gradient Norm: 0.05\n",
      "Epoch [212/300], Validation Loss: 0.0013\n",
      "Epoch [213/300], Train Loss: 0.0017, Gradient Norm: 0.03\n",
      "Epoch [213/300], Validation Loss: 0.0018\n",
      "Epoch [214/300], Train Loss: 0.0016, Gradient Norm: 0.04\n",
      "Epoch [214/300], Validation Loss: 0.0010\n",
      "Epoch [215/300], Train Loss: 0.0018, Gradient Norm: 0.03\n",
      "Epoch [215/300], Validation Loss: 0.0018\n",
      "Epoch [216/300], Train Loss: 0.0017, Gradient Norm: 0.01\n",
      "Epoch [216/300], Validation Loss: 0.0016\n",
      "Epoch [217/300], Train Loss: 0.0015, Gradient Norm: 0.02\n",
      "Epoch [217/300], Validation Loss: 0.0025\n",
      "Epoch [218/300], Train Loss: 0.0020, Gradient Norm: 0.04\n",
      "Epoch [218/300], Validation Loss: 0.0017\n",
      "Epoch [219/300], Train Loss: 0.0019, Gradient Norm: 0.08\n",
      "Epoch [219/300], Validation Loss: 0.0019\n",
      "Epoch [220/300], Train Loss: 0.0017, Gradient Norm: 0.01\n",
      "Epoch [220/300], Validation Loss: 0.0015\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_220.png\n",
      "Model saved at epoch 220\n",
      "Epoch [221/300], Train Loss: 0.0018, Gradient Norm: 0.02\n",
      "Epoch [221/300], Validation Loss: 0.0018\n",
      "Epoch [222/300], Train Loss: 0.0018, Gradient Norm: 0.07\n",
      "Epoch [222/300], Validation Loss: 0.0014\n",
      "Epoch [223/300], Train Loss: 0.0018, Gradient Norm: 0.01\n",
      "Epoch [223/300], Validation Loss: 0.0020\n",
      "Epoch [224/300], Train Loss: 0.0019, Gradient Norm: 0.06\n",
      "Epoch [224/300], Validation Loss: 0.0021\n",
      "Epoch [225/300], Train Loss: 0.0018, Gradient Norm: 0.04\n",
      "Epoch [225/300], Validation Loss: 0.0018\n",
      "Epoch [226/300], Train Loss: 0.0016, Gradient Norm: 0.03\n",
      "Epoch [226/300], Validation Loss: 0.0017\n",
      "Epoch [227/300], Train Loss: 0.0017, Gradient Norm: 0.01\n",
      "Epoch [227/300], Validation Loss: 0.0019\n",
      "Epoch [228/300], Train Loss: 0.0019, Gradient Norm: 0.01\n",
      "Epoch [228/300], Validation Loss: 0.0018\n",
      "Epoch [229/300], Train Loss: 0.0017, Gradient Norm: 0.02\n",
      "Epoch [229/300], Validation Loss: 0.0014\n",
      "Epoch [230/300], Train Loss: 0.0017, Gradient Norm: 0.03\n",
      "Epoch [230/300], Validation Loss: 0.0017\n",
      "Epoch 00230: reducing learning rate of group 0 to 3.9063e-07.\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_230.png\n",
      "Model saved at epoch 230\n",
      "Epoch [231/300], Train Loss: 0.0018, Gradient Norm: 0.05\n",
      "Epoch [231/300], Validation Loss: 0.0019\n",
      "Epoch [232/300], Train Loss: 0.0015, Gradient Norm: 0.05\n",
      "Epoch [232/300], Validation Loss: 0.0017\n",
      "Epoch [233/300], Train Loss: 0.0017, Gradient Norm: 0.03\n",
      "Epoch [233/300], Validation Loss: 0.0020\n",
      "Epoch [234/300], Train Loss: 0.0017, Gradient Norm: 0.06\n",
      "Epoch [234/300], Validation Loss: 0.0018\n",
      "Epoch [235/300], Train Loss: 0.0017, Gradient Norm: 0.03\n",
      "Epoch [235/300], Validation Loss: 0.0024\n",
      "Epoch [236/300], Train Loss: 0.0018, Gradient Norm: 0.03\n",
      "Epoch [236/300], Validation Loss: 0.0027\n",
      "Epoch [237/300], Train Loss: 0.0020, Gradient Norm: 0.04\n",
      "Epoch [237/300], Validation Loss: 0.0021\n",
      "Epoch [238/300], Train Loss: 0.0017, Gradient Norm: 0.06\n",
      "Epoch [238/300], Validation Loss: 0.0015\n",
      "Epoch [239/300], Train Loss: 0.0018, Gradient Norm: 0.02\n",
      "Epoch [239/300], Validation Loss: 0.0018\n",
      "Epoch [240/300], Train Loss: 0.0019, Gradient Norm: 0.02\n",
      "Epoch [240/300], Validation Loss: 0.0013\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_240.png\n",
      "Model saved at epoch 240\n",
      "Epoch [241/300], Train Loss: 0.0018, Gradient Norm: 0.04\n",
      "Epoch [241/300], Validation Loss: 0.0019\n",
      "Epoch [242/300], Train Loss: 0.0016, Gradient Norm: 0.03\n",
      "Epoch [242/300], Validation Loss: 0.0018\n",
      "Epoch [243/300], Train Loss: 0.0017, Gradient Norm: 0.02\n",
      "Epoch [243/300], Validation Loss: 0.0018\n",
      "Epoch [244/300], Train Loss: 0.0018, Gradient Norm: 0.02\n",
      "Epoch [244/300], Validation Loss: 0.0018\n",
      "Epoch [245/300], Train Loss: 0.0019, Gradient Norm: 0.06\n",
      "Epoch [245/300], Validation Loss: 0.0016\n",
      "Epoch [246/300], Train Loss: 0.0018, Gradient Norm: 0.02\n",
      "Epoch [246/300], Validation Loss: 0.0015\n",
      "Epoch 00246: reducing learning rate of group 0 to 1.9531e-07.\n",
      "Epoch [247/300], Train Loss: 0.0019, Gradient Norm: 0.02\n",
      "Epoch [247/300], Validation Loss: 0.0013\n",
      "Epoch [248/300], Train Loss: 0.0015, Gradient Norm: 0.01\n",
      "Epoch [248/300], Validation Loss: 0.0018\n",
      "Epoch [249/300], Train Loss: 0.0017, Gradient Norm: 0.01\n",
      "Epoch [249/300], Validation Loss: 0.0020\n",
      "Epoch [250/300], Train Loss: 0.0017, Gradient Norm: 0.04\n",
      "Epoch [250/300], Validation Loss: 0.0016\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_250.png\n",
      "Model saved at epoch 250\n",
      "Epoch [251/300], Train Loss: 0.0021, Gradient Norm: 0.03\n",
      "Epoch [251/300], Validation Loss: 0.0019\n",
      "Epoch [252/300], Train Loss: 0.0016, Gradient Norm: 0.02\n",
      "Epoch [252/300], Validation Loss: 0.0017\n",
      "Epoch [253/300], Train Loss: 0.0018, Gradient Norm: 0.01\n",
      "Epoch [253/300], Validation Loss: 0.0019\n",
      "Epoch [254/300], Train Loss: 0.0018, Gradient Norm: 0.00\n",
      "Epoch [254/300], Validation Loss: 0.0022\n",
      "Epoch [255/300], Train Loss: 0.0016, Gradient Norm: 0.02\n",
      "Epoch [255/300], Validation Loss: 0.0023\n",
      "Epoch [256/300], Train Loss: 0.0017, Gradient Norm: 0.05\n",
      "Epoch [256/300], Validation Loss: 0.0016\n",
      "Epoch [257/300], Train Loss: 0.0018, Gradient Norm: 0.02\n",
      "Epoch [257/300], Validation Loss: 0.0018\n",
      "Epoch [258/300], Train Loss: 0.0016, Gradient Norm: 0.05\n",
      "Epoch [258/300], Validation Loss: 0.0014\n",
      "Epoch [259/300], Train Loss: 0.0018, Gradient Norm: 0.04\n",
      "Epoch [259/300], Validation Loss: 0.0015\n",
      "Epoch [260/300], Train Loss: 0.0018, Gradient Norm: 0.06\n",
      "Epoch [260/300], Validation Loss: 0.0015\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_260.png\n",
      "Model saved at epoch 260\n",
      "Epoch [261/300], Train Loss: 0.0016, Gradient Norm: 0.03\n",
      "Epoch [261/300], Validation Loss: 0.0018\n",
      "Epoch [262/300], Train Loss: 0.0019, Gradient Norm: 0.03\n",
      "Epoch [262/300], Validation Loss: 0.0025\n",
      "Epoch 00262: reducing learning rate of group 0 to 9.7656e-08.\n",
      "Epoch [263/300], Train Loss: 0.0018, Gradient Norm: 0.01\n",
      "Epoch [263/300], Validation Loss: 0.0014\n",
      "Epoch [264/300], Train Loss: 0.0016, Gradient Norm: 0.03\n",
      "Epoch [264/300], Validation Loss: 0.0017\n",
      "Epoch [265/300], Train Loss: 0.0018, Gradient Norm: 0.03\n",
      "Epoch [265/300], Validation Loss: 0.0021\n",
      "Epoch [266/300], Train Loss: 0.0017, Gradient Norm: 0.01\n",
      "Epoch [266/300], Validation Loss: 0.0015\n",
      "Epoch [267/300], Train Loss: 0.0018, Gradient Norm: 0.01\n",
      "Epoch [267/300], Validation Loss: 0.0021\n",
      "Epoch [268/300], Train Loss: 0.0019, Gradient Norm: 0.02\n",
      "Epoch [268/300], Validation Loss: 0.0018\n",
      "Epoch [269/300], Train Loss: 0.0018, Gradient Norm: 0.02\n",
      "Epoch [269/300], Validation Loss: 0.0015\n",
      "Epoch [270/300], Train Loss: 0.0016, Gradient Norm: 0.07\n",
      "Epoch [270/300], Validation Loss: 0.0020\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_270.png\n",
      "Model saved at epoch 270\n",
      "Epoch [271/300], Train Loss: 0.0017, Gradient Norm: 0.04\n",
      "Epoch [271/300], Validation Loss: 0.0016\n",
      "Epoch [272/300], Train Loss: 0.0018, Gradient Norm: 0.05\n",
      "Epoch [272/300], Validation Loss: 0.0015\n",
      "Epoch [273/300], Train Loss: 0.0017, Gradient Norm: 0.07\n",
      "Epoch [273/300], Validation Loss: 0.0018\n",
      "Epoch [274/300], Train Loss: 0.0019, Gradient Norm: 0.01\n",
      "Epoch [274/300], Validation Loss: 0.0017\n",
      "Epoch [275/300], Train Loss: 0.0018, Gradient Norm: 0.06\n",
      "Epoch [275/300], Validation Loss: 0.0025\n",
      "Epoch [276/300], Train Loss: 0.0016, Gradient Norm: 0.02\n",
      "Epoch [276/300], Validation Loss: 0.0015\n",
      "Epoch [277/300], Train Loss: 0.0017, Gradient Norm: 0.05\n",
      "Epoch [277/300], Validation Loss: 0.0017\n",
      "Epoch [278/300], Train Loss: 0.0019, Gradient Norm: 0.01\n",
      "Epoch [278/300], Validation Loss: 0.0020\n",
      "Epoch 00278: reducing learning rate of group 0 to 4.8828e-08.\n",
      "Epoch [279/300], Train Loss: 0.0019, Gradient Norm: 0.04\n",
      "Epoch [279/300], Validation Loss: 0.0016\n",
      "Epoch [280/300], Train Loss: 0.0016, Gradient Norm: 0.01\n",
      "Epoch [280/300], Validation Loss: 0.0021\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_280.png\n",
      "Model saved at epoch 280\n",
      "Epoch [281/300], Train Loss: 0.0016, Gradient Norm: 0.06\n",
      "Epoch [281/300], Validation Loss: 0.0018\n",
      "Epoch [282/300], Train Loss: 0.0017, Gradient Norm: 0.02\n",
      "Epoch [282/300], Validation Loss: 0.0017\n",
      "Epoch [283/300], Train Loss: 0.0018, Gradient Norm: 0.02\n",
      "Epoch [283/300], Validation Loss: 0.0019\n",
      "Epoch [284/300], Train Loss: 0.0018, Gradient Norm: 0.01\n",
      "Epoch [284/300], Validation Loss: 0.0015\n",
      "Epoch [285/300], Train Loss: 0.0017, Gradient Norm: 0.02\n",
      "Epoch [285/300], Validation Loss: 0.0020\n",
      "Epoch [286/300], Train Loss: 0.0017, Gradient Norm: 0.06\n",
      "Epoch [286/300], Validation Loss: 0.0019\n",
      "Epoch [287/300], Train Loss: 0.0017, Gradient Norm: 0.06\n",
      "Epoch [287/300], Validation Loss: 0.0017\n",
      "Epoch [288/300], Train Loss: 0.0017, Gradient Norm: 0.05\n",
      "Epoch [288/300], Validation Loss: 0.0015\n",
      "Epoch [289/300], Train Loss: 0.0017, Gradient Norm: 0.01\n",
      "Epoch [289/300], Validation Loss: 0.0018\n",
      "Epoch [290/300], Train Loss: 0.0016, Gradient Norm: 0.03\n",
      "Epoch [290/300], Validation Loss: 0.0019\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_290.png\n",
      "Model saved at epoch 290\n",
      "Epoch [291/300], Train Loss: 0.0019, Gradient Norm: 0.01\n",
      "Epoch [291/300], Validation Loss: 0.0017\n",
      "Epoch [292/300], Train Loss: 0.0018, Gradient Norm: 0.06\n",
      "Epoch [292/300], Validation Loss: 0.0012\n",
      "Epoch [293/300], Train Loss: 0.0017, Gradient Norm: 0.02\n",
      "Epoch [293/300], Validation Loss: 0.0019\n",
      "Epoch [294/300], Train Loss: 0.0019, Gradient Norm: 0.03\n",
      "Epoch [294/300], Validation Loss: 0.0023\n",
      "Epoch 00294: reducing learning rate of group 0 to 2.4414e-08.\n",
      "Epoch [295/300], Train Loss: 0.0017, Gradient Norm: 0.03\n",
      "Epoch [295/300], Validation Loss: 0.0014\n",
      "Epoch [296/300], Train Loss: 0.0019, Gradient Norm: 0.01\n",
      "Epoch [296/300], Validation Loss: 0.0021\n",
      "Epoch [297/300], Train Loss: 0.0018, Gradient Norm: 0.04\n",
      "Epoch [297/300], Validation Loss: 0.0021\n",
      "Epoch [298/300], Train Loss: 0.0017, Gradient Norm: 0.03\n",
      "Epoch [298/300], Validation Loss: 0.0013\n",
      "Epoch [299/300], Train Loss: 0.0018, Gradient Norm: 0.01\n",
      "Epoch [299/300], Validation Loss: 0.0018\n",
      "Epoch [300/300], Train Loss: 0.0017, Gradient Norm: 0.01\n",
      "Epoch [300/300], Validation Loss: 0.0025\n",
      "Model checkpoint saved at cascadedddpm128added4_checkpoint.pth\n",
      "Comparison images saved at generated_images/training/Cascaded/128to256lol3/comparison_epoch_300.png\n",
      "Model saved at epoch 300\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the UNet model and DDPM\n",
    "in_channels = 2  # For grayscale images\n",
    "out_channels = 1  # For grayscale images\n",
    "emb_dim = 128\n",
    "num_timesteps = 2000\n",
    "\n",
    "\n",
    "unet = SuperResUNet(in_channels, out_channels, emb_dim).to(device)\n",
    "ddpm = SuperResDDPM(unet, num_timesteps).to(device)\n",
    "\n",
    "# Train the model\n",
    "train_diffusion_model(ddpm, train_loader, valid_loader, epochs=300, save_interval=10, checkpoint_path='Model_Savepoints/cascadedddpm128added4_checkpoint.pth')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
